<div class="WordSection1">

![firm_3840_7d7727_big](readme_pliki/image001.jpg)

**<span style="font-size:16.0pt;line-height:115%;text-transform:uppercase">Porównanie istniejących algorytmów analityki obrazu - na podstawie algorytmów użytych w systemie Kamerdyner</span>**

**<span style="font-size:16.0pt;line-height:115%;text-transform:uppercase"> </span>**

<table class="MsoNormalTable" border="0" cellspacing="0" cellpadding="0" align="right" style="border-collapse:collapse;margin-left:6.75pt;margin-right:6.75pt">

<tbody>

<tr>

<td width="319" style="width:239.25pt;border:none;border-bottom:solid #BFBFBF 1.0pt;  padding:0cm 5.4pt 0cm 5.4pt">

<span lang="EN-US" style="font-size:14.0pt;line-height:115%;color:#5B9BD5;font-variant:bold">Face recognition, human silhouette recognition and fall detection</span>

</td>

</tr>

<tr>

<td width="319" style="width:239.25pt;border:none;border-bottom:solid #BFBFBF 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt">

<span lang="EN-US" style="font-size:14.0pt;line-height:115%;color:#5B9BD5">Version</span> <span lang="EN-US" style="font-size:14.0pt;line-height:115%">1.5</span>

</td>

</tr>

<tr>

<td width="319" style="width:239.25pt;border:none;padding:0cm 5.4pt 0cm 5.4pt">

<span lang="EN-US" style="font-size:14.0pt;line-height:115%;color:#5B9BD5">On</span> <span lang="EN-US" style="font-size:14.0pt;line-height:115%">2017-10-01</span>

</td>

</tr>

</tbody>

</table>

**<span style="font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,sans-serif;color:#5B9BD5;text-transform:uppercase;letter-spacing:.25pt">  
</span>**

<a name="_Toc301516488"></a><a name="_Toc488734921"><span class="MsoIntenseReference"><span style="font-size:24.0pt;line-height:115%;font-variant:bold!important;text-transform:uppercase">Informacje o dokumencie</span></span></a>

<table class="MsoNormalTable" border="1" cellspacing="0" cellpadding="0" width="0" style="width:489.05pt;margin-left:5.4pt;background:#CCCCCC;border-collapse:
 collapse;border:none">

<tbody>

<tr style="height:1.0cm">

<td width="198" style="width:148.85pt;border:solid #BFBFBF 1.0pt;border-right:  none;background:#F3F3F3;padding:0cm 5.4pt 0cm 5.4pt;height:1.0cm">

**<span lang="EN-US" style="line-height:115%;color:#8496B0">Wersja</span>**

</td>

<td width="454" style="width:12.0cm;border:solid #BFBFBF 1.0pt;border-left:  none;background:white;padding:0cm 5.4pt 0cm 5.4pt;height:1.0cm">

**<span lang="EN-US" style="line-height:115%">0.15</span>**

</td>

</tr>

<tr style="height:1.0cm">

<td width="198" style="width:148.85pt;border-top:none;border-left:solid #BFBFBF 1.0pt;
  border-bottom:solid #BFBFBF 1.0pt;border-right:none;background:#F3F3F3;
  padding:0cm 5.4pt 0cm 5.4pt;height:1.0cm">

**<span lang="EN-US" style="line-height:115%;color:#8496B0">Data</span>**

</td>

<td width="454" style="width:12.0cm;border-top:none;border-left:none;
  border-bottom:solid #BFBFBF 1.0pt;border-right:solid #BFBFBF 1.0pt;
  background:white;padding:0cm 5.4pt 0cm 5.4pt;height:1.0cm">

**<span lang="EN-US" style="line-height:115%">2017-10-01</span>**

</td>

</tbody>

</table>

**<span lang="EN-US" style="color:#5B9BD5;text-transform:uppercase;letter-spacing:.25pt">  
</span>****<span style="font-size:18.0pt;line-height:115%;font-family:&quot;Arial&quot;,sans-serif;color:#5B9BD5;text-transform:uppercase">Historia zmian</span>**

<table class="MsoNormalTable" border="1" cellspacing="0" cellpadding="0" width="0" style="width:489.05pt;margin-left:5.4pt;border-collapse:collapse;border:none">

<tbody>

<tr style="height:36.0pt">

<td width="66" style="width:49.65pt;border:solid #BFBFBF 1.0pt;border-right:  none;background:#F3F3F3;padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

**<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif;  color:#8496B0">Wersja</span>**

</td>

<td width="340" style="width:9.0cm;border-top:solid #BFBFBF 1.0pt;border-left:  none;border-bottom:solid #BFBFBF 1.0pt;border-right:none;background:#F3F3F3;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

**<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif;  color:#8496B0">Opis</span>**

</td>

<td width="246" style="width:184.25pt;border:solid #BFBFBF 1.0pt;border-left:  none;background:#F3F3F3;padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

**<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif;  color:#8496B0">Data</span>**

</td>

</tr>

<tr style="height:36.0pt">

<td width="66" style="width:49.65pt;border-top:none;border-left:solid #BFBFBF 1.0pt;
  border-bottom:solid #BFBFBF 1.0pt;border-right:none;background:white;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

**<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">0.1</span>**

</td>

<td width="340" style="width:9.0cm;border:none;border-bottom:solid #BFBFBF 1.0pt;
  background:white;padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">Utworzenie dokumentu</span>

</td>

<td width="246" style="width:184.25pt;border-top:none;border-left:none;
  border-bottom:solid #BFBFBF 1.0pt;border-right:solid #BFBFBF 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">2017-07-27</span>

</td>

</tr>

<tr style="height:36.0pt">

<td width="66" style="width:49.65pt;border-top:none;border-left:solid #BFBFBF 1.0pt;
  border-bottom:solid #BFBFBF 1.0pt;border-right:none;background:white;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

**<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">0.2</span>**

</td>

<td width="340" style="width:9.0cm;border:none;border-bottom:solid #BFBFBF 1.0pt;
  background:white;padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">Wstępne informacje o EmguCV, LBP, Optical Flow, OpenCV</span>

</td>

<td width="246" style="width:184.25pt;border-top:none;border-left:none;
  border-bottom:solid #BFBFBF 1.0pt;border-right:solid #BFBFBF 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">2017-09-18</span>

</td>

</tr>

<tr style="height:36.0pt">

<td width="66" style="width:49.65pt;border-top:none;border-left:solid #BFBFBF 1.0pt;
  border-bottom:solid #BFBFBF 1.0pt;border-right:none;background:white;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

**<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">1.0</span>**

</td>

<td width="340" style="width:9.0cm;border:none;border-bottom:solid #BFBFBF 1.0pt;
  background:white;padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">Uzupełnienie dokumentacji o wyniki eksperymentów z algorytmem LBP</span>

</td>

<td width="246" style="width:184.25pt;border-top:none;border-left:none;
  border-bottom:solid #BFBFBF 1.0pt;border-right:solid #BFBFBF 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">2017-09-20</span>

</td>

</tr>

<tr style="height:36.0pt">

<td width="66" style="width:49.65pt;border-top:none;border-left:solid #BFBFBF 1.0pt;
  border-bottom:solid #BFBFBF 1.0pt;border-right:none;background:white;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

**<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">1.1</span>**

</td>

<td width="340" style="width:9.0cm;border:none;border-bottom:solid #BFBFBF 1.0pt;
  background:white;padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">Dodanie opisu algorytmu Haar</span>

</td>

<td width="246" style="width:184.25pt;border-top:none;border-left:none;
  border-bottom:solid #BFBFBF 1.0pt;border-right:solid #BFBFBF 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">2017-09-21</span>

</td>

</tr>

<tr style="height:36.0pt">

<td width="66" style="width:49.65pt;border-top:none;border-left:solid #BFBFBF 1.0pt;
  border-bottom:solid #BFBFBF 1.0pt;border-right:none;background:white;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

**<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">1.2</span>**

</td>

<td width="340" style="width:9.0cm;border:none;border-bottom:solid #BFBFBF 1.0pt;
  background:white;padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">Opracowanie wyników badań algorytmów rozpoznawania twarzy</span>

</td>

<td width="246" style="width:184.25pt;border-top:none;border-left:none;
  border-bottom:solid #BFBFBF 1.0pt;border-right:solid #BFBFBF 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">2017-09-25</span>

</td>

</tr>

<tr style="height:36.0pt">

<td width="66" style="width:49.65pt;border-top:none;border-left:solid #BFBFBF 1.0pt;
  border-bottom:solid #BFBFBF 1.0pt;border-right:none;background:white;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

**<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">1.3</span>**

</td>

<td width="340" style="width:9.0cm;border:none;border-bottom:solid #BFBFBF 1.0pt;
  background:white;padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">Opracowanie detekcji upadku</span>

</td>

<td width="246" style="width:184.25pt;border-top:none;border-left:none;
  border-bottom:solid #BFBFBF 1.0pt;border-right:solid #BFBFBF 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">2017-09-28</span>

</td>

</tr>

<tr style="height:36.0pt">

<td width="66" style="width:49.65pt;border-top:none;border-left:solid #BFBFBF 1.0pt;
  border-bottom:solid #BFBFBF 1.0pt;border-right:none;background:white;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

**<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">1.4</span>**

</td>

<td width="340" style="width:9.0cm;border:none;border-bottom:solid #BFBFBF 1.0pt;
  background:white;padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">Opracowanie wpływu parametrów algorytmów rozpoznawania twarzy i śledzenia obiektów na ich skuteczność.</span>

</td>

<td width="246" style="width:184.25pt;border-top:none;border-left:none;
  border-bottom:solid #BFBFBF 1.0pt;border-right:solid #BFBFBF 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">2017-09-29</span>

</td>

</tr>

<tr style="height:36.0pt">

<td width="66" style="width:49.65pt;border-top:none;border-left:solid #BFBFBF 1.0pt;
  border-bottom:solid #BFBFBF 1.0pt;border-right:none;background:white;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

**<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">1.5</span>**

</td>

<td width="340" style="width:9.0cm;border:none;border-bottom:solid #BFBFBF 1.0pt;
  background:white;padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">Ukończenie dokumentu</span>

</td>

<td width="246" style="width:184.25pt;border-top:none;border-left:none;
  border-bottom:solid #BFBFBF 1.0pt;border-right:solid #BFBFBF 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:36.0pt">

<span lang="EN-US" style="font-family:&quot;Arial&quot;,sans-serif">2017-10-01</span>

</td>

</tr>

</tbody>

</table>

<span style="font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,sans-serif">  
</span>

<span class="MsoIntenseReference"><span style="font-size:24.0pt;line-height:115%;font-variant:bold !important;text-transform:uppercase">Spis treści</span></span>

<span class="MsoHyperlink">[1.<span style="color:windowtext;text-decoration:none"></span> <span style="background:white">OpenCV</span><span style="color:windowtext;display:none;text-decoration:none">..</span> <span style="color:windowtext;display:none;text-decoration:none">9</span>](#_Toc494697037)</span>

<span class="MsoHyperlink">[1.1.<span style="color:windowtext;text-decoration:none"></span> Wstęp<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">9</span>](#_Toc494697038)</span>

<span class="MsoHyperlink">[1.2.<span style="color:windowtext;text-decoration:none"></span> Historia<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">9</span>](#_Toc494697039)</span>

<span class="MsoHyperlink">[1.3.<span style="color:windowtext;text-decoration:none"></span> Aplikacje<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">10</span>](#_Toc494697040)</span>

<span class="MsoHyperlink">[1.4.<span style="color:windowtext;text-decoration:none"></span> Języki programowania<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">11</span>](#_Toc494697041)</span>

<span class="MsoHyperlink">[2.<span style="color:windowtext;text-decoration:none"></span> EmguCV<span style="color:windowtext;display:none;text-decoration:none">..</span> <span style="color:windowtext;display:none;text-decoration:none">12</span>](#_Toc494697042)</span>

<span class="MsoHyperlink">[2.1.<span style="color:windowtext;text-decoration:none"></span> Wstęp<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">12</span>](#_Toc494697043)</span>

<span class="MsoHyperlink">[2.2.<span style="color:windowtext;text-decoration:none"></span> Zalety<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">12</span>](#_Toc494697044)</span>

<span class="MsoHyperlink">[2.3.<span style="color:windowtext;text-decoration:none"></span> Budowa EmguCV<span style="color:windowtext;display:none;text-decoration:none">..</span> <span style="color:windowtext;display:none;text-decoration:none">13</span>](#_Toc494697045)</span>

<span class="MsoHyperlink">[2.4.<span style="color:windowtext;text-decoration:none"></span> Licencja<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">14</span>](#_Toc494697046)</span>

<span class="MsoHyperlink">[2.5.<span style="color:windowtext;text-decoration:none"></span> Wydajność<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">14</span>](#_Toc494697047)</span>

<span class="MsoHyperlink">[2.6.<span style="color:windowtext;text-decoration:none"></span> Rozpoznawanie krawędzi i konturów<span style="color:windowtext;display:none;text-decoration:none">..</span> <span style="color:windowtext;display:none;text-decoration:none">14</span>](#_Toc494697048)</span>

<span class="MsoHyperlink">[2.7.<span style="color:windowtext;text-decoration:none"></span> Rozpoznawanie twarzy<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">15</span>](#_Toc494697049)</span>

<span class="MsoHyperlink">[2.8.<span style="color:windowtext;text-decoration:none"></span> Algorytmy nauczania maszynowego udostępniane przez EmguCV<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">15</span>](#_Toc494697050)</span>

<span class="MsoHyperlink">[3.<span style="color:windowtext;text-decoration:none"></span> Metody śledzenia/wykrywania ruchu<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">16</span>](#_Toc494697051)</span>

<span class="MsoHyperlink">[3.1.<span style="color:windowtext;text-decoration:none"></span> Metody różnicowe<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">20</span>](#_Toc494697052)</span>

<span class="MsoHyperlink">[3.2.<span style="color:windowtext;text-decoration:none"></span> Metody gradientowe<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">22</span>](#_Toc494697053)</span>

<span class="MsoHyperlink">[3.3.<span style="color:windowtext;text-decoration:none"></span> Metody częstotliwościowe<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">26</span>](#_Toc494697054)</span>

<span class="MsoHyperlink">[3.4.<span style="color:windowtext;text-decoration:none"></span> Metody korelacyjne<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">27</span>](#_Toc494697055)</span>

<span class="MsoHyperlink">[4.<span style="color:windowtext;text-decoration:none"></span> Algorytmy śledzenia rozpoznanych obiektów- wpływ wyboru parametrów na poszczególne algorytmy<span style="color:windowtext;display:none;text-decoration:none"></span> <span style="color:windowtext;display:none;text-decoration:none">33</span>](#_Toc494697056)</span>

<span class="MsoHyperlink">[4.1.<span style="color:windowtext;text-decoration:none"></span> Poszukiwanie informacji <span style="color:windowtext;display:none;text-decoration:none"></span> <span style="color:windowtext;display:none;text-decoration:none">33</span>](#_Toc494697057)</span>

<span class="MsoHyperlink">[4.2.<span style="color:windowtext;text-decoration:none"></span> Optical flow<span style="color:windowtext;display:none;text-decoration:none">..</span> <span style="color:windowtext;display:none;text-decoration:none">33</span>](#_Toc494697058)</span>

<span class="MsoHyperlink">[4.2.1.<span style="color:windowtext;text-decoration:none"></span> Algorytmy Meanshift oraz Camshift<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">34</span>](#_Toc494697059)</span>

<span class="MsoHyperlink">[4.3.<span style="color:windowtext;text-decoration:none"></span> Lucas-Kanade Optical Flow<span style="color:windowtext;display:none;text-decoration:none">..</span> <span style="color:windowtext;display:none;text-decoration:none">37</span>](#_Toc494697060)</span>

<span class="MsoHyperlink">[4.3.1.<span style="color:windowtext;text-decoration:none"></span> Przeprowadzone testy<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">40</span>](#_Toc494697061)</span>

<span class="MsoHyperlink">[4.3.2.<span style="color:windowtext;text-decoration:none"></span> Wyniki dla zmian parametrów Lucas-Kanade<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">40</span>](#_Toc494697062)</span>

<span class="MsoHyperlink">[4.4.<span style="color:windowtext;text-decoration:none"></span> Farneback (Dense) Optical Flow<span style="color:windowtext;display:none;text-decoration:none">..</span> <span style="color:windowtext;display:none;text-decoration:none">46</span>](#_Toc494697063)</span>

<span class="MsoHyperlink">[4.4.1.<span style="color:windowtext;text-decoration:none"></span> Wyniki dla zmian parametrów Farneback<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">48</span>](#_Toc494697064)</span>

<span class="MsoHyperlink">[5.<span style="color:windowtext;text-decoration:none"></span> Algorytmy śledzenia twarzy w płaszczyźnie<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">55</span>](#_Toc494697065)</span>

<span class="MsoHyperlink">[5.1.<span style="color:windowtext;text-decoration:none"></span> Śledzenie oczu<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">55</span>](#_Toc494697066)</span>

<span class="MsoHyperlink">[5.1.1.<span style="color:windowtext;text-decoration:none"></span> Wykrywanie zmian<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">57</span>](#_Toc494697067)</span>

<span class="MsoHyperlink">[5.1.2.<span style="color:windowtext;text-decoration:none"></span> Test symetrii <span style="color:windowtext;display:none;text-decoration:none"></span> <span style="color:windowtext;display:none;text-decoration:none">57</span>](#_Toc494697068)</span>

<span class="MsoHyperlink">[5.1.3.<span style="color:windowtext;text-decoration:none"></span> Śledzenie źrenic<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">58</span>](#_Toc494697069)</span>

<span class="MsoHyperlink">[5.2.<span style="color:windowtext;text-decoration:none"></span> Algorytm Lucas-Kanade<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">61</span>](#_Toc494697070)</span>

<span class="MsoHyperlink">[5.3.<span style="color:windowtext;text-decoration:none"></span> Algorytm Horn-Schunck<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">65</span>](#_Toc494697071)</span>

<span class="MsoHyperlink"><span lang="EN-US">[5.4.<span lang="PL" style="color:windowtext;text-decoration:none"></span> Algorytmy Mean-Shift i Camshift<span lang="PL" style="color:windowtext;display:none;text-decoration:none">.</span> <span lang="PL" style="color:windowtext;display:none;text-decoration:none">67</span>](#_Toc494697072)</span></span>

<span class="MsoHyperlink">[5.5.<span style="color:windowtext;text-decoration:none"></span> Filtr Kalmana<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">70</span>](#_Toc494697073)</span>

<span class="MsoHyperlink">[5.6.<span style="color:windowtext;text-decoration:none"></span> Przykład zastosowania<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">70</span>](#_Toc494697074)</span>

<span class="MsoHyperlink">[5.6.1.<span style="color:windowtext;text-decoration:none"></span> Inicjalizacja<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">70</span>](#_Toc494697075)</span>

<span class="MsoHyperlink">[5.7.<span style="color:windowtext;text-decoration:none"></span> Obserwacje / porównanie algorytmów<span style="color:windowtext;display:none;text-decoration:none">..</span> <span style="color:windowtext;display:none;text-decoration:none">74</span>](#_Toc494697076)</span>

<span class="MsoHyperlink">[5.7.1.<span style="color:windowtext;text-decoration:none"></span> Algorytm CAMSHIFT<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">74</span>](#_Toc494697077)</span>

<span class="MsoHyperlink">[5.7.2.<span style="color:windowtext;text-decoration:none"></span> Algorytm Lucas-Kanade<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">74</span>](#_Toc494697078)</span>

<span class="MsoHyperlink">[5.7.3.<span style="color:windowtext;text-decoration:none"></span> CAMSIFT I Lucas-Kanade<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">75</span>](#_Toc494697079)</span>

<span class="MsoHyperlink">[5.7.4.<span style="color:windowtext;text-decoration:none"></span> Śledzenie źrenic<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">76</span>](#_Toc494697080)</span>

<span class="MsoHyperlink">[5.7.5.<span style="color:windowtext;text-decoration:none"></span> Porównanie algroytmów<span style="color:windowtext;display:none;text-decoration:none">..</span> <span style="color:windowtext;display:none;text-decoration:none">77</span>](#_Toc494697081)</span>

<span class="MsoHyperlink">[6.<span style="color:windowtext;text-decoration:none"></span> Detekcja upadku (kamera statyczna RGB)<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">78</span>](#_Toc494697082)</span>

<span class="MsoHyperlink">[6.1.<span style="color:windowtext;text-decoration:none"></span> Wstęp<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">78</span>](#_Toc494697083)</span>

<span class="MsoHyperlink">[6.2.<span style="color:windowtext;text-decoration:none"></span> Wstępne przetwarzanie sygnału wideo<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">79</span>](#_Toc494697084)</span>

<span class="MsoHyperlink">[6.3.<span style="color:windowtext;text-decoration:none"></span> Często występujące problemy<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">79</span>](#_Toc494697085)</span>

<span class="MsoHyperlink">[6.4.<span style="color:windowtext;text-decoration:none"></span> Ogólny podział metod (kontekst pojedynczej klatki, kontekst sekwencji). <span style="color:windowtext;display:none;text-decoration:none"></span> <span style="color:windowtext;display:none;text-decoration:none">85</span>](#_Toc494697086)</span>

<span class="MsoHyperlink">[6.4.1.<span style="color:windowtext;text-decoration:none"></span> Podział ze względu na rodzaj kamer<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">86</span>](#_Toc494697087)</span>

<span class="MsoHyperlink">[6.4.2.<span style="color:windowtext;text-decoration:none"></span> Podział ze względu na ilość kamer<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">88</span>](#_Toc494697088)</span>

<span class="MsoHyperlink">[6.4.3.<span style="color:windowtext;text-decoration:none"></span> Podział ze względu na umiejscowienie kamer<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">89</span>](#_Toc494697089)</span>

<span class="MsoHyperlink">[6.4.4.<span style="color:windowtext;text-decoration:none"></span> Podział ze względu na rodzaj algorytmu<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">89</span>](#_Toc494697090)</span>

<span class="MsoHyperlink">[6.5.<span style="color:windowtext;text-decoration:none"></span> Analiza poszczególnych algorytmów detekcji upadku<span style="color:windowtext;
display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">90</span>](#_Toc494697091)</span>

<span class="MsoHyperlink">[6.5.1.<span style="color:windowtext;text-decoration:none"></span> Podstawowa estymacja postawy oraz detekcja ruchu obserwowanej osoby. <span style="color:windowtext;display:none;text-decoration:none"></span> <span style="color:windowtext;display:none;text-decoration:none">90</span>](#_Toc494697092)</span>

<span class="MsoHyperlink">[6.5.2.<span style="color:windowtext;text-decoration:none"></span> Śledzenie pozycji głowy. <span style="color:windowtext;display:none;text-decoration:none"></span> <span style="color:windowtext;display:none;text-decoration:none">108</span>](#_Toc494697093)</span>

<span class="MsoHyperlink">[6.5.3.<span style="color:windowtext;text-decoration:none"></span> Estymacja postawy za środka ciężkości sylwetki (centroidu) oraz VPS (Vertical Projection Histogram). <span style="color:windowtext;display:none;text-decoration:none"></span> <span style="color:windowtext;display:none;text-decoration:none">128</span>](#_Toc494697094)</span>

<span class="MsoHyperlink">[6.5.4.<span style="color:windowtext;text-decoration:none"></span> Analiza aktywności i strefy pomieszczeń. <span style="color:windowtext;display:none;text-decoration:none"></span> <span style="color:windowtext;display:none;text-decoration:none">136</span>](#_Toc494697095)</span>

<span class="MsoHyperlink">[6.5.5.<span style="color:windowtext;text-decoration:none"></span> Analiza ruchu po upadku<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">145</span>](#_Toc494697096)</span>

<span class="MsoHyperlink">[6.5.6.<span style="color:windowtext;text-decoration:none"></span> Ukryte modele markowa korzystające z wideo i audio. <span style="color:windowtext;display:none;text-decoration:none"></span> <span style="color:windowtext;display:none;text-decoration:none">148</span>](#_Toc494697097)</span>

<span class="MsoHyperlink">[6.5.7.<span style="color:windowtext;text-decoration:none"></span> PCANet + SVM<span style="color:windowtext;display:none;text-decoration:none">...</span> <span style="color:windowtext;display:none;text-decoration:none">155</span>](#_Toc494697098)</span>

<span class="MsoHyperlink">[6.5.8.<span style="color:windowtext;text-decoration:none"></span> Spatial temporal interest points. <span style="color:windowtext;display:none;text-decoration:none"></span> <span style="color:windowtext;display:none;text-decoration:none">162</span>](#_Toc494697099)</span>

<span class="MsoHyperlink">[6.5.9.<span style="color:windowtext;text-decoration:none"></span> Pictorial Structures for Articulated Pose Estimation<span style="color:windowtext;
display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">173</span>](#_Toc494697100)</span>

<span class="MsoHyperlink">[6.5.10.<span style="color:windowtext;text-decoration:none"></span> Wnioskowanie rozmyte bazujące na upadku i czasu nieaktywności osoby. <span style="color:windowtext;
display:none;text-decoration:none"></span> <span style="color:windowtext;display:none;text-decoration:none">178</span>](#_Toc494697101)</span>

<span class="MsoHyperlink"><span lang="EN-US">[7.<span lang="PL" style="color:windowtext;text-decoration:none"></span> LBP<span lang="PL" style="color:windowtext;display:none;text-decoration:none">.</span> <span lang="PL" style="color:windowtext;display:none;text-decoration:none">183</span>](#_Toc494697102)</span></span>

<span class="MsoHyperlink"><span lang="EN-US">[7.1.<span lang="PL" style="color:windowtext;text-decoration:none"></span> Wstęp<span lang="PL" style="color:windowtext;display:none;text-decoration:none">.</span> <span lang="PL" style="color:windowtext;display:none;text-decoration:none">183</span>](#_Toc494697103)</span></span>

<span class="MsoHyperlink">[7.2.<span style="color:windowtext;text-decoration:none"></span> Działanie algorytmu<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">183</span>](#_Toc494697104)</span>

<span class="MsoHyperlink">[7.3.<span style="color:windowtext;text-decoration:none"></span> LBP Extended<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">184</span>](#_Toc494697105)</span>

<span class="MsoHyperlink"><span lang="EN-US">[7.4.<span lang="PL" style="color:windowtext;text-decoration:none"></span> Porównywanie histogramów<span lang="PL" style="color:windowtext;
display:none;text-decoration:none">..</span> <span lang="PL" style="color:windowtext;display:none;text-decoration:none">184</span>](#_Toc494697106)</span></span>

<span class="MsoHyperlink">[7.5.<span style="color:windowtext;text-decoration:none"></span> Jednorodność<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">186</span>](#_Toc494697107)</span>

<span class="MsoHyperlink">[7.6.<span style="color:windowtext;text-decoration:none"></span> LBP i rozpoznawanie twarzy<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">186</span>](#_Toc494697108)</span>

<span class="MsoHyperlink">[7.7.<span style="color:windowtext;text-decoration:none"></span> Zależność efektywności wykrywania obrazu od kolejnych parametrów<span style="color:windowtext;
display:none;text-decoration:none">..</span> <span style="color:windowtext;display:none;text-decoration:none">188</span>](#_Toc494697109)</span>

<span class="MsoHyperlink"><span style="background:white">[7.8.<span style="color:windowtext;background:windowtext;text-decoration:none"></span> Analiza parametrów dla LBP<span style="color:windowtext;display:none;background:windowtext;text-decoration:none">.</span> <span style="color:windowtext;display:none;background:windowtext;text-decoration:none">198</span>](#_Toc494697110)</span></span>

<span class="MsoHyperlink">[8.<span style="color:windowtext;text-decoration:none"></span> Porównanie algorytmów LBP i Haar<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">202</span>](#_Toc494697111)</span>

<span class="MsoHyperlink">[9.<span style="color:windowtext;text-decoration:none"></span> Wstępne przetwarzanie obrazu dla algorytmów Eigenfaces oraz Fisherfaces<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">204</span>](#_Toc494697112)</span>

<span class="MsoHyperlink">[9.1.<span style="color:windowtext;text-decoration:none"></span> Obróbka obrazu dla detekcji twarzy<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">204</span>](#_Toc494697113)</span>

<span class="MsoHyperlink">[9.2.<span style="color:windowtext;text-decoration:none"></span> Rozpoznawanie twarzy<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">208</span>](#_Toc494697114)</span>

<span class="MsoHyperlink">[9.3.<span style="color:windowtext;text-decoration:none"></span> Przetwarzanie wykrytej twarzy<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">210</span>](#_Toc494697115)</span>

<span class="MsoHyperlink">[9.4.<span style="color:windowtext;text-decoration:none"></span> Wykrywanie obszaru oczu<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">211</span>](#_Toc494697116)</span>

<span class="MsoHyperlink">[9.5.<span style="color:windowtext;text-decoration:none"></span> Kombinacja transformacji <span style="color:windowtext;display:none;text-decoration:none"></span> <span style="color:windowtext;display:none;text-decoration:none">214</span>](#_Toc494697117)</span>

<span class="MsoHyperlink">[9.6.<span style="color:windowtext;text-decoration:none"></span> Oddzielne wyrównywanie histogramów dla prawej oraz lewej strony<span style="color:windowtext;
display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">217</span>](#_Toc494697118)</span>

<span class="MsoHyperlink">[9.7.<span style="color:windowtext;text-decoration:none"></span> Wygładzanie<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">220</span>](#_Toc494697119)</span>

<span class="MsoHyperlink">[9.8.<span style="color:windowtext;text-decoration:none"></span> Maska eliptyczna<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">221</span>](#_Toc494697120)</span>

<span class="MsoHyperlink">[10.<span style="color:windowtext;text-decoration:none"></span> Wykorzystanie sieci neuronowych w rozpoznawaniu twarzy<span style="color:windowtext;
display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">223</span>](#_Toc494697121)</span>

<span class="MsoHyperlink">[10.1.<span style="color:windowtext;text-decoration:none"></span> Wstęp<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">223</span>](#_Toc494697122)</span>

<span class="MsoHyperlink">[10.2.<span style="color:windowtext;text-decoration:none"></span> Typy sieci neuronowych w rozpoznawaniu twarzy<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">225</span>](#_Toc494697123)</span>

<span class="MsoHyperlink">[10.3.<span style="color:windowtext;text-decoration:none"></span> Zastosowanie<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">226</span>](#_Toc494697124)</span>

<span class="MsoHyperlink">[11.<span style="color:windowtext;text-decoration:none"></span> Łączenie klasyfikatorów detekcji twarzy w celu polepszenia wyników<span style="color:windowtext;display:none;text-decoration:none">..</span> <span style="color:windowtext;display:none;text-decoration:none">229</span>](#_Toc494697125)</span>

<span class="MsoHyperlink">[11.1.<span style="color:windowtext;text-decoration:none"></span> Framework<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">229</span>](#_Toc494697126)</span>

<span class="MsoHyperlink">[11.2.<span style="color:windowtext;text-decoration:none"></span> Rezultaty eksperymentalne<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">231</span>](#_Toc494697127)</span>

<span class="MsoHyperlink">[11.3.<span style="color:windowtext;text-decoration:none"></span> Spostrzeżenia<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">236</span>](#_Toc494697128)</span>

<span class="MsoHyperlink">[12.<span style="color:windowtext;text-decoration:none"></span> Wpływ niskiej rozdzielczości na jakość detekcji i rozpoznawania twarzy<span style="color:
windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">242</span>](#_Toc494697129)</span>

<span class="MsoHyperlink">[12.1.<span style="color:windowtext;text-decoration:none"></span> Biometryczne standardy rozpoznawania twarzy odnoszące się do CCTV<span style="color:windowtext;
display:none;text-decoration:none">..</span> <span style="color:windowtext;display:none;text-decoration:none">242</span>](#_Toc494697130)</span>

<span class="MsoHyperlink">[12.2.<span style="color:windowtext;text-decoration:none"></span> Wpływ na detekcję twarzy<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">246</span>](#_Toc494697131)</span>

<span class="MsoHyperlink">[12.3.<span style="color:windowtext;text-decoration:none"></span> Doświadczalny wpływ rozdzielczości na proces detekcji twarzy<span style="color:windowtext;
display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">250</span>](#_Toc494697132)</span>

<span class="MsoHyperlink">[12.4.<span style="color:windowtext;text-decoration:none"></span> Badanie wpływu źródła światła na detekcję twarzy<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">251</span>](#_Toc494697133)</span>

<span class="MsoHyperlink">[12.5.<span style="color:windowtext;text-decoration:none"></span> Rozpoznawanie twarzy<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">256</span>](#_Toc494697134)</span>

<span class="MsoHyperlink">[12.6.<span style="color:windowtext;text-decoration:none"></span> Wpływ rozdzielczości obrazu na rozpoznawanie twarzy<span style="color:windowtext;
display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">258</span>](#_Toc494697135)</span>

<span class="MsoHyperlink">[13.<span style="color:windowtext;text-decoration:none"></span> Algorytmy detekcji źródła i kąta padania światła w kontekście rozpoznawania twarzy<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">265</span>](#_Toc494697136)</span>

<span class="MsoHyperlink">[13.1.<span style="color:windowtext;text-decoration:none"></span> Poszukiwanie informacji <span style="color:windowtext;display:none;text-decoration:none"></span> <span style="color:windowtext;display:none;text-decoration:none">265</span>](#_Toc494697137)</span>

<span class="MsoHyperlink">[13.2.<span style="color:windowtext;text-decoration:none"></span> Jak sprawić, by oświetlenie nie wpływało negatywnie na działanie algorytmów rozpoznawania twarzy?<span style="color:windowtext;display:none;text-decoration:none"></span> <span style="color:windowtext;display:none;text-decoration:none">265</span>](#_Toc494697138)</span>

<span class="MsoHyperlink">[13.3.<span style="color:windowtext;text-decoration:none"></span> Normalizacja światła<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">266</span>](#_Toc494697139)</span>

<span class="MsoHyperlink">[13.4.<span style="color:windowtext;text-decoration:none"></span> Optymalny kąt padania światła<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">275</span>](#_Toc494697140)</span>

<span class="MsoHyperlink">[13.5.<span style="color:windowtext;text-decoration:none"></span> Zwiększanie skuteczności algorytmu przez dobór oświetlenia<span style="color:windowtext;
display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">277</span>](#_Toc494697141)</span>

<span class="MsoHyperlink">[14.<span style="color:windowtext;text-decoration:none"></span> Literatura<span style="color:windowtext;display:none;text-decoration:none">.</span> <span style="color:windowtext;display:none;text-decoration:none">281</span>](#_Toc494697142)</span>

# <a name="_Toc494697037">1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> <span style="background:white">OpenCV</span></a>

## <a name="_Toc494697038">1.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wstęp</a>

OpenCV (ang. Open Source Computer Vision Library) - to biblioteka funkcji, przeznaczonych głównie do przetwarzania obrazu w czasie rzeczywistym. Początkowo opracowana przez firmę Intel, później wspierana przez Willow Garage, a teraz prowadzona przez Itseez. Biblioteka jest multiplatformowa i dopuszczona do wolnego użytku na licencji BSD, dlatego jest bezpłatna do celów akademickich, jak i komercyjnych. <span lang="EN-US">OpenCV wspiera frameworki Deep Learning, TensorFlow, Torch/PyTorch i Caffe.</span>

Biblioteka posiada interfejsy do języków: C++, C, Java i Python oraz obsługuje systemy Windows, Linux, Mac OS, iOS i Android. Napisana jest w języku C/C++, co pozwala korzystać z przetwarzania wielordzeniowego.

## <a name="_Toc494697039">1.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Historia</a>

Oficjalnie uruchomiony w 1999 roku projekt OpenCV był początkowo inicjatywą Intel Research w celu przyspieszenia aplikacji wymagających dużej ilości CPU.

Głównymi jego celami były:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Zaawansowane badania wizualne, które zapewnią nie tylko otwarty, ale także zoptymalizowany kod, dla podstawowej infrastruktury wizualnej.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Propagowanie wiedzy na temat rozpoznawania obrazu poprzez udostępnianie wspólnej infrastruktury, którą programiści mogliby wykorzystać, dzięki czemu kod byłby bardziej czytelny i przenośny.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Zaawansowane aplikacje komercyjne oparte na przetwarzaniu obrazu, dzięki udostępnieniu bezpłatnego, przenośnego kodu, zoptymalizowanego pod kątem wydajności.

Pierwsza wersja alfa OpenCv została udostępniona opinii publicznej w 2000 roku podczas konferencji IEEE Conference on Computer Vision and Pattern Recognition. Kolejne pięć wersji beta powstały w latach 2001 - 2005\. Wersja 1.0 została wydana w 2006 roku. Wersja 1.1 "pre-release" w 2008 r.

W październiku 2009 roku wydano drugą wersję biblioteki OpenCV, która zawierała w sobie duże zmiany dotyczące interfejsu w C++, które miały na celu uproszczenia go, usprawnienie i dodanie nowych funkcji.

W sierpniu 2012 roku wsparcie dla OpenCV zostało przejęte przez organizację non-profit OpenCV.org.

## <a name="_Toc494697040">1.3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Aplikacje</a>

Obszary zastosowań OpenCV obejmują:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Zestawy narzędzi 2D i 3D

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Oszacowanie egomotyczne

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>System rozpoznawania twarzy

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Rozpoznawanie gestów

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Interakcja człowiek-komputer

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Robotyka mobilna

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Zrozumienie ruchu

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Identyfikacja obiektu

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Segmentacja i rozpoznawanie

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Stereofis stereofoniczny wizji: percepcja głębokości z dwóch kamer

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Struktura z ruchu

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Rozszerzona rzeczywistość

Aby wspierać niektóre z powyższych obszarów, OpenCV ma w sobie statystyczną bibliotekę nauczania maszynowego, która zawiera:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Nauka drzewa decyzyjnego

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Gradient pobudzający drzewa

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Algorytm maksymalizacji oczekiwania k-najbliższy algorytm sąsiedztwa

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Klasyfikator Naive Bayes

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Sztuczne sieci neuronowe

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Losowy las

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Maszyna wektora nośnego

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Głębokie sieci neuronowe

## <a name="_Toc494697041">1.4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Języki programowania</a>

OpenCV jest napisane w języku C++, który jest równocześnie jego głównym interfejsem, lecz wciąż zachowuje mniej wszechstronny, ale szeroki interfejs w języku C. Istnieją powiązania z językiem Python, Java i MATLAB/OCTAVE. API tych interfejsów może być znalezione w dokumentacji online. Wrappery do innych języków takich jak C#, Perl, Ch, Haskell i Ruby zostały opracowane w celu zachęcenia szerszej publiczności. Wszystkie nowe algorytmy oraz usprawnienia w OpenCV są teraz rozwijane w interfejsie C++.

# <a name="_Toc494697042">2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> EmguCV</a>

## <a name="_Toc494697043">2.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wstęp</a>

Emgu CV to platforma cross-platform .Net do biblioteki przetwarzania obrazu OpenCV. Dzięki niej możemy przetwarzać i analizować statyczne i ruchome obrazy oraz inne sygnały cyfrowe.

Korzystając z takiego wrappera jak Emgu CV możliwe jest wykorzystanie bardzo zoptymalizowanej biblioteki bezpośrednio z C#, bez konieczności schodzenia do niskopoziomowej implementacji w C.

Do przykładowych zastosowań Emgu CV należą aplikacje do sprawdzania jakości produktów w fabrykach, medyczne przetwarzanie obrazów, kalibracje kamer i zastosowania w szeroko pojętej robotyce. Poza przetwarzaniem obrazu wrapper również pozwala korzystać z biblioteki nauczania maszynowego pochodzącej z OpenCV. (“Emgu CV Essentials”, Shin Shi, 2013)

Pakiet może być kompilowany przez Visual Studio, Xamarin Studio i Unity, może działać na systemach Windows, Linux, Mac OS X, iOS, Android i Windows Phone.

## <a name="_Toc494697044">2.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Zalety</a>

Główne zalety platformy:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Emgu CV jest w całości napisany w C#. Korzyścią jest to, że można go skompilować w Mono, a zatem może działać na dowolnych platformach Mono, w tym iOS, Android, Windows Phone, Mac OS X i Linux.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Image Class z ogólnym kolorem i głębokością

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Automatyczne gromadzenie śmieci.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>XML Serializable Image

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Dokumentacja XML i pomoc techniczna

## <a name="_Toc494697045">2.3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Budowa EmguCV</a>

Emgu CV ma dwie główne warstwy, jak pokazano na schemacie:

![](readme_pliki/image002.png)

Ogólny zarys architektury

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Warstwa podstawowa (layer 1) zawiera odwzorowania funkcji, struktur i wyliczeń, które bezpośrednio odzwierciedlają te w OpenCV

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Druga warstwa (layer 2) zawiera klasy łączące zalety świata .NET

## <a name="_Toc494697046">2.4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Licencja</a>

EmguCV oparta jest na licencji GPLv3, co znaczy, że produkt oparty na niej musi być open-source i darmowy. Możliwe jest uzyskanie licencji komercyjnej po umieszczeniu opłaty. (“Emgu CV Essentials”, Shin Shi, 2013).

## <a name="_Toc494697047">2.5.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wydajność</a>

Według (“Emgu CV Essentials”, Shin Shi, 2013) na podstawie prostych operacji na obrazach czarno-białych Emgu CV zajmuje trzecie miejsce pod względem szybkości przetwarzania (po bibliotece OpenCV napisanej w C oraz trudnej metody wywoływania jej za pomocą P/Invoke w C#).

## <a name="_Toc494697048">2.6.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Rozpoznawanie krawędzi i konturów</a>

W EmguCV możliwe jest korzystanie z detektora Canny Edge w celu rozpoznawania krawędzi. Poniższy kod z książki (“Emgu CV Essentials”, Shin Shi, 2013) pozwoli na stworzenie prostego czarno-białego outline’u obrazu.

![](readme_pliki/image003.png)

Zastosowanie algorytmu wykrywania krawędzi Canny

Po wykryciu krawędzi w formie osobnych pikseli możliwe jest też ich połączenie w kontury. Odpowiada za to funkcja FindCountours(). Wynik tej funkcji może zostać przetransformowany w łańcuch Freemana, czyli sekwencji kolejnych kroków wartości. 

Dodatkową funkcjonalnością EmguCV jest możliwość konwertowania konturów do polygonów.

## <a name="_Toc494697049">2.7.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Rozpoznawanie twarzy</a>

Proces rozpoznawania twarzy składa się z Preprocessingu obrazu lub wideo, detekcji twarzy, rozpoznawania cech charakterystycznych oraz dopasowywania ich do danych z bazy.

Ważną częścią rozpoznawania twarzy są algorytmy nauczania maszynowego. Są one w stanie na podstawie obrazów treningowych nauczyć się rozróżniać pomiędzy obrazami zawierającymi twarze a inne obiekty.

## <a name="_Toc494697050">2.8.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Algorytmy nauczania maszynowego udostępniane przez EmguCV</a>

<span lang="EN-US" style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">ANN_MLP</span>

<span lang="EN-US" style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Boost</span>

<span lang="EN-US" style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">DTree</span>

<span lang="EN-US" style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">EM</span>

<span lang="EN-US" style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">GBTrees</span>

<span lang="EN-US" style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">KNearest</span>

<span lang="EN-US" style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">NormalBayesClassifier</span>

<span lang="EN-US" style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">HaarCascade</span>

<span lang="EN-US" style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">CascadeClassifer</span>

<span lang="EN-US" style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Facerecognizer</span>

# <a name="_Toc494697051">3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Metody śledzenia/wykrywania ruchu</a>

Podstawowym zagadnieniem przy parametryzacji ruchu jest wyznaczanie wektora ruchu pomiędzy dwoma następującymi po sobie obrazami z sekwencji. W celu opisania tego procesu, warto zwrócić uwagę na następujące terminy.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>pole ruchu (ang. motion field) - pojęcie czysto geometryczne, definiowane jako zrzutowanie przestrzeni rzeczywistych trójwymiarowych wektorów na płaszczyznę obrazu. Otrzymane pole wektorowe zawiera informacje o składowej wektora ruchu obiektu równoległej do płaszczyzny obrazu. Brak informacji o składowej prostopadłej powoduje, ˙ze nie jest możliwe odtworzenie z niego pierwotnej informacji - rzeczywistego wektora ruchu.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>przepływ optyczny (ang. optical flow) - jest to pole wektorowe zawierające informacje umożliwiające przekształcenie jednego obrazu w drugi poprzez wykonanie transformacji przemieszczającej obszary z pierwszego obrazu zgodnie z wektorem przepływu optycznego do drugiego obrazu. A zatem przepływ optyczny nie jest ściśle zdeterminowany, a co za tym idzie, możliwe jest znalezienie wielu pól wektorowych umożliwiających transformacje obrazu pierwszego w drugi. Przykładową interpretację przedstawiono na rysunku.

![](readme_pliki/image004.png)

Róznica pomiędzy polem ruchu, a przepływem optycznym

Wydaje się, że powyższe definicje są jednakowe, w rzeczywistości istnieje znacząca różnica pomiędzy nimi. Mianowicie: pole ruchu mówi o rzeczywistym ruchu obiektów, natomiast przepływ optyczny jest interpretacją zmieniającego się obrazu.

Można jednak przy pewnych założeniach przyjąć, że pole przepływu jest aproksymacją pola ruchu, który jest wygodnym i łatwym w interpretacji sposobem reprezentacji ruchu na obrazie. Dzisiejsze procesory są w stanie bez większych problemów analizować obrazy nadchodzące ze standardowych kamer nawet w czasie rzeczywistym, zapewniając użytkownikowi możliwość efektywnego wykorzystania skomplikowanych algorytmów obróbki i analizy obrazu.

Nie zmienia to faktu, że metody estymacji przepływu optycznego obarczone są różnego rodzaju problemami Do najważniejszych z nich należą:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>problem szczelinowy _<span style="font-size:11.5pt;line-height:
150%">(ang. aperture problem)</span> __<span style="font-size:3.0pt;line-height:150%">-</span> _problem błędnego określenia ruchu w <span style="letter-spacing:-.45pt">przypadku lokalnych zmian jasności obrazu, które mają charakter jednowymiarowy.</span>

<span style="font-family:Symbol;letter-spacing:-.45pt">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="letter-spacing:-.45pt">jednolita barwa lub przeźroczystość - brak tekstury może skutkować błędnym określe</span><span style="letter-spacing:-.4pt">niem wektora przepływu, gdyż na obrazie ruch takiego obiektu może być niezauważalny,</span> <span style="letter-spacing:
-.2pt">np. obrót kuli o jednolitym ubarwieniu;</span>

<span style="font-family:Symbol;letter-spacing:-.5pt">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="letter-spacing:-.5pt">zmienne oświetlenie - może być powodem powstania pozornych ruchów związanych z</span> <span style="letter-spacing:
-.45pt">przesuwaniem się cieni na powierzchniach obiektów, bądź z przemieszczaniem się gra­dientów jasności wykorzystywanych przez znaczną część algorytmów śledzenia ruchu;</span> <span style="letter-spacing:-.35pt">pomocne w takich sytuacjach mogą być techniki kompensacji oświetlenia oraz posługi</span><span style="letter-spacing:-.25pt">wanie się kolorem obrazu w dziedzinie HSV ;</span>

<span style="font-family:Symbol;letter-spacing:-.65pt">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="letter-spacing:-.65pt">zachodzenie obiektów na siebie - powoduje zanikanie, pojawianie się, bądź iluzję zmiany</span> <span style="letter-spacing:-.45pt">obiektów poruszających się na obrazie; w takich przypadkach wyjątkowo trudno jest zna</span><span style="letter-spacing:-.6pt">leźć transformację, gdy poszczególne obszary z pierwszego obrazu nie mają swoich odpo­wiedników na kolejnym obrazie. Prowadzi to do błędnego wskazania wektora przepływu;</span> niemniej jednak taką informację także można wykorzystać do analizy ruchu na obrazie, <span style="letter-spacing:-.4pt">tzn.: przepływ optyczny wyznaczony na krawędzi obszaru zachodzenia obiektów może</span> <span style="letter-spacing:-.45pt">zostać wykorzystany do wyznaczenia kierunku jej translacji i segmentacji sceny na nie</span><span style="letter-spacing:-.25pt">zależnie poruszające się przedmioty;</span>

<span style="font-family:Symbol;letter-spacing:-.5pt">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="letter-spacing:-.5pt">specyficzne teksturowanie - zmiany jasności na obiekcie, które występują wzdłuż tylko</span> <span style="letter-spacing:-.35pt">jednego kierunku mogą prowadzić do utraty informacji o ruchu. Czasami zmiany ta</span><span style="letter-spacing:-.55pt">kie są wywołane specyficznym wzorem, który znajduje się na poruszającym się obiekcie.</span>

![](readme_pliki/image005.jpg)

Problem szczelinowy

Istnieje wiele metod wykrywania i śledzenia ruchu na obrazie. W dalszej części pracy zostaną przedstawione głównie metody zaimplementowane w wykorzystywanej w części praktycznej bibliotece OpenCV. Metody detekcji/śledzenia ruchu możemy podzielić na 4 podstawowe grupy ze względu na sposób działania:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>różnicowe - umożliwiające detekcję i śledzenie ruchu dzięki różnicom pomiędzy następującymi klatkami,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>gradientowe - opierające się na wyznaczaniu pochodnych czasowych i przestrzennych obrazu,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>częstotliwościowe - bazujące na filtrach operujących w dziedzinie częstotliwości,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>korelacyjne - przeszukujące obraz w poszukiwaniu dopasowania regionów.

Pomimo zróżnicowania opisywanych metod można wyróżnić 3 wspólne etapy przetwarzania:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>wstępne filtrowanie lub wygładzanie przy użyciu dolno- lub pasmowo-przepustowych filtrów w celu uwydatnienia interesujących nas sygnałów oraz wzmocnienia stosunku sygnału do szumu (SNR-Signal to Noise Ratio);

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>przeprowadzenie podstawowych pomiarów takich jak: pochodne czasoprzestrzenne lub lokalne powierzchnie korelacji;

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>integracja powyższych danych w celu stworzenia 2-wymiarowego pola przepływu

Wstępna obróbka obrazu okazuje się być bardzo ważną czynnością, pozwalającą znacznie zwiększyć zarówno jakość jak i wydajność algorytmów.

## <a name="_Toc494697052">3.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Metody różnicowe</a>

Pierwszą z metod i zarazem najbardziej podstawową jest metoda różnicowa. Jest to bardzo wydajna i czuła metoda detekcji zmian zachodzących w kolejnych sekwencjach obrazów. Cechy te sprawiły, że stała się bardzo popularna w systemach detekcji (a po rozwinięciu, także śledzenia) ruchu, w przypadkach, gdy korzysta się ze stacjonarnych kamer wykorzystywanych do obserwowania dynamicznej sceny. Metody różnicowe nie skupiają się na wyliczeniu pola przepływu optycznego, a na prostym wyodrębnieniu poruszającego się obiektu.

![](readme_pliki/image006.jpg)

Wynik działania algorytmu różnicowego – białe pola oznaczają wykryte zmiany na obrazie

Zasadniczą wadą algorytmów różnicowych jest ograniczenie ich zastosowania do sekwencji pobranych ze stacjonarnych kamer. W przypadku, gdyby kamera, z której jest pobierany obraz była w ruchu, po odjęciu klatek, okazałoby się, że prawie każdy piksel obrazu uległ zmianie. Zasadniczym rozwinięciem podstawowego algorytmu różnicowego było stworzenie metody budowania obrazu historii ruchu (ang. MHI - Motion History Image). Na obrazie takim intensywność piksela zależy od chwilowego ruchu w tym punkcie, tzn.: poruszający się obiekt pozostawia, w każdej kolejnej klatce, swoją sylwetkę rzutowaną na płaszczyznę (na zasadzie zwykłego algorytmu różnicowego), a z każdą następną klatką jasność starszych sylwetek jest zmniejszana. Przy takim podejściu, gradient obrazu MHI wyznacza bezpośrednio kierunek ruchu, a nawet w pewnym sensie jego prędkość. Reprezentacja ruchu za pomocą obrazu MHI jest bardzo wydajną techniką śledzenia.

![](readme_pliki/image007.png)

Proces tworzenia obrazu historii ruchu - MHI

## <a name="_Toc494697053">3.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Metody gradientowe</a>

Techniki gradientowe są jedną z najbardziej rozwiniętych grup algorytmów wyznaczających przepływ optyczny. Obliczenia te są oparte o pochodne przestrzenno-czasowe intensywności obrazu. Ich zastosowanie zakłada, że dziedzina obrazu jest ciągła (lub różniczkowalna) w przestrzeni i czasie. Algorytmy z dziedziny metod gradientowych opierają się na 3 głównych założeniach:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>stałe oświetlenie - jasność piksela danego obiektu nie zmienia się podczas ewentualnego ruchu pomiędzy kolejnymi klatkami obrazu;

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>niewielkie przesunięcia - w praktyce oznacza to, że obiekty nie poruszają się zbyt szybko na obrazie;

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>spójność przestrzenna - sąsiadujące punkty należą do tych samych powierzchni i wykonują podobne ruchy.

![](readme_pliki/image008.jpg)

Założenia metod gradientowych: stałe oświetlenie (góra), powolny ruch w stosunku do ilości klatek (lewy róg), spójność sąsiadujących punktów (prawy róg)

Założenia te pozwalają stworzyć efektywne algorytmy śledzenia ruchu. Pierwsze z nich zakłada, że jasność śledzonego punktu nie będzie się zmieniała w czasie

![](readme_pliki/image009.jpg)

Niezmienność jasności w czasie

Drugie założenie mówi, że ruch jest mały w następujących po sobie klatkach. Założenie to gwarantuje, że w przypadku jednolitego oświetlenia, pierwsze z nich także będzie spełnione, poza szczególnymi sytuacjami takimi jak zachodzące na siebie krawędzie. Po kolejnych przekształceniach, otrzymujemy:

![](readme_pliki/image010.jpg)

Równanie ograniczenia przepływu optycznego

Równanie to nazywane jest: równaniem ograniczenia przepływu optycznego i definiuje pojedyncze, lokalne ograniczenie na wektor prędkości optycznej. Poniższy rysunek przedstawia prostą zdefiniowaną przez to ograniczenie.

![](readme_pliki/image011.png)

Prosta ograniczająca w przestrzeni prędkości

Wśród metod gradientowych należy wyróżnić dwa podejścia: globalne i lokalne. Pierwsze z nich wykorzystują równanie ograniczające przepływ optyczny wraz z dodatkowymi globalnymi ograniczeniami, zazwyczaj na szybkość zmian prędkości optycznej. W efekcie otrzymuje się tzw. "gęste pole przepływu" określające ruch dla większej ilości punktów (często dla wszystkich punktów obrazu). Metody lokalne opierają się na informacjach dotyczących prędkości normalnych z sąsiedztwa, które wykorzystuje się do minimalizacji określonego funkcjonału pozwalającego znaleźć prędkość optyczną punktu. Ich zaletą jest fakt, że pole przepływu może być obliczane dla niewielkiej liczby punktów z obrazu, a co za tym idzie wymagają znacznie mniejszego nakładu obliczeniowego. W niektórych zastosowaniach można by jednak uznać to za wadę. Warto dodać, że gdy nie jest zapewniony wspomniany na początku rozdziału warunek spójności przestrzennej, metody te dają niedokładne wyniki.

Stosowanie metod gradientowych powoduje konieczność wprowadzenia dodatkowych zabiegów. Wstępne filtrowanie obrazu jest konieczne, aby uniknąć efektu aliasingu, a obliczenia numeryczne muszą być wykonywane z odpowiednią dokładnością. Jakość ich działania często jest uzależniona od wymagań jakie powinny spełniać dane wejściowe:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>zmiany intensywności powinny mieć charakter liniowy,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>prędkości optyczne nie powinny być większe niż 1 piksel na klatkę.

Wymagania te wywodzą się z ograniczenia jakim jest wykorzystanie tylko 2 klatek obrazu, niewłaściwych metod numerycznych różniczkowania lub zakłóconych aliasingiem danych wejściowych. Dlatego też większość pierwotnych metod zostało zmodyfikowanych w celu poprawienia jakości działania a także zwiększenia ich wydajności.

Często stosowaną techniką do poprawy działania algorytmów gradientowych jest dekompozycja hierarchiczna (piramidalna). Polega ona na wykonaniu najpierw obliczeń na pomniejszonym obrazie i późniejszym wykorzystaniu ich w obliczeniach na obrazie o coraz to wyższej rozdzielczości, do obrazu pierwotnego włącznie. Wykorzystanie obrazu o mniejszej skali pozwala zredukować problem aliasingu, a także redukuje prędkości optyczne obiektów, co zwiększa szanse spełnienia warunku na niewielkie przemieszczenia wymagane przez algorytmy gradientowe.

![](readme_pliki/image012.png)

Wykorzystanie dekompozycji hierarchicznej do analizy obrazu

## <a name="_Toc494697054">3.3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Metody częstotliwościowe</a>

Kolejną klasą technik służących do wyliczania pola przepływu optycznego są metody częstotliwościowe oparte na użyciu filtrów częstotliwościowych. Wykorzystują one filtry czułe na kierunek ruchu w przestrzeni fourierowskiej. Główną zaletą takiego podejścia (wykorzystania czułego na ruch mechanizmu operującego na czasowo-przestrzennym rozkładzie energii w przestrzeni fourierowskiej) jest możliwość wykrycia ruchu w sygnale obrazu, który nie byłby wykrywalny dla metod gradientowych czy korelacyjnych. Dla przykładu, ruch wzorów losowo rozmieszczonych punktów mógłby zostać błędnie określony przy użyciu metod gradientowych bądź korelacyjnych, natomiast w przestrzeni Fouriera skutkuje on powstaniem zorientowanej energii, z której można wydobyć informacje umożliwiające wyliczenie przepływu optycznego.

Transformata Fouriera przesuwającego się dwuwymiarowego sygnału intensywności przybiera postać:

![](readme_pliki/image013.jpg)

Równanie 1 Iloczyn transformaty Fouriera funkcji intensywności oraz funkcją delty Diraca

Z transformaty tej wynika równanie:

![](readme_pliki/image014.jpg)

Równanie 2 Równanie ograniczające przepływu optycznego w przestrzeni częstotliwościowej

które pokazuje, że prędkość przesuwającego się dwuwymiarowego wzoru jest funkcją jego czasowo-przestrzennych częstości i tworzy płaszczyznę na obrazie źródłowym w przestrzeni fourierowskiej.

## 3.4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span>  <a name="_Toc494697055">Metody korelacyjne</a>

Następną grupą metod obliczania pola przepływu optycznego są metody korelacyjne. Ich ogól<span style="letter-spacing:-.1pt">nym założeniem jest wyznaczenie przemieszczenia dającego najlepsze dopasowanie regionów</span> obrazów następujących po sobie w czasie. Wcześniejsze metody wymagały specyficznych cech, np.: naroży, aby poprawnie móc wyznaczać przepływ optyczny. Jak łatwo się domyślić <span style="letter-spacing:-.1pt">w rzeczywistym obrazie cechy takie są dość rzadkie, chociażby w porównaniu z krawędziami.</span> Zdarza się, że nawet przy pomocy tych unikalnych elementów obrazu trudno jest wyznaczyć odpowiednie dopasowanie. Do tego wszystkiego dochodzi jeszcze problem zachodzących na siebie obiektów, który może prowadzić do powstania przekłamań w wynikach. Podstawową zaletą metod korelacyjnych jest ich zmniejszona wrażliwość na wymienione powyżej problemy. Ponieważ nie bazują one na występowaniu tych unikatowych cech, nadają się do generowania gęstego pola przepływu optycznego, a dzięki możliwości manipulowania wielkością okna korelacji, mogą być wykorzystane w okolicach zachodzących na siebie obiektów - do śledzenia złożonych ruchów. Występowanie szumów, niewielka liczba klatek albo aliasing występujący w procesie akwizycji obrazu, mogą spowodować, że numeryczne różniczkowanie będzie obarczone sporymi błędami. Dlatego też naturalnym wydaje się być zastosowanie w takich sytuacjach metod opartych o dopasowywanie regionów.

Technika dopasowywania obrazów znacząco rozwinęła się dzięki stereowizji, gdzie głównym zadaniem była korelacja odpowiednich regionów zdjęć, które były zrobione z różnych perspektyw. Przyjmuje się, że przynajmniej lokalnie, zniekształcenia spowodowane różnicą kąta patrzenia są pomijalne. Podstawowym sposobem dopasowania takich regionów jest maksymalizacja zadanego kryterium. W szczególności, współczynnik korelacji pomiędzy dwiema funkcjami _f_ i _g_ można zdefiniować następująco:

![](readme_pliki/image015.jpg)

Współczynnik korelacji

Metody korelacyjne można podzielić na dwie grupy: metody bazujące na dopasowaniu fragmentów obrazu i metody oparte na dopasowaniu pewnych, określonych cech charakterystycznych obrazu.

W metodach z pierwszej grupy, z założenia proces korelacji danego punktu/regionu obrazu polega na odnalezieniu z wykorzystaniem kryterium korelacji, w określonym sąsiedztwie, odpowiadającego mu punktu/regionu na kolejnej ramce. Dodatkowo proces ten opiera się na założeniu, że sąsiednie punkty obrazu, należące do tego samego obiektu, poruszają się w tym samym kierunku, co zostało zilustrowane na rysunkach:

![](readme_pliki/image016.jpg)

Obszar poszukiwania i wektor przemieszczenia dla punktu obrazu

![](readme_pliki/image017.png)

Przyjęcie założenia równego przepływu optycznego dla sąsiadujących punktów

Ważnym parametrem algorytmu jest wielkość okna sąsiedztwa. Jego rozmiar ma zasadniczy wpływ na jakość, wydajność i zakres zastosowań algorytmu. Przy dużym oknie możliwa jest detekcja dużych ruchów, jednak należy pamiętać, że zwiększa to zdecydowanie złożoność obliczeniową, a co za tym idzie zmniejsza wydajność algorytmu. Ustalenie odpowiedniego rozmiaru okna poszukiwań staje się zasadniczym problemem technik korelacyjnych. Warto zauważyć, że zarówno stosowanie dużego obszaru jak i małego może powodować wystąpienie pewnych przekłamań. Dla przykładu: w przypadku małego okna, jeżeli korelowany region nie będzie się mieścił w oknie poszukiwań, zamiast niego zostanie wybrany inny, który będzie maksymalizował współczynnik korelacji -prędkość optyczna zostanie błędnie wyznaczona. Z kolei w przypadku dużego obszaru poszukiwań, jeżeli nasz region uległ pewnym zmianom (np. z uwagi na obrót, skalowanie, lub przesłonięcie) także może się okazać, że maksimum funkcji kryterialnej wypadnie w niewłaściwym punkcie. Ważną modyfikacją w takim przypadku jest zastosowanie funkcji wagowej uwzględniającej zmniejszenie dopasowania wraz z odległością od środka okna poszukiwań. Kryterium korelacji może przybierać rożne formy:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>korelacji bezpośredniej, gdzie wartości odpowiednich punktów w obszarze okna są przez siebie mnożone, a następnie sumowane;

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>korelacji znormalizowanej, gdzie od wartości punktów odejmowana jest średnia wartość intensywności wyliczona dla danego okna, a następnie są one mnożone i sumowane;

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>korelacji znormalizowanej wariancją, gdzie suma iloczynów odpowiednich punktów dzielona jest przez iloczyn wariancji wyliczonych dla obu okien;

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>sumy kwadratów różnic, gdzie sumowane są kwadraty różnicy intensywności odpowiednich punktów dla obszaru okna;

W każdym z tych kryteriów można zastosować modyfikację wagową dla funkcji okna.

Drugą grupą metod korelacyjnych są techniki bazujące na dopasowaniu pewnych cech szczególnych obrazu. Opierając się one na analizie obrazu pod kątem cech wyższego poziomu, do korelacji których wykorzystywany jest opis symboliczny struktur geometrycznych obecnych na obrazie. Zakłada się, że strukturom geometrycznym widocznym na obrazie odpowiadają struktury rzeczywiste w przestrzeni zrzutowane na obraz. Założenie to gwarantuje, że struktury takie posiadają pewne cechy charakterystyczne, łatwe do wydobycia i rozróżnienia. Dodatkowo zakłada się, że cechy te nie zmieniają się podczas ruchu. Przy zastosowaniu właściwego opisu oraz miary tych cech możliwe jest ich odnalezienie i dopasowanie. Do interesujących struktur mogą należeć proste obiekty, np.: punkty czy linie, jak też złożone: krawędzie lub obszary. Niestety czasami wyznaczanie cech skomplikowanych może być obarczone dużymi obciążeniami obliczeniowymi.

Podobnie jak w przypadku metod gradientowych, także tutaj wykorzystanie podejścia hierarchicznego pozwala znacznie zwiększyć dokładność i wydajność algorytmów. Przy dużych prędkościach ruchu konieczne jest użycie zwiększonego obszaru poszukiwań, co z kolei prowadzi do zmniejszenia wydajności i zazwyczaj zwiększenia zawodności metody. Przy wy-korzystaniu dekompozycji obrazów i stworzeniu kilku nowych, o coraz to mniejszych rozmiarach, możliwe jest wychwycenie wszystkich ruchów przy użyciu niewielkiego okna poszukiwań. Dodatkowo wstępne dane uzyskane przy analizie obrazów o niskiej rozdzielczości mogą służyć, jako podstawa dla dalszej pracy algorytmu, przez co w kolejnych poziomach zdekomponowanego obrazu możliwe jest coraz to dokładniejsze obliczenie przesunięcia, aż do otrzymania ostatecznego wyniku na pierwotnym obrazie.

O mnogości zastosowań metod korelacyjnych, ze względu na ich znaczną ilość zalet, może chociażby świadczyć fakt, że algorytmy te znalazły szerokie zastosowanie w kompresji wideo i metodach kodowania. Jedną z najbardziej rozpowszechnionych metod korelacyjnych jest algorytm dopasowywania bloków (ang. BMA - Błock Matching Algorithm)

![](readme_pliki/image018.png)

BMA Zastosowanie bloków o stałej (zdjęcie lewe) oraz zmiennej (prawe zdjęcie) wielkości

# <a name="_Toc494697056"></a><a name="_Toc493851930"></a><a name="_Toc494111832"></a><a name="_Toc494200748"></a><a name="_Toc494315511"><span style="font-family:&quot;Calibri&quot;,sans-serif">4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="font-family:&quot;Calibri&quot;,sans-serif">Algorytmy śledzenia rozpoznanych obiektów</span></a><span style="font-family:&quot;Calibri&quot;,sans-serif">- wpływ wyboru parametrów na poszczególne algorytmy</span>

## <a name="_Toc493851931"></a><a name="_Toc494111833"></a><a name="_Toc494200749"></a><a name="_Toc494315512"></a><a name="_Toc494697057">4.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Poszukiwanie informacji</a>

Rejestracja obrazu video staje się coraz powszechniejsza w dzisiejszym świecie. Kamery pojawiają się na ulicach, w bankach i siedzibach firm, by na bieżąco rejestrować otaczający świat. Ze względu na dużą ilość otrzymanych w ten sposób danych, poszukuje się automatycznych metod analizy. Celem naszego projektu był przegląd zagadnień związanych ze śledzeniem obiektów na obrazie video. Szczególną uwagę poświęciliśmy zagadnieniu przepływu optycznego (Optical Flow) i algorytmowi Lucas-Kanade, który sprawdza przemieszczenia poszczególnych pikseli obrazu.

## <a name="_Toc493595188"></a><a name="_Toc493851932"></a><a name="_Toc494111834"></a><a name="_Toc494200750"></a><a name="_Toc494315513"></a><a name="_Toc494697058">4.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Optical flow</a>

![](readme_pliki/image019.png)

<span style="color:windowtext"> Źródło: Bzdawski M., ,,Śledzenie obiektów w sekwencjach obrazów”</span>

1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Pole wektorowe (gradient) opisujące ruch fragmentów obrazu.

2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Transformacja przekształca obraz wejściowy w wyjściowy.

3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Opiera się na dwóch założeniach:

a.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Jasność punktu nie zmienia się między klatkami.

b.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Sąsiadujące piksele poruszają się z podobną prędkością i w zbliżonym kierunku.

4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Równanie ruchu pikseli rozwinięte w szereg Taylora po przekształceniach daje równanie przepływu optycznego.

5.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Jest to równanie liniowe, podczas gdy wektor prędkości optycznej posiada dwie niewiadome.

Wśród metod wyznaczania przepływu optycznego wyróżnia się trzy główne grupy:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Metody gradientowe (wykorzystujące analizę pochodnych czasowych i przestrzennych obrazu)

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Metody częstotliwościowe (wykorzystujące filtrację częstotliwościową obrazu)

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Metody korelacyjne (dopasowujące odpowiednie fragmenty obrazu).

### <a name="_Toc494200751"></a><a name="_Toc494315514"></a><a name="_Toc494697059"><span lang="EN-US">4.2.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Algorytmy Meanshift oraz Camshift</span></a>

Działanie algorytmu Meanshift oraz zastosowanie tego algorytmu w OpenCV opisano w artykule umieszczonym pod adresem: http://docs.opencv.org/3.2.0/db/df8/tutorial_py_meanshift.html.

Algorytm Meanshift dla danego zbioru punktów ze zdjęcia próbuje znaleźć zbiór o największej gęstości lub wartości punktów na danym obrazie. W OpenCV należy przekazać histogram poszczególnych klatek do algorytmu.

Algorytm CAMshift (Continuously Adaptive Meanshift) dodatkowo dostosowuje rozmiar okna na którym działa (radzi sobie z obiektami, które zbliżają się lub oddalają).

Poniżej znajduje się kod Camshift w OpenCV oraz grafika obrazująca działanie tego algorytmu:

![](readme_pliki/image020.png)

![](readme_pliki/image021.jpg)

<span style="color:windowtext">Działanie algorytmu Camshift. Źródło:</span> <span class="MsoHyperlink">**[http://docs.opencv.org/3.2.0/db/df8/tutorial_py_meanshift.html](http://docs.opencv.org/3.2.0/db/df8/tutorial_py_meanshift.html)**</span><a name="_Toc493595189"></a><a name="_Toc493851933"></a><a name="_Toc494111835"></a>

## <a name="_Toc494200752"></a><a name="_Toc494315515"></a><a name="_Toc494697060">4.3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Lucas-Kanade Optical Flow</a>

Algorytm Lucas-Kanade służy do śledzenia poruszających się obiektów na obrazie video. Zakłada ten sam ruch sąsiedztwa 3x3 poruszających się punktów. Daje to 9 równań z dwiema niewiadomymi – jest to układ nadokreślony.

Lepsze przybliżenia daje metoda najmniejszych kwadratów. Efektem końcowym są dwa równania z dwiema niewiadomymi:

![](readme_pliki/image022.png)

Wykorzystując przedstawiony poniżej kod testowaliśmy wpływ poszczególnych parametrów algorytmu Lucas-Kanade: maxLevel, minDistance, winSize, qualityLevel (…) na skuteczność śledzenia sylwetki człowieka. Do testowania posłużyły nagrane przez nas kilkusekundowe filmy przedstawiające jedną lub kilka osób przechodzących przez pomieszczenie. Kamera została umieszczona w górnym rogu pokoju, aby nasze filmy były zbliżone do nagrań z monitoringu przemysłowego.

![](readme_pliki/image023.jpg)

### <a name="_Toc493851934"></a><a name="_Toc494111836"></a><a name="_Toc494200753"></a><a name="_Toc494315516"></a><a name="_Toc494697061"><span lang="EN-US">4.3.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Przeprowadzone testy</span></a>

Nagrano dwa filmy (<span style="font-family:&quot;Tw Cen MT&quot;,sans-serif">VID_20170918_133943.mp4, VID_20170918_134045.mp4</span>) przedstawiające sylwetki chodzących ludzi z kamery pod sufitem w rogu. Jeden z filmów przy włączonym świetle drugi przy wyłączonym, edytowano krótkie 10-15 sekundowe wycinki filmów na których testowano wpływ zmiany parametrów. Wygenerowano filmy porównujące wyniki dla 4 wartości każdego parametru. Poniżej kadry z filmów porównawczych.

### <a name="_Toc493851935"></a><a name="_Toc494111837"></a><a name="_Toc494200754"></a><a name="_Toc494315517"></a><a name="_Toc494697062">4.3.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wyniki dla zmian parametrów Lucas-Kanade</a>

1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Shi-Tomasi dla pierwszej klatki, algorytm do wykrywania wyraźnych narożników

a.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> qualityLevel - im większa wartość parametru tym program mniej punktów znajduje

![](readme_pliki/image024.jpg)

<span style="color:windowtext"> Wartości qualityLevel od lewej: a) 0,1 b) 0.3 c) 0.5 d) 0.7</span>

<span style="line-height:150%">b.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="line-height:150%">minDistance -</span> jest to minimalna odległość euklidesowa pomiędzy wykrytymi punktami. Jeżeli jest zbyt duży niektóre mogą zostać pominięte.

c.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> maxCorners – maksymalna liczba narożników, jeśli znaleziono więcej zostaną odrzucone

![](readme_pliki/image025.jpg)

<span style="color:windowtext">Wartości minDistance od lewej: a) 45 b) 100\. Na dole wartości minDistance=5 i maxCorners od lewej: a) 100 b) 30</span>

d.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> blockSize – wielkość bloku uśredniania do obliczania pochodnej matrycy kowariancji dla każdego piksela sąsiedztwa

![](readme_pliki/image026.jpg)

<span style="color:windowtext">Wartości blockSize od lewej: a) 2 b) 15 c) 50 d) 70</span>

<span style="font-size:12.0pt;line-height:115%"> </span>

<span lang="EN-US">2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Lucas <span class="ParagraphChar">Kanade</span>

a.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> winSize - wielkość okna uśredniania, większe wartości uodparniają algorytm na szumy i dają szansę na wykrycie szybkiego ruchu; ale dają bardziej rozmyte pole ruchu

![](readme_pliki/image027.jpg)

<span style="color:windowtext">Wartości minSize od lewej: a) 3 i 3 b) 15 i 15 c) 60 i 60 d) 240 i 240</span>

b.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> maxLevel - liczba użytych piramid obrazu, 0 = brak, 1 = dwie, itd.

![](readme_pliki/image028.jpg)

<span style="color:windowtext"> Wartości maxLevel od lewej: a) 0 b) 2 c) 4 d) 12</span>

c.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Criteria - parametry definiujące kiedy zakończyć iteracyjny algorytm przeszukiwania. Definiuje maksimum iteracji oraz kiedy okno przeszukiwania przesuwa się o mniej niż ostatni parametr. W jakikolwiek zauważalny sposób wpływają tylko na zachowanie znalezionych punktów w przypadku przenikania się dwóch obiektów na których zostały wyszczególnione śledzone punkty.

![criteria analiza koniec](readme_pliki/image029.png)

<span style="color:windowtext">Wartości Criteria max i przesunięcie od lewej: a) 10 i 3 b) 10 i 0.3 c) 10 i 0.003 d) 100 i 0.003</span>

## <a name="_Toc493595190"></a><a name="_Toc493851936"></a><a name="_Toc494111838"></a><a name="_Toc494200755"></a><a name="_Toc494315518"></a><a name="_Toc494697063">4.4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Farneback (Dense) Optical Flow</a>

Algorytm Lucas-Kanade działa dla rzadkich (sparse) zbiorów cech charakterystycznych. OpenCV dostarcza algorytm dla gęstego Optical Flow, obliczający przepływ optyczny dla wszystkich punktów w klatce. <span lang="EN-US">Jest on opisany w pracy Gunnera Farnebacka</span> <span lang="EN-US" style="color:black">"Two-Frame Motion Estimation Based on Polynomial Expansion".</span>

Użyto tych samych filmów co w  oświadczeniu z algorytmem Lucas-Kanade. Wygenerowano filmy porównujące wyniki dla 4 wartości każdego parametru. Poniżej kadry z filmów porównawczych.

<a name="_Toc493851938"></a><a name="_Toc494111840"><span style="font-size:12.0pt;line-height:150%"> </span></a>

<span style="font-size:12.0pt;line-height:150%"> </span>

<span style="font-size:12.0pt;line-height:150%"> </span>

<span style="font-size:12.0pt;line-height:150%"> </span>

![](readme_pliki/image030.png)  

### <a name="_Toc494200756"></a><a name="_Toc494315519"></a><a name="_Toc494697064"><span lang="EN-US">4.4.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Wyniki dla zmian parametrów Farneback</span></a>

#### pyr_scale

<span class="ParagraphChar"><span style="font-size:12.0pt;line-height:
150%">Parametr skali do budowania piramid obrazu. 0.5 oznacza klasyczne piramidy – każda jest o połowę mniejsza od poprzedniej. Na filmie nie widać różnicy między wartościami od 0.001 do 0.75\. Różnica w szybkości działania jest zauważalna.</span></span>![](readme_pliki/image031.jpg)

<span style="color:windowtext"> Wartości pyr_scale od lewej: a) 0,001 b) 0,25 c) 0,5 d) 0,75</span>

<span style="font-size:12.0pt;line-height:115%;font-family:&quot;Calibri&quot;,sans-serif">  
</span>

<span style="font-size:12.0pt;line-height:115%"> </span>

#### **levels**

Ilość użytych piramid, 1 = brak piramid.

![](readme_pliki/image032.jpg)

<span style="color:windowtext">Wartości levels od lewej: a) 3 b) 1 c) 6 d) 20</span>

![](readme_pliki/image033.jpg)

<span style="color:windowtext">Wartości pyr_scale i level od lewej: a) 0,5 i 20 b) 0,00000001 i 20 c) 0.9 i 20 d) 0 i 1</span>

<span style="font-size:12.0pt;line-height:115%"> </span>

#### winsize

<span class="ParagraphChar"><span style="font-size:12.0pt;line-height:150%">Wielkość okna uśredniania, większe wartości uodparniają algorytm na szumy i dają szansę na wykrycie szybkiego ruchu; ale dają bardziej rozmyte pole ruchu  
</span></span>![](readme_pliki/image034.jpg)

<span style="font-size:12.0pt;line-height:150%"> </span>

#### iterations

<span class="ParagraphChar"><span style="font-size:12.0pt;line-height:
150%">Ilość iteracji algorytmu dla każdej piramidy.  
</span></span>![](readme_pliki/image035.jpg)

Wartości iterations od lewej: a) 3 b) 10 c) 80 d) 120

#### poly_n

<span style="font-size:12.0pt;line-height:150%">Rozmiar sąsiedztwa piksela użytego do znajdowania rozwinięcia wielomianowego w każdym pikselu, większe wartości oznaczają że obraz będzie przybliżany bardziej łagodnymi powierzchniami</span>

![](readme_pliki/image036.jpg)

<span style="color:windowtext"> Wartości poly_n  od lewej: a) 1 b) 5 c) 10 d) 50</span>

#### poly_sigma

<span class="ParagraphChar"><span style="font-size:12.0pt;line-height:
150%">Odchylenie standardowe funkcji Gaussa, która jest używana do wygładzania pochodnych bazowych dla rozwinięcia wielomianowego.</span></span>  
![](readme_pliki/image037.jpg)

<span style="color:windowtext"> Wartości poly_sigma od lewej: a) 1,2 b) 3 c) 25 d) 100</span>

# <a name="_Toc494697065">5.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Algorytmy śledzenia twarzy w płaszczyźnie</a>

## <a name="_Toc494697066">5.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Śledzenie oczu</a>

Jednym z proponowanych algorytmów jest metoda oparta na wykrywaniu i śledzeniu oczu. Wstępne odnajdywanie oczu odbywa się przez wykrywanie mrugnięć. Dalej śledzony jest obszar obrazu znajdujący się między oczami, a następnie odnajdywane są źrenice w celu doprecyzowania wyniku śledzenia.

Wygląd oczu jest dość charakterystyczny, duże różnice w jasności między białkiem a źrenicami pozwalają na dość precyzyjne śledzenie oczu pomiędzy klatkami. Z kolei zjawisko mrugania umożliwia odnajdywanie oczu nawet bez wcześniejszej wiedzy o położeniu i rozmiarze twarzy. Mrugnięcia objawiają się nagłą zmianą w dwóch obszarach obrazu naraz. Występują one też na tyle często, iż – nawet utraciwszy śledzoną twarz – można ją szybko odnaleźć ponownie.

Algorytm wykrywania mrugnięć opiera się na wykryciu obszarów zmian w analizowanym obrazie, które mają pożądane cechy i spełniają zestaw warunków. Poniżej opisane są kroki algorytmu:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Ustalić piksele, w których nastąpiły istotne zmiany w stosunku do poprzedniej klatki, jako binarny obraz różnicowy C<sub>n</sub>

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Jeśli liczba pikseli, w których zarejestrowano zmiany, jest większa od zadanego progu, przerwać przetwarzanie tej klatki. Oznacza to, iż prawdopodobnie zarejestrowane zmiany są zbyt duże i raczej nie są ograniczone do obszaru oczu (gwałtowny ruch głową, zmiany w tle).

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Zastosować filtrację na obrazie C<sub>n</sub>, aby wyeliminować izolowane piksele. Wybrany został filtr, zerujący każdy piksel, w którego sąsiedztwie o zadanym rozmiarze ilość pikseli niezerowych jest mniejsza od zdefiniowanego progu.

![](readme_pliki/image038.png)

Maska zmian po zastosowaniu filtrowania

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Znaleźć spójne obszary przefiltrowanego obrazu C<sub>n</sub>.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Na każdym ze spójnych obszarów opisać prostokąt. Środki tych prostokątów stanowią kandydatów na położenie oczu w bieżącej klatce.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Dla każdego kandydata oblicz wielkość zmiany eyeChange, czyli sumę wartości pikseli obrazu różnicowego w prostokącie o zadanych wymiarach. Wybrać dwóch kandydatów o największym eyeChange.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Sprawdzić poniższe warunki dla dwóch najlepszych kandydatów. Jeśli je spełnią, zostaną uznani za położenie oczu w bieżącej klatce. Niespełnienie któregokolwiek z warunków oznacza, że nie wykryto mrugnięcia i należy kontynuować szukanie w następnej klatce:

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>wartość eyeChange dla kandydatów nie może być mniejsza od zadanego progu;

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>odległość pomiędzy kandydatami musi zawierać się w zadanym przedziale;

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>wartość bezwzględna kąta nachylenia linii łączącej kandydatów do osi x nie może być większa od zadanej wartości;

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>kandydaci muszą przejść test symetrii

### <a name="_Toc494697067"><span lang="EN-US">5.1.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Wykrywanie zmian</span></a>

Aby odnaleźć obszary oczu na obrazie, algorytm wykrywania mrugnięć potrzebuje informacji o zmianach w analizowanym obrazie. Informacja ta jest przekazywana pod postacią obrazu C<sub>n</sub>(x,y), traktowanego jako binarny (niezerowa wartość oznacza zmianę). Istnieje wiele metod na obliczenie takiej maski zmian. Kiedy zachodzi mrugnięcie, wybrana metoda powinna zwracać spójne obszary zmian, jednocześnie redukując wpływ szumu i fluktuacje obrazu związane z pracą detektora kamery. W tym przypadku zaimplementowane zostały dwa algorytmy: progowanie adaptacyjne obrazu różnicowego oraz metoda oparta na zależności liniowej wektorów.

### <a name="_Toc494697068"><span lang="EN-US">5.1.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Test symetrii</span></a>

Test symetrii, służy dodatkowemu upewnieniu się, że faktycznie wykryto mrugnięcie. Bazuje on na założeniu, iż obszar twarzy między oczami jest niemal symetryczny względem osi biegnącej pomiędzy nimi. Test przebiega następująco:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Należy obrócić obraz tak, aby linia łącząca potencjalne środki oczu stała się równoległa do osi x.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Pobrać obszar o zadanym rozmiarze z obróconego obrazu, ze środkiem w punkcie leżącym w środku odcinka łączącego oczy.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Obliczyć funkcję projekcyjną (Integral projection function) w kierunku pionowym – zsumować wartości pikseli dla każdej kolumny obrazu.

![](readme_pliki/image039.jpg)

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Analizując osobno lewą i prawą połowę obrazu, należy obliczyć, odpowiednio, funkcje _pffl_ i _pffr_, które dla każdego _x_ przyporządkują częstość występowania _ipf(x)_ w analizowanej połowie obrazu.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Sumując po trzy kolejne wartości _pffl_ i odpowiadające im trzy kolejne wartości _pffr_, sprawdzić czy różnice między wszystkimi sumami odpowiadających sobie trójek są mniejsze od zadanego parametru. Jest to warunek przejścia testu.

### <a name="_Toc494697069"><span lang="EN-US">5.1.3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Śledzenie źrenic</span></a>

Gdy znane jest położenie oczu na podstawie wykrytych mrugnięć, rozpoczyna się ich śledzenie. W każdej kolejnej klatce odnajdywane są oczy na podstawie ich położenia w klatce poprzedniej. Poszukiwanie oczu odbywa się metodą dopasowania szablonu. Aby zmniejszyć ilość obliczeń, zamiast obrazów samych oczu bądź całej twarzy, śledzony jest jedynie prostokątny obszar o stałych rozmiarach znajdujący się pomiędzy oczami.

![](readme_pliki/image040.png)

Śledzony prostokątny obszar oraz położenie źrenic

Algorytm składa się z następujących kroków:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Predykcja położenia szablonu w bieżącej klatce na podstawie położenia szablonu w poprzednich dwóch klatkach. Niech X<sub>n</sub> będzie wektorem zawierającym położeniem środka szablonu w klatce _n_, a X<sub>np</sub> przewidywanym położeniem środka szablonu w klatce n. Wówczas X<sub>np</sub> oblicza się na podstawie:

![](readme_pliki/image041.jpg)

Równanie 3

przyjmując za położenie początkowe X<sub>0</sub> wynik algorytmu wykrywającego mrugnięcia, zaś X<sub>-1</sub>=X<sub>0</sub>.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Określić nowe położenie szablonu X<sub>n</sub> , którym jest środek najlepszego dopasowania szablonu wokół punktu X<sub>np</sub>.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Dopasowanie szablonu polega na znalezieniu obszaru obrazu o rozmiarze szablonu, który jest najbardziej podobny. Mając dany szablon T<sub>n-1</sub> pochodzący z poprzedniej klatki oraz bieżący obraz I<sub>n</sub>, obszar ten znajduje się w punkcie odpowiadającym maksymalnej wartości R(x,y), wyrażającej się wzorem:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>![](readme_pliki/image042.jpg)

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Równanie 4 Maksymalna wartość korelacji

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>przy czym x' i y' oznaczają współrzędne zawarte w rozmiarze szablonu, zaś x i y są współrzędnymi w ramach badanego obszaru. Do eksperymentów w ramach tej pracy wybrano ostatecznie sposób liczenia korelacji zaimplementowany w bibliotece OpenCV.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Zakłada się, iż szablon nie przemieścił się zbytnio od poprzedniej klatki. W celu zmniejszenia ilości obliczeń, poszukiwania przeprowadzane są w obszarze o zdefiniowanym wcześniej rozmiarze.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Znaleźć nowe położenia oczu Er<sub>n</sub> i El<sub>n</sub> (odpowiednio prawego i lewego) w klatce _n_, na podstawie położenia szablonu X<sub>n</sub> i zapamiętanych wektorów przesunięcia oczu względem środka szablonu er<sub>n-1</sub> i el<sub>n-1</sub> w poprzedniej klatce.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Wyszukiwanie polega na znalezieniu źrenic, przy założeniu, iż są najciemniejszym fragmentem obrazu oka. W prostokątach o zadanych rozmiarach i środkach odpowiednio w punktach X<sub>n</sub> + er<sub>n-1</sub> oraz X<sub>n</sub> + el<sub>n-1</sub>, szuka się punktów, w których otoczeniu o wymiarach 5 x 5 średnia jasność jest najmniejsza.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Dla znalezionych punktów Er<sub>n</sub> i El<sub>n</sub> sprawdzić warunki geometryczne, aby upewnić się czy znalezione punkty mogą być faktycznie położeniem oczu. Niespełnienie choć jednego z warunków oznacza, iż śledzenie nie powiodło się, oczy uznaje się za zgubione, a system powraca do stanu wykrywania mrugnięć.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Aktualizacja danych algorytmu.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Środek szablonu X<sub>n</sub> zmienia wartość na środek odcinka łączącego Er<sub>n</sub> i El<sub>n</sub>. Należy też zapamiętać położenie źrenic względem X<sub>n</sub> (er<sub>n</sub> i el<sub>n</sub>) oraz nowy szablon T<sub>n</sub>, o środku w X<sub>n</sub> i zdefiniowanym rozmiarze.

## <a name="_Toc494697070">5.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Algorytm Lucas-Kanade</a>

Jednym z podstawowych algorytmów z grupy metod gradientowych jest bardzo popularny algorytm Lucas-Kanade. W swojej podstawowej formie zaproponowanej w 1981 roku próbował generować tzw. "gęste" pole przepływu, czyli obliczał wektor przepływu dla każdego punktu obrazu. Okazało się jednak, że metodę można łatwo wykorzystać do wyliczania wektorów dla zbiorów pojedynczych punktów. Algorytm ten mógł zostać zastosowany do generowania "rzadkiego" pola, gdyż zależy tylko od pewnych lokalnych informacji pochodzących z określonego sąsiedztwa otaczającego każdy interesujący nas punkt. Jest to zasadnicza różnica w porównaniu z globalnym algorytmem Horn and Shunck, który zostanie opisany później. Wadą stosowania małego lokalnego okna do analizy obrazu jest możliwość zgubienia większego ruchu, który w kolejnych klatkach może wyjść poza obszar poszukiwania, a co za tym idzie, okaże się niemożliwy do odnalezienia przez algorytm. Problem ten spowodował rozwinięcie piramidalnego algorytmu LK, który zaczyna analizę od najwyższego poziomu piramidy obrazu (o najmniejszej zawartości detali) i kontynuuje pracę schodząc na niższe poziomy (o większej liczbie szczegółów) - **Błąd! Nie można odnaleźć źródła odwołania.**. Takie podejście pozwala wychwycić zdecydowane ruchy, a także umożliwia przyspieszenie działania algorytmu, jeżeli w kolejnych etapach będą wykorzystywane już uzyskane informacje.

Implementacja algorytmu Lucas-Kanade w bibliotece OpenCV składa się z dwóch głównych etapów działania:

1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> znajdź punkty dobre do śledzenia

2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> uruchom pętlę śledzącą zadane punkty

Pierwsze z tych zagadnień związane jest ze specyfiką metody Lucas-Kanade opisanej w wcześniej i wiąże się ze znalezieniem punktów, których gradient jasności ma charakter dwuwymiarowy. Do tego celu służy funkcja:

![](readme_pliki/image043.png)

Oprócz podstawowych parametrów takich jak: obraz wejściowy, funkcja ta pozwala określić dokładność z jaką mają być obliczana pozycja naroży, minimalny dystans pomiędzy punktami lub też pozwala zdefiniować interesujący nas region poszukiwań. Dodatkowo, aby wskazać z subpikselową dokładnością położenie wykrytych naroży należy zastosować funkcję:

![](readme_pliki/image044.png)

Posiadając listę interesujących nas punktów do śledzenia możemy uruchomić w pętli funkcję śledzącą, której w każdym kolejnym wywołaniu pozwalamy analizować kolejną klatkę obrazu. W programie zastosowano wersję piramidalną algorytmu Lucas-Kanade:

![](readme_pliki/image045.png)

Jednym z głównych parametrów funkcji jest element _criteria_, pozwalający określić liczbę iteracji i akceptowalny błąd epsilon. Aby ustawić parametry na: 20 iteracji i błąd 0.03 możemy skorzystać z następującego makra: _vTermCriteria(CV_TERMCRIT_ITER|CV_TERMCRIT_EPS,20,0.03)._ Funkcja ta znajduje położenie zadanych punktów z pierwszej klatki na drugiej klatce. Koordynaty tych punktów są podawane z subpikselową dokładnością. Warto zauważyć, że algorytm operuje na obrazie w skali szarości. Do uzyskania takiego obrazu możemy użyć funkcji: cvCvtColor( image, grey, CV_BGR2GRAY );

![](readme_pliki/image046.jpg)

Przykład działania algorytmu Lucas-Kanade

## <a name="_Toc494697071">5.3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Algorytm Horn-Schunck</a>

Autorzy algorytmu wyszli z założenia, że nie jest możliwe obliczenie przepływu optycznego dla punktu niezależnie od jego sąsiedztwa bez dodatkowych ograniczeń. Dzieje się tak, ponieważ prędkość w każdym punkcie posiada dwie składowe, podczas gdy zmiana jasności punktu na płaszczyźnie obrazu daje tylko jedno ograniczenie. Dla przykładu można podać ekstremalny przypadek: poruszający się wzór, którego jasność zmienia się tylko w jednym wymiarze, natomiast w drugim jest stała. Ruch takiego wzoru w jednym z tych wymiarów skutkuje zmianą jasności w danym punkcie - wykryciem ruchu, natomiast w drugim nie powoduje żadnych zmian.

Pierwszym założeniem postawionym przez autorów metody jest stała jasność punktu poruszającego się obszaru.

Drugie ograniczenie przedstawione przez autorów mówi o gładkości zmian zachodzących na obrazie, tzn. zakłada, że sąsiadujące punkty wykonują podobne ruchy, posiadają zbliżoną prędkość, a pole ruchu zmienia się płynnie na prawie całej powierzchni obrazu. Niestety w miejscach, w których nachodzą na siebie dwa obiekty, będzie występować nieciągłość tego pola, tak więc wiadomym jest, że algorytm ten (jak i inne oparte na takim ograniczeniu) będzie miał problemy w przypadkach zachodzących na siebie krawędzi.

Algorytm Horn-Schunck należy do grupy algorytmów obliczających "gęste" pole przepływu. Z uwagi na to jego wydajność jest wyjątkowo niska w porównaniu choćby z algorytmem Lucas-Kanade. Jak dowiedziono, jego dokładność także znacznie odbiega od większości algorytmów obliczania pola przepływu (dla algorytmu w oryginalnej, niemodyfikowanej wersji).

Implementacja algorytmu Horn-Schunck jest nieco prostsza, sprowadza się bowiem do wywołania jednej funkcji:

![](readme_pliki/image047.png)

Interesującymi parametrami są: mnożnik Lagrange'a - współczynnik wagowy uwzględniany przy minimalizacji błędów związanych ze spełnieniem ograniczeń na: gładkość zmian prze-pływu optycznego i równania pola przepływu. Dodatkowo parametrem _criteria_ możemy ustawić liczbę iteracji dla algorytmu - co w przypadku metody Horn-Schunck jest bardzo ważnym czynnikiem. Obrazy wejściowe imgA i imgB powinny być obrazami w skali szarości.

## <span lang="EN-US">5.4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span> <a name="_Toc494697072"><span lang="EN-US">Algorytmy Mean-Shift i Camshift</span></a>

Poniżej zostaną przedstawione dwie techniki, które także zostały zaimplementowane w bibliotece OpenCV. Metoda Mean-shift, która jest ogólną techniką analizy danych oraz algorytm Camshift - wykorzystujący tę technikę w dziedzinie analizy obrazów, tworzą kolejne narzędzie, które pozwala śledzić obiekty poruszające się w sekwencji wideo. Co więcej metody te pozwalają skutecznie podążać za obiektami, których rozmiary zmieniają się w trakcie ruchu.

Algorytm mean-shift jest deterministyczną metodą znajdowania lokalnego ekstremum w rozkładzie gęstości zbioru danych. Dla ciągłych dystrybucji jest to proste zadanie, które sprowadza się do "wspinaczki po zboczu" histogramu danych. Niestety, dla dyskretnych danych, problem ten jest zdecydowanie mniej trywialny.

Określenie "deterministyczny" zostało użyte z powodu pewnych założeń. Mianowicie technika mean-shift ignoruje dane, które znacznie odbiegają od pozostałych. Dzieje się tak ponieważ algorytm analizuje tylko punkty w pewnym określonym lokalnym oknie, które jest przesuwane w trakcie działania algorytmu.

Zasadę działania algorytmu mean-shift można zapisać w następujących punktach:

1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wybierz okno poszukiwań:

2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> jego wstępną lokację;

a.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> jego charakter (stały, wielomianowy, eksponencjalny, przyjmujący postać funkcji Gaussa);

b.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> jego kształt (symetryczne albo niesymetryczne), orientację;

c.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> jego rozmiar.

3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wylicz środek masy dla danego okna.

4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wyśrodkuj okno w wyliczonym punkcie.

5.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Jeżeli okno się poruszyło w poprzednim kroku, wróć do punktu 2.

![](readme_pliki/image048.png)

Zasada działania algorytmu mean-shift wstępne okno jest umieszczane na dwuwymiarowym obrazie zawierającym zbiór punktów, w kolejnych krokach jest ono centrowane nad lokalnym ekstremum rozkładu punktów, dopóki nie osiągnie zbieżności

Główną funkcją algorytmu Camshift jest:

![](readme_pliki/image049.png)

Pierwszym z parametrów algorytmu jest obraz, którego każdy piksel ma przypisane prawdopodobieństwo należenia do obiektu, dla którego został stworzony histogram. Aby obliczyć taki obraz należy posłużyć się funkcją:

![](readme_pliki/image050.png)

W programie zaimplementowano metodę zaznaczania myszą obszaru, dla którego chcemy obliczyć histogram. Aby zaznaczyć interesujący nas obszar, należy nacisnąć i przytrzymać lewy przycisk myszy po czym rozciągnąć zaznaczenie na interesujący nas obszar. Dla wyznaczonego pola zostanie obliczony histogram przy użyciu funkcji:

![](readme_pliki/image051.png)

Wstępne okno poszukiwań zostanie także przypisane zaznaczonemu obszarowi.

## <a name="_Toc494697073">5.5.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Filtr Kalmana</a>

Proces śledzenia na podstawie sekwencji wideo podatny jest na błędy, wynikające zarówno z szumów w obrazie, jak i niedokładności metod śledzenia. Aby temu zaradzić, można zastosować filtr Kalmana. Technika ta służy do reprezentacji procesu przebiegającego w czasie, o którym dostępne są jedynie niedokładne wartości pomiarów stanowiące zniekształconą kombinację właściwych parametrów procesu. Wiedza o przybliżonym położeniu śledzonego obiektu jest potrzebna wielu algorytmom śledzącym, zaś dokładność tego przybliżenia <span style="font-size:11.5pt;line-height:150%">może stanowić o szybkości i dokładności tych algorytmów. W tej pracy filtr Kalmana został zastosowany do wygładzenia wyników śledzenia, opisanymi wcześniej, metodami Lucas-Kanade oraz CAMSHIFT. Użycie filtru Kalmana miało zmniejszyć wpływ niedokładnego śledzenia w pojedynczych klatkach na obserwowany wynik śledzenia.</span>

## <a name="_Toc494697074">5.6.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Przykład zastosowania</a>

### <a name="_Toc494697075"><span lang="EN-US">5.6.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Inicjalizacja</span></a>

Śledzenie w algorytmie CAMSHIFT odbywa się na podstawie kanału H z modelu kolorów HSV, jednak kanały nasycenia (S) oraz intensywności (V) nie są całkiem ignorowane. Autor algorytmu sugeruje wykorzystanie zawartych w nich informacji do ykluczenia z przetwarzania pikseli niedoświetlonych (o jasności poniżej pewnego progu), zbyt jasnych oraz o małym nasyceniu koloru. Takie piksele są wykluczane przy użyciu maski, ponieważ informacje o barwie w nich zawarte nie są wystarczająco pewne. Poniżej przedstawiona jest maska wykluczająca piksele na podstawie jasności i nasycenia. Na ilustracjach obrazy wejściowe do algorytmu CAMSHIFT mają zmniejszoną o połowę rozdzielczość. Wiąże się to z tym, iż algorytm ten był w ramach eksperymentu widocznego na obrazach stosowany jako krok wstępny przed dopasowywaniem obrazu przy pomocy algorytmu Lucas-Kanade; liczyła się więc wydajność, a nie dokładność.

![](readme_pliki/image052.png)

Maska z zaznaczonym oknem śledzenia, utworzona na podstawie jasności i nasycenia

![](readme_pliki/image053.png)

Rozkład prawdopodobieństwa barw twarzy, z uwzględnieniem maski

![](readme_pliki/image054.png)

Wynik śledzenia algorytmem CAMSHIFT

Algorytm CAMSHIFT wymaga określenia rozkładu barw w śledzonym obiekcie. Rozkład ten może ulec zmianie między kolejnymi uruchomieniami systemu, zależy on bowiem od koloru skóry śledzonej twarzy, oświetlenia (uwzględnić trzeba tu również światło padające z ekranu komputera) oraz tła (dotyczy szczególnie zastosowań w komputerach przenośnych). Histogram musi być przygotowany przed rozpoczęciem śledzenia. W testach zastosowana została prosta metoda uzyskania tego histogramu, polegająca na obliczeniu histogramu eliptycznego obszaru o zadanym rozmiarze, znajdującego się na środku obrazu. Zakłada się tu, iż użytkownik w celu inicjacji algorytmu prezentuje swoją twarz kamerze w pozycji neutralnej. Przy tworzeniu histogramu brane są pod uwagę jedynie piksele spełniające warunki na jasność i nasycenie koloru opisane wcześniej.

Kolejną badaną metodą śledzenia twarzy było sformułowanie tego zagadnienia jako problemu dopasowania obrazów. Następnie zastosowany został algorytm _Lucas-Kanade_. Wykorzystanie wersji modelującej przekształcenia afiniczne umożliwia śledzenie zmian w położeniu i obrocie w płaszczyźnie obrazu, możliwe jest również wyśledzenie zmian rozmiaru twarzy. Skalowanie w pewnym stopniu odpowiada odległości od kamery. Jako że obrót względem osi pionowej i poziomej mogą również zmieniać rozmiar widzianego obszaru twarzy, skalowanie ostatecznie nie zostało wykorzystane przy prezentacji wyniku śledzenia.

W celu odnalezienia śledzonego obiektu w klatce bieżącej potrzebny jest szablon oraz położenie obiektu we wcześniejszej klatce; aby rozpocząć śledzenie, potrzebna jest wcześniejsza informacja o położeniu początkowym. Dalsze śledzenie może odbywać się poprzez dopasowanie klatki bieżącej albo do szablonu z klatki poprzedniej, lub też do szablonu z klatki początkowej. Dopasowywanie do klatki poprzedniej pozwala na śledzenie drobnych, stopniowych zmian zachodzących w obrazie (powolna zmiana oświetlenia, mimika), jednak prowadzi również do kumulacji błędów śledzenia.

<span style="font-size:11.5pt;line-height:150%">W tym przypadku filtr Kalmana został wykorzystany przy śledzeniu twarzy metodami _CAMSHIFT_ oraz _Lucas-Kanade_. Filtrowaniu poddane zostały wszystkie możliwe do wyśledzenia obiema algorytmami właściwości obrazu twarzy, a więc położenie, kąt nachylenia oraz wysokość i szerokość.</span>

## <a name="_Toc494697076">5.7.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Obserwacje / porównanie algorytmów</a>

### <a name="_Toc494697077"><span lang="EN-US">5.7.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Algorytm CAMSHIFT</span></a>

Algorytm CAMSHIFT, który prowadzi śledzenie w oparciu o kanał barwy, wykazuje dużą podatność na kolor i oświetlenie obserwowanej sceny. Jeśli pomieszczenie nie jest wystarczająco oświetlone, istotny wpływ na kolor twarzy użytkownika ma światło padające z ekranu komputera. W związku z tym algorytm może utracić śledzoną twarz, jeśli obraz na monitorze ulegnie zbyt dużym zmianom. Oświetlenie powinno być w miarę możliwości jednolite, rozproszone i jasne, aby umożliwiać dokładny pomiar i pełne wykorzystanie zakresu kanału barwy do rozpoznawania obrazu twarzy. Nawet w idealnych warunkach oświetlenia, występowanie za plecami użytkownika obiektów w kolorach beżu, różu albo czerwieni, a więc posiadających zbliżoną barwę (choć odmienną intensywność lub nasycenie) może utrudnić inicjalizację algorytmu, a w skrajnych przypadkach uniemożliwić śledzenie. W innej pracy zaproponowana jest metoda inicjalizacji algorytmu pomagająca odrzucić barwy występujące poza śledzonym obszarem, co może zmniejszyć wpływ obiektów o podobnej barwie do twarzy obecnych w scenie od początku. O ile warunki są sprzyjające, algorytm CAMSHIFT wykazuje dobre właściwości śledzenia położenia twarzy w sekwencji obrazów, szczególnie skutecznie jest w stanie odnajdywać śledzony obszar po chwilowym przesłonięciu. Położenie nie jest odnajdywane z precyzją co do piksela, zaś obrót i skalowanie bardziej wynikają z kształtu obrazu twarzy na ekranie, niż faktycznemu ułożeniu głowy. Wyniki nie są dokładne, jednak algorytm, w sprzyjających warunkach, nie traci śledzonej twarzy – jest więc wiarygodny w zastosowaniach potrzebujących jedynie określenie ogólnego położenia głowy w obrazie.

### <a name="_Toc494697078"><span lang="EN-US">5.7.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Algorytm Lucas-Kanade</span></a>

Algorytm Lucas-Kanade działa natomiast na podstawie kanału jasności oraz gradientu jasności, dzięki temu nie jest podatny na obecność obszarów o podobnej barwie. Z drugiej strony, algorytm ten do śledzenia wymaga niewielkich zmian w wyglądzie śledzonego obiektu od wzorcowej klatki. Jeśli śledzony obiekt przesunie się w większej części poza obszar, w którym znajdował się poprzednio, najprawdopodobniej jego nowa pozycja nie zostanie rozpoznana poprawnie. Dodatkowo, obecność obiektu podobnego do śledzonego może spowodować, że algorytm zgubi śledzony obiekt i zacznie podążać za niewłaściwym. Zastosowanie aktualizowania obrazu wzorcowego odnalezionym wizerunkiem śledzonego obiektu może zniwelować te zagrożenia w przypadku śledzenia twarzy przy rozsądnej liczbie klatek na sekundę – przemieszczenie między klatkami jest wówczas znacząco mniejsze od wymiarów śledzonego obszaru. Aktualizowanie szablonu na bieżąco jest szczególnie istotne w przypadku śledzenia dużych obiektów, których wygląd może ulec znaczącym zmianom w związku z obrotem względem kamery; bazowanie na wzorcu z klatki poprzedniej powoduje jednak, że algorytm może zaakceptować obce obiekty znajdujące się w tle lub przesłaniające śledzoną twarz.

W podążaniu za szybko przemieszczającymi się obiektami pomaga również zastosowanie piramidy Gaussa. W eksperymentach dało się jednak zauważyć, że wykorzystanie piramidy Gaussa powodowało gromadzenie błędu dopasowania, ponieważ w każdej klatce wykrywane było minimalne przesunięcie i skalowanie w dół. Może to być związane z błędem zaokrąglania lub specyfiką tworzenia piramidy Gaussa.

### <a name="_Toc494697079"><span lang="EN-US">5.7.3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">CAMSIFT I Lucas-Kanade</span></a>

Wykorzystanie algorytmu CAMSHIFT do wyznaczenia początkowego przybliżenia zniekształcenia obrazu twarzy względem poprzedniej klatki na potrzeby algorytmu LK dało bardziej stabilne rezultaty niż zastosowanie owych algorytmów z osobna. Przy przetwarzaniu tej samej testowej sekwencji obrazów nastąpiło zmniejszenie tendencji do oscylacji czy ucieczki wyniku śledzenia. Ponadto wykorzystanie obu algorytmów nie obniżyło znacząco szybkości działania systemu.

### <a name="_Toc494697080">5.7.4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Śledzenie źrenic</a>

Specyficzną cechą algorytmu śledzenia źrenic wykorzystującego wykrywanie mrugnięć, jest brak potrzeby wykonania ręcznej inicjalizacji. W trybie detekcji mrugnięć położenie twarzy nie jest znane, jednak zaraz po wykryciu oczu rozpoczyna się śledzenie. Co więcej, w przypadku utraty śledzonych źrenic, algorytm wraca do trybu wykrywania, aby podjąć śledzenie po następnym rozpoznanym mrugnięciu. Z drugiej strony, żadna z badanych metod wykrywania mrugnięć nie jest doskonała i zdarza się wykrycie fałszywych mrugnięć, szczególnie w obecności silnych szumów. Nawet jeśli faktyczne obszary mrugnięć znajdą się na liście potencjalnych wyników, nie zawsze wygrywają w rankingu wielkości zmiany (eyeChange). Na przykład jako jedna ze źrenic, rozpoznawana była wystarczająco ruchliwa brew. Algorytm miał również problemy w wykryciu mrugnięć w przypadku oczu przysłoniętych okularami. Obie metody wykrywania zmian okazały się również wrażliwe na właściwy dobór parametrów.

W praktyce metoda wykrywania zmian, oparta o liniową zależność wektorów wykazywała większą skuteczność przy wykrywaniu mrugnięć, jak również zwracała mniej fałszywych wyników. Jest ona też bardziej skomplikowana obliczeniowo – dało się zauważyć kilkukrotny spadek szybkości przetwarzania systemu względem metody progowania adaptacyjnego. Ta druga technika wymagała z kolei zastosowania pomijania klatek w przypadku źródeł obrazu pracujących z częstotliwością większą niż 7 Hz (optymalną dla wykrywania mrugnięć).

### <a name="_Toc494697081"><span lang="EN-US">5.7.5.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Porównanie algroytmów</span></a>

W poniższej tabeli przedstawione jest porównanie zaobserwowanych cech algorytmów. Z braku wyznaczonej skali, porównanie odbyło się względem obserwacji pozostałych algorytmów.

<table class="MsoNormalTable" border="1" cellspacing="0" cellpadding="0" width="0" style="width:467.45pt;margin-left:-5.4pt;border-collapse:collapse;border:none">

<tbody>

<tr style="height:20.3pt">

<td width="162" valign="top" style="width:121.25pt;border-top:2.25pt;border-left:
  2.25pt;border-bottom:1.0pt;border-right:1.0pt;border-color:windowtext;
  border-style:solid;padding:0cm 5.4pt 0cm 5.4pt;height:20.3pt">

**<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;  color:black">Metoda</span>**

</td>

<td width="150" valign="top" style="width:112.5pt;border-top:solid windowtext 2.25pt;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:20.3pt">

**<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;  color:black">Inicjalizacja</span>**

</td>

<td width="156" valign="top" style="width:117.0pt;border-top:solid windowtext 2.25pt;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:20.3pt">

**<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;  color:black">Dokładność</span>**

</td>

<td width="156" valign="top" style="width:116.7pt;border-top:solid windowtext 2.25pt;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 2.25pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:20.3pt">

**<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;  color:black">Niezawodność</span>**

</td>

</tr>

<tr style="height:19.3pt">

<td width="162" valign="top" style="width:121.25pt;border-top:none;border-left:
  solid windowtext 2.25pt;border-bottom:solid windowtext 1.0pt;border-right:
  solid windowtext 1.0pt;padding:0cm 5.4pt 0cm 5.4pt;height:19.3pt">

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;color:black">CAMSHIFT</span>

</td>

<td width="150" valign="top" style="width:112.5pt;border-top:none;border-left:  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:19.3pt">

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;color:black">Wymaga wzorca</span>

</td>

<td width="156" valign="top" style="width:117.0pt;border-top:none;border-left:  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:19.3pt">

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;color:black">Mała</span>

</td>

<td width="156" valign="top" style="width:116.7pt;border-top:none;border-left:  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 2.25pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:19.3pt">

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;color:black">Wysoka</span>

</td>

</tr>

<tr style="height:19.3pt">

<td width="162" valign="top" style="width:121.25pt;border-top:none;border-left:
  solid windowtext 2.25pt;border-bottom:solid windowtext 1.0pt;border-right:
  solid windowtext 1.0pt;padding:0cm 5.4pt 0cm 5.4pt;height:19.3pt">

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;color:black">Lucas-Kanade</span>

</td>

<td width="150" valign="top" style="width:112.5pt;border-top:none;border-left:  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:19.3pt">

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;color:black">Wymaga wzorca</span>

</td>

<td width="156" valign="top" style="width:117.0pt;border-top:none;border-left:  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:19.3pt">

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;color:black">Średnia</span>

</td>

<td width="156" valign="top" style="width:116.7pt;border-top:none;border-left:  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 2.25pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:19.3pt">

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;color:black">Średnia</span>

</td>

</tr>

<tr style="height:33.1pt">

<td width="162" valign="top" style="width:121.25pt;border-top:none;border-left:
  solid windowtext 2.25pt;border-bottom:solid windowtext 1.0pt;border-right:
  solid windowtext 1.0pt;padding:0cm 5.4pt 0cm 5.4pt;height:33.1pt">

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;color:black">CAMSHIFT</span>

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;color:black">Lucas-Kanade</span>

</td>

<td width="150" valign="top" style="width:112.5pt;border-top:none;border-left:  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:33.1pt">

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;color:black">Wymaga wzorca</span>

</td>

<td width="156" valign="top" style="width:117.0pt;border-top:none;border-left:  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:33.1pt">

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;color:black">Średnia</span>

</td>

<td width="156" valign="top" style="width:116.7pt;border-top:none;border-left:  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 2.25pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:33.1pt">

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;color:black">Średnia</span>

</td>

</tr>

<tr style="height:19.3pt">

<td width="162" valign="top" style="width:121.25pt;border-top:none;border-left:
  solid windowtext 2.25pt;border-bottom:solid windowtext 2.25pt;border-right:
  solid windowtext 1.0pt;padding:0cm 5.4pt 0cm 5.4pt;height:19.3pt">

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;color:black">Śledzenie oczu</span>

</td>

<td width="150" valign="top" style="width:112.5pt;border-top:none;border-left:  none;border-bottom:solid windowtext 2.25pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:19.3pt">

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;color:black">Samoczynna</span>

</td>

<td width="156" valign="top" style="width:117.0pt;border-top:none;border-left:  none;border-bottom:solid windowtext 2.25pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:19.3pt">

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;color:black">Wysoka</span>

</td>

<td width="156" valign="top" style="width:116.7pt;border-top:none;border-left:  none;border-bottom:solid windowtext 2.25pt;border-right:solid windowtext 2.25pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:19.3pt">

<span lang="EN-US" style="font-size:11.5pt;font-family:&quot;Times New Roman&quot;,serif;  color:black">Średnia</span>

</td>

</tr>

</tbody>

</table>

Porównanie algorytmów

<span style="font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,sans-serif">  
</span>

<span lang="EN-US"> </span>

# <a name="_Toc494548782"></a><a name="_Toc494697082">6.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Detekcja upadku (kamera statyczna RGB)</a>

## <a name="_Toc494548783"></a><a name="_Toc494697083">6.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wstęp</a>

Zagadnienie zautomatyzowanego rozpoznawania upadków jest dziedziną, na temat której istnieje sporo artykułów. Szczególnie uwzględniają one opiekę nad osobami starszymi w szpitalu lub domu.

Obecne systemy zautomatyzowanego rozpoznawania (detekcji) upadków polegają na:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Systemach wizyjnych (kamery w skali szarości, kamery RGB, kamery głębi),

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>czujnikach ruchu noszonych przez osobę (czujniki specjalnie stworzone w tym celu, smartfony)- najczęściej są nimi akcelerometry,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>systemach monitorujących parametry życiowe osób,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>mikrofonach- zazwyczaj jako wspomaganie do kamer).

Niewątpliwą zaletą systemów wizyjnych jest fakt, że nie wymagają od obserwowanych osób żadnych działań (w przeciwieństwie do czujników ruchu). Urządzenia noszone przy sobie mogą mieć w zastosowaniach praktycznych obniżoną efektywność, wynikającą z konieczności ich noszenia przy sobie i ładowania do nich baterii, o czym często osoby zapominają.

W niniejszym rozdziale skupiam się na opracowaniu wizyjnych systemów detekcji upadków, ze szczególnym uwzględnieniem jednej kamery RGB umiejscowionej w pomieszczeniu, w rogu pokoju, przy suficie. Zaprezentuje opracowane w literaturze fachowej algorytmy (metody) detekcji upadku, a także systemy detekcji upadku, które najczęściej wykorzystują wiele metod równocześnie w celu podniesienia ich skuteczności rozpoznawania i wyeliminowania fałszywych alarmów.

Istnieje wiele komercyjnych systemów o zamkniętym kodzie źródłowym służących rozpoznawaniu upadków (szczególnie opierających się na czujnikach ruchu), jednakże trudno znaleźć takie otwarto-źródłowe systemy lub metody. W szczególności biblioteka OpenCV zawiera wiele metod służących wyabstrahowaniu sylwetki osoby i śledzenia go, które okazują się bardzo przydatne do detekcji upadku, jednakże nie posiada narzędzi służących konkretnie detekcji upadku.

## <a name="_Toc494548784"></a><a name="_Toc494697084">6.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wstępne przetwarzanie sygnału wideo</a>

Do analizy i klasyfikowania zachowań badanej osoby konieczne jest wykrycie jej sylwetki na obrazie z kamery. Często wykorzystuje się również metody podnoszące skuteczność wykrywania sylwetki takie jak usuwanie cieni. Ponieważ większość skutecznych metod detekcji upadku analizuje zachowanie osoby w czasie (w odróżnieniu do analizy pojedynczej klatki filmu) konieczne jest też śledzenie sylwetki osoby na filmie wideo. Niektóre metody detekcji upadku wykorzystują śledzenie głowy osoby (wynika to z tego, że gwałtowne ruchy głową rejestruje się bardzo rzadko podczas codziennych czynności, a często towarzyszą one przewróceniu się). Biblioteka OpenCV posiada zaimplementowane funkcjonalności bezpośrednio wykonujące wspomniane czynności (wyabstrahowanie sylwetki z tła) lub pomagające przy ich implementacji (np. śledzenie punktów kluczowych). Ponieważ temat wykrywania sylwetki oraz głowy, a także ich śledzenia jest obszerny i nie jest tematem tego opracowania, w dalszej części pracy zakładam, że te wspomniane algorytmy są opracowane.

## <a name="_Toc494548785"></a><a name="_Toc494697085">6.3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Często występujące problemy</a>

Poniżej omawiam częste problemy, które często napotyka się podczas opracowania wizyjnych systemów detekcji upadków. Często utrudniają one

wykrycie upadku lub powodują powstawanie fałszywych alarmów (klasyfikacji upadku przez system, podczas gdy w rzeczywistości upadek nie występuje). Część z nich wynika z niedoskonałości systemów wydzielających sylwetkę osoby z tła.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Akcje lub pozy osoby nie będące upadkiem (szybkie siadanie, spanie na łóżku). Systemy polegające na analizie pozy osoby często klasyfikują pozycję leżącą człowieka jako upadek. Natomiast systemy, które analizują zachowanie człowieka w czasie, źle klasyfikują błędnie gwałtowne zmiany położenia człowieka. Mogą nimi być na przykład:

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>      gwałtowne siadanie,

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>      schylanie się w celu na przykład zawiązania buta.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Błędna klasyfikacja otoczenia. Błędne wydzielenie sylwetki osoby od tła ma wpływ na większość systemów detekcji upadku, szczególnie na te, które analizują postawę osoby oraz jej zachowanie w czasie.

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>      rozpoznanie sylwetki zwierząt (np. psa) jako człowieka,

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>      rozpoznanie zmian tła jako sylwetka człowieka lub jej część (np. otwarta szafka, która wcześniej była zamknięta),

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>      rozpoznanie cienia osoby jako części jego sylwetki,

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>      rozpoznanie odbicia w lustrze/szybie jako części sylwetki człowieka,

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>      rozpoznanie elementów otoczenia, z którymi osoba wchodzi w interakcję jako część jej sylwetki- na przykład niesiony karton lub przenoszone krzesło,

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>      rozpoznanie dwóch osób wchodzących w interakcję lub przebywających blisko jako sylwetkę jednej osoby.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Częściowo przysłonięta sylwetka osoby przez przedmiot lub jej sylwetka niemieszcząca się w pełni w kadrze. Skutki są podobne jak powyższym punkcie.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Możliwości urządzeń (pamięć i moc do przechowania i analizy pewnego fragmentu wideo). Przykładowo niewystarczająca moc obliczeniowa urządzenia lub pamięć niepozwalająca na przechowanie dłuższego fragmentu filmu (metody sprawdzające jak długo w danej pozie znajduje się człowiek).

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Niedostateczna jakość obrazu kamery wynikająca ze złego naświetlenia,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Zbyt mała ilość danych trenujących dla metod bazujących na uczeniu maszynowym.

![szafka1](readme_pliki/image055.png)

Klatka przedstawiająca tło, źródło: [3]

<a name="_Toc493585998"></a><a name="_Toc493771064"></a><a name="_Toc494548786">![szafka2](readme_pliki/image056.png)</a>

Klatka przedstawiająca zmienione otoczenie, źródło: [3]

![szafka3](readme_pliki/image057.png)

Zobrazowana różnica klatki drugiej i pierwszej, źródło: [3]

Przedstawione poniżej ilustracje przedstawiają wydzieloną sylwetkę razem z cieniem. Cień może zaburzyć sylwetkę do stopnia, w którym niemożliwe będzie stwierdzenie w jakiej pozycji znajduje się dana osoba.

![cien1](readme_pliki/image058.png)

Klatka przedstawiająca upadającą osobę, źródło: [4]

![cien2](readme_pliki/image059.png)

Sylwetka wraz z cieniem, źródło: [4]

Jeżeli obiekty z którymi osoba wchodzi w interakcję interpretowane są jako jego sylwetka, system detekcji może mieć problem z rozpoznaniem kąta głównej osi człowieka i tym samym zaklasyfikowaniem czy dana osoba stoi (oś prostopadła do podłogi), czy leży (oś równoległa do podłogi). Poniżej przedstawiona jest sylwetka wyznaczona przez program (wraz z obiektami, które błędnie interpretowane są również jako sylwetka człowieka. Zielona linia obrazuje nakreśloną przez program główną oś osoby. Wyniki są wyraźnie zaburzone błędną interpretacją sylwetki.

![obiekty-czlowiek](readme_pliki/image060.jpg)

Sylwetka wraz z cieniem, źródło: [1]

## <a name="_Toc494548787"></a><a name="_Toc494697086">6.4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Ogólny podział metod (kontekst pojedynczej klatki, kontekst sekwencji).</a>

Systemy wizyjne rozpoznawania upadku znacznie różnią się w zależności od zakładanych warunków instalacji kamer i rodzaju używanych urządzeń.

Systemy można dzielić ze względu na:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>rodzaj użytych kamer (i ewentualnie inne urządzenia wspomagające jak na przykład mikrofony),

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>ilość kamer,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>umiejscowienie kamery lub kamer,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>rodzaj algorytmu- bazujące na analizie pojedynczej klatki filmu lub ich sekwencji.

### <a name="_Toc494548788"></a><a name="_Toc494697087">6.4.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Podział ze względu na rodzaj kamer</a>

W literaturze fachowej często napotkać można na systemy wykorzystujące kamery głębi. Sprawdzają się one lepiej od kamer z obrazem czarno-białym oraz kamer RGB, ze względu na to możliwości bardziej precyzyjnego wyznaczenia sylwetki osoby. Wynika to z faktu, że są dość niewrażliwe na oświetlenie sceny, a zatem zmianę pory dnia, zmianę sztucznych źródeł światła (żarówki). Co więcej, problem klasyfikowania cienia osoby jako części sylwetki nie występuje. Kamery takie znacznie ułatwiają również wykrywanie płaszczyzny podłogi, co pozwala na wyznaczenie kąta osi głównej osoby w stosunku do podłogi. Ma to szczególne znaczenie gdy kamera nie jest skierowana na wprost (czyli jej oś nie jest równoległa do podłogi), a znajduje się na przykład w rogu pokoju i skierowana jest do niej pod kątem.

Szczególny wpływ na rozwój systemów z kamerą głębi miało powstanie i upowszechnienie się kamery Xbox Kinect.

Standardowe kamery z obrazem czarno-białym lub kamery RGB są również często opisywane w artykułach na temat detekcji upadku, ze względu na powszechność ich stosowania i stosunkowo niskie koszty.

Część systemów polega na kamerach IR (dzięki czemu są skuteczne w ciemności) lub kamerach termowizyjnych (które również są skuteczne w ciemności, dodatkowo dobrze wydzielają sylwetkę człowieka).

Czasami napotkać można na systemy działające ze wspomaganiem przez mikrofon. Sama analiza dźwięku raczej nie pozwala na precyzyjne określenie, czy obserwowana osoba upadła, natomiast występuje w roli wspomagającej systemy wizyjne. Użycie mikrofonu opisane jest na przykład w artykule [2].

Poniższe obrazy kamer wskazują na różnicę w obrazie kamery w zależności od oświetlenia. Prześwietlenie zdjęcia powoduje ograniczenie ilości informacji z niego płynących i może znacznie obniżyć skuteczność metod detekcji upadków, szczególnie jeżeli wpływa na metody wydzielania sylwetki lub szukania punktów kluczowych na zdjęciu.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Górny lewy róg- światło słoneczne powoduje prześwietlenie w miejscu okna.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Górny prawy róg- prześwietlenie spowodowane lampami halogenowymi

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Lewy dolny róg- zdjęcie prześwietlone w lekkim stopniu.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Prawy dolny róg- zdjęcie w nocy wykonane przy pomocy światła podczerwonego.

![oswietlenie](readme_pliki/image061.jpg)

Kadry obrazujące wpływ oświetlenia sceny na obraz kamery, źródło: [6]

### <a name="_Toc494548789"></a><a name="_Toc494697088">6.4.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Podział ze względu na ilość kamer</a>

Spora część opracowań opisuje systemy używające jednej kamery. Istnieją jednak opracowania systemów korzystających z dwóch kamer. Znacznie zmniejsza to ilość fałszywych alarmów, ponieważ nawet jeżeli jedna kamera nie posiada pełnego obrazu sylwetki człowieka, druga kamera umiejscowiona w innym miejscu weryfikuje wnioski pierwszej. Dodatkowo system wyposażony w dwie kamery lepiej radzi sobie z wykrywaniem głowy, co również jest bardzo pomocne w wykryciu upadku.

### <a name="_Toc494548790"></a><a name="_Toc494697089">6.4.3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Podział ze względu na umiejscowienie kamer</a>

Zazwyczaj kamerę w pomieszczeniu umieszcza się w jednej z pozycji:

1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> w rogu pokoju, przy suficie,

2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> na wysokości wzroku, z osią kamery równoległą do podłogi,

3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> na suficie pomieszczenia, skierowaną centralnie do dołu.

Wydaje się, że najwyższą skutecznością charakteryzują się systemy z umiejscowieniem kamery w drugiej pozycji. Wynika to z faktu, że łatwo jest oszacować czy sylwetka obserwowanej osoby jest pionowa czy pozioma. W praktycznych zastosowaniach takie umiejscowienie kamery może nie być praktyczne, co wynika z małego pola widzenia tak umiejscowionej kamery oraz zajmowanej powierzchni na ścianie.

Istnieje wiele publikacji na temat systemów, w których kamery umiejscowione są w rogu pomieszczenia. Wynika to głównie z powszechności tego rozwiązania montażu w obecnych kamerach bezpieczeństwa i niskiej inwazyjności w pomieszczenie.

### <a name="_Toc494548791"></a><a name="_Toc494697090">6.4.4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Podział ze względu na rodzaj algorytmu</a>

Oczywistym jest, że rodzaj algorytmu ma kluczowy wpływ na skuteczność systemu. Wybór odpowiedniego algorytmu uwarunkowany jest wspomnianymi wyżej parametrami (typ, ilość kamer, ich umiejscowieniem), specyfiką pomieszczeń (czyli między innymi ich wielkością, ilością mebli lub przedmiotów mogących zaburzyć rozpoznanie sylwetki), a także ilością monitorowanych osób.  Istnieje wiele podejść do problemu detekcji upadku. Od najprostszych analizujących jedynie pojedynczą klatkę w celu klasyfikacji pozy osoby, po systemy polegające na badaniu przyspieszenia poszczególnych części ciała. Najczęściej jednak największym powodzeniem cieszą się systemy które łączą różne metody i klasyfikują zachowanie osoby w oparciu o opracowany specjalnie w tym celu model wnioskowania.

Następny podrozdział stanowi przedstawienie metod detekcji upadku.

## <a name="_Toc494548792"></a><a name="_Toc494697091">6.5.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Analiza poszczególnych algorytmów detekcji upadku</a>

Niniejszy rozdział przedstawia algorytmy służące detekcji upadku, które zostały opisane w publikacjach naukowych. Większość publikacji opisuje cały system składający się z kilku metod i system wnioskujący wykorzystujący zastosowane algorytmy. Ze szczególnym uwzględnieniem zostaną opisane algorytmy korzystające z jednej kamery RGB w rogu pomieszczenia.

### <a name="_Toc494548793"></a><a name="_Toc494697092">6.5.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Podstawowa estymacja postawy oraz detekcja ruchu obserwowanej osoby.</a>

Najbardziej podstawowym algorytmem analizy upadku jest analiza sylwetki człowieka na pojedynczej klatce. Pozwala ona zaklasyfikować pozycję jako na przykład stojącą, leżącą lub siedzącą. W praktycznych zastosowaniach często wykorzystuje się ją równocześnie mierząc jak długo i w jakiej miejscu znajduje się człowiek. Niektórych obszarów kadru można nie brać pod uwagę (przykładowo łóżko). Z drugiej analiza takich obszarów pozwala na stwierdzenie czy obserwowana osoba nie jest w stanie zagrożenia innym niż upadek (na przykład nie wstaje i nie rusza się w łóżku dłużej niż 12 godzin). Przykład: [3] (http://tunn.us/arduino/falldetector2.php).

Wspomniane wykrywanie ruchu człowieka wykorzystuje się w połączeniu z algorytmami analizującymi zachowanie obserwowanej osoby w czasie (na przykład przyspieszenie). Jeżeli po gwałtownym przyspieszeniu osoba nie rusza się, można przyjąć że uległa wypadkowi (lub straciła przytomność), gdyż zazwyczaj po innych gwałtownych czynnościach (siadanie, schylenie się w celu zawiązania buta) osoba zazwyczaj rusza się.

Poniższy obrazek ilustruje następujące metody:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>fioletowy kolor- prostokąt ROI,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>biały kolor- wpisana elipsa,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>zielony kolor- środek ciężkości,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>niebieski kolor- pozycja głowy.

Czarny prostokąt zakrywa twarz w celu zapewnienia anonimowości prezentowanej osobie.

![rozne-postawy](readme_pliki/image062.jpg)

Ilustracja różnych metod analizy postawy człowieka, źródło: [6]

#### Stosunek wysokość/szerokość prostokąta ROI.

Obszar zainteresowania definiuje się jako część obrazu, w której znajduje się obiekt będący podmiotem analizy. W literaturze anglojęzycznej nazywa się go ROI (Region of Interest) lub salient region. W zależności od skuteczności algorytmów wydzielających sylwetkę człowieka oraz stawianym wymaganiom, ROI może przybierać różny kształt. Najbardziej podstawowy to prostokąt.

Według autorów artykułu [8] sprawdzanie stosunku wysokość/szerokość prostokąta przynosi dobre efekty, jeżeli modelu wyboru prostokąta jest wyznaczony w odpowiedni sposób. Trywialne sposoby wyznaczania prostokąta, które jako współrzędne wierzchołków biorą współrzędne najbardziej wysuniętych pikseli sylwetki okazują się niewystarczające ze względu na wpływ wartości odstających, które pojawiają się przez niedoskonałość algorytmów wydzielających sylwetkę.

Zamiast tego autorzy proponują model przedstawiony poniżej.

Załóżmy, że mamy wydzieloną sylwetkę. Niech funkcja <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image063.png)</span> dla argumentu <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image064.png)</span> przyjmuje ilość pikseli należących do sylwetki w kolumnie <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image064.png)</span>. Jest to funkcja podobna do histogramu, ale zliczająca piksele tylko w kolumnach obrazu. Funkcja na rysunku niżej oznaczona jest jako x-projection. <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image065.png)</span>oznacza taki argument <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image064.png)</span>, dla którego funkcja <span style="font-size:11.0pt;line-height:107%;font-family:
&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image063.png)</span> przyjmuje wartość maksymalną, czyli <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image066.png)</span>

Wtedy <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image067.png)</span> (współrzędne <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image068.png)</span> prawych wierzchołków prostokąta ROI) i <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image069.png)</span> (dolnych) wyznaczamy w sposób następujący:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image070.png)</span>

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image071.png)</span>

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image072.png)</span>

gdzie <span style="font-size:11.0pt;line-height:
107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image073.png)</span> jest pewnym współczynnikiem (może być wyznaczony eksperymentalnie w zależności od warunków.

W analogiczny sposób wyznaczamy <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image074.png)</span> (współrzędne <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image075.png)</span> górnych wierzchołków prostokąta ROI) i <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image076.png)</span> (dolnych).

Poniższy rysunek dobrze obrazuje ogólną koncepcję oraz wyznaczone parametry.

![](readme_pliki/image077.jpg)

Zobrazowany wybór wierzchołków prostokąta ROI, źródło: [8]

Opis rysunku 1:

(a)<span style="font:7.0pt &quot;Times New Roman&quot;"></span> obraz binarny z wydzieloną sylwetką i funkcje x-projection (w formułach jako <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image078.png)</span> oraz y-projection,

(b)<span style="font:7.0pt &quot;Times New Roman&quot;"></span> histogram osi <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image064.png)</span> oraz wybór współrzędnych <span style="font-size:11.0pt;line-height:107%;font-family:
&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image064.png)</span> wierchołków prostokąta ROI,

(c)<span style="font:7.0pt &quot;Times New Roman&quot;"></span> ilustracja prostokąta ROI.

Stosunek wysokość/szerokość <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image079.png)</span> można wyliczyć w następujący sposób:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image080.png)</span>

Poniższy rysunek obrazuje rozkład parametru <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image079.png)</span> w zależności od postaw:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>stojącej lub osoby chodzącej,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>siedzącej lub kucającej,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>leżącej.

![](readme_pliki/image081.png)

Porównanie rozkładów parametru r (stosunku wysokość/szerokość prostokąta ROI sylwetki) w zależności pozycji monitorowanej osoby, źródło [8].

Autorzy pracy [8] proponują następnie detekcję upadku opartą o dwu-stanowy automat.

Jak widać na poniższym rysunku umiejscowienie kamery może mieć kluczowe znaczenie, jeżeli korzysta się z metody prostokąta do analizy pojedynczej klatki. Przy wysoko umiejscowionej kamerze, w zależności od kąta stosunek wysokość/szerokość sylwetki człowieka może niezmienić się istotnie podczas upadku.

![](readme_pliki/image082.jpg)

Przykład detekcji upadku przy kamerze umiejscowionej 2 metry nad podłogą, źródło [8]

#### Analiza elipsy ROI

Autorzy pracy [11] proponują zbudowanie elipsy, w którą będzie wpisana sylwetka człowieka.

Elipsę definiuje się przy pomocy środka <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image083.png)</span>, orientacji <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image084.png)</span>, długości wielkiej półosi <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image085.png)</span> oraz małej półosi <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image086.png)</span>.

Dla obrazu traktowanego jako ciągła funkcja z przestrzeni dwuwymiarowej do przestrzeni jednowymiarowej (obraz w skali szarości) wspomniane momenty możemy zdefiniować następująco:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image087.png)</span>

dla <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image088.png)</span>.

Środek elipsy jest wyznaczany za pomocą momentów <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image089.png)</span> w sposób następujący:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image090.png)</span>

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image091.png)</span>

Moment centralny oblicza się analizy sylwetki za pomocą współrzędnych środka elipsy:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image092.png)</span>

Kąt między główną osią elipsy sylwetki, a osią <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image064.png)</span> obrazu liczymy w sposób następujący:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image093.png)</span>

Aby obliczyć długość wielkiej półosi <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image085.png)</span> oraz długość małej półosi <span style="font-size:11.0pt;line-height:107%;font-family:
&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image086.png)</span> elipsy, należy obliczyć najmniejszy moment bezwładności <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image094.png)</span> oraz największy moment bezwładności <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image095.png)</span>. Można to zrobić przy pomocy wartości własnych macierzy kowariancji:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image096.png)</span>

Wartości własne powyższej macierzy dane są wzorami:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image097.png)</span>

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image098.png)</span>

Następnie długość wielkiej półosi <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image085.png)</span> i długość małej półosi <span style="font-size:11.0pt;line-height:107%;font-family:
&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image086.png)</span> elipsy liczy się:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image099.png)</span>

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image100.png)</span>

Obliczamy parametr <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image101.png)</span> jako stosunek wielkiej półosi do małej półosi elipsy:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image102.png)</span>

Tak wyznaczona elipsa może służyć do analizy pojedynczego kadru z obrazu wideo lub analizy zachowania osoby na przestrzeni fragmentu wideo. Wtedy brana jest pod uwagę zmiana parametrów elipsy w czasie.

Jeżeli chodzi o rozpoznanie sylwetki, metoda wyznaczania elipsy jest w stanie klasyfikować klatki wideo jako momenty upadku, kiedy kąt <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image084.png)</span> między wielką półosią elipsy, a osią <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image064.png)</span> jest zbliżony do <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image103.png)</span> stopni. Metoda polegająca na prostokącie w tym przypadku okazuje się nieskuteczna.

Poniższy rysunek ilustruje porównanie metody prostokąta i elipsy w kontekście wykrywania momentu upadku.

![](readme_pliki/image104.jpg)

Dopasowanie prostokąta ROI i elipsy ROI w chwili upadku osoby, źródło: [11].

Metoda elipsy jednak najlepiej sprawdza się przy analizie fragmentu wideo, na którym możemy obliczyć odchylenie standardowe stosunku <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image105.png)</span> oraz kąta <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image084.png)</span>. Wysokie odchylenia standardowe wskazują na gwałtowne ruchy, oznaczające często upadek. Metody wykorzystywania zmian elipsy ROI w czasie do detekcji upadku zostaną zostały opisane w [10], [11].

![](readme_pliki/image106.png)

Elipsy opisujące sylwetkę człowieka  w różnych pozycjach i podczas różnych akcji, źródło: [10].

Poniższa ilustracja przedstawia zmiany w czasie wariancji parametrów <span style="font-size:11.0pt;line-height:107%;font-family:
&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image105.png)</span> oraz <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image084.png)</span>.

Parametr <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image107.png)</span> reprezentuje generalną zmianę pozycji wydzielonej sylwetki (czyli ruch).

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image108.png)</span>

Gdzie <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image109.png)</span> to obraz zmian (motion history image). Współczynnik jest następnie skalowany, 0% oznacza brak ruchu, a 100% pełny ruch. Blob oznacza wydzieloną sylwetkę.

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image110.png)</span>

![h](readme_pliki/image111.png)

Ilustracja zmian współczynników w czasie, źródło: [11].

Osoba w następujących punktach oznaczonych na wykresie:

1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> siada,

2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> wstaje,

3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> przewraca się,

4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> nie rusza się.

Ponieważ system w artykule [11] analizuje również to, czy osoba rusza się po upadku, detekcja następuje w punkcie czwartym. Na wykresie widać także progi parametrów które twórcy wyznaczyli eksperymentalnie w celu detekcji upadku.

Jak widać zmiana sylwetki człowieka w czasie może skutecznie służyć detekcji upadku.

Autorzy artykułu [11] przeanalizowali 24 filmy na których widać codzienne czynności (chodzenie, siadanie, wstawanie, kucanie) oraz 17 symulowanych upadków (upadki do przodu, do tyłu, upadki podczas siadania, stracenie równowagi). <span lang="EN-US">Wyniki są następujące:</span>

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>True Positive (upadek wystąpił i został poprawnie zaklasyfikowany): 15

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>False Negative (upadek wystąpił, ale czynność nie została zaklasyfikowana jako upadek przez algorytm): 2

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>False Positive (upadek nie wystąpił, ale czynność została zaklasyfikowana jako upadek algorytm): 3

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>True Negative (upadek nie wystąpił i czynność nie została zaklasyfikowana jako upadek algorytm): 21

Wyniki badań są obiecujące. Skuteczność wykrywania to 88%, przy niskiej ilości czynności codziennych zaklasyfikowanych jako upadek przez aglorytm. Autorzy sugerują, że dodatkowo można obniżyć ilość fałszywych alarmów przez zdefiniowanie na obrazie miejsca braku aktywności osoby (kanapa, łóżko). Wspomniane miejsca można określić w sposób automatyczny.

#### Wyznaczenie kąta głównej osi sylwetki

Autorzy artykułu [1] proponują metodę polegającą na wyznaczaniu głównej osi sylwetki oraz stosunku wariancji w osiach poziomej i pionowej. Warto zaznaczyć, że system który prezentują polega jednak na dwóch kamerach w pomieszczeniu.

Najpierw należy wyznaczyć macierz kowariancji <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image112.png)</span>:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image113.png)</span>

gdzie <span style="font-size:11.0pt;line-height:
107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image114.png)</span> oznacza średnią średnią, a <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image115.png)</span> oraz <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image116.png)</span> są rozkładami prawdopodobieństwa pikseli sylwetki zmiennych w osi <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image064.png)</span> oraz <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image117.png)</span>. <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image118.png)</span> oznacza wartość oczekiwaną.

Stosunek wariancji w osi poziomej pionowej <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image105.png)</span> opisujemy wzorem:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image119.png)</span>

Kąt głównej osi może być znaleziony za pomocą rozkładu macierzy kowariancji <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image112.png)</span> na wartości osobliwe (SVD- Singular Value Decomposition):

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image120.png)</span>

Kąt pomiędzy osią poziomą kadru, a główną osią <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image121.png)</span> oblicza się w sposób następujący:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image122.png)</span>

Proces jest podobny do analizy głównych składowych (PCA- principal component analysis). Główna oś jest zdeterminowana przez składową o największej wariancji.

Dla sylwetki osoby stojącej <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image123.png)</span>

Jedna kamera może mierzyć kąt głównej osi tylko w swojej płaszczyźnie. Zatem przynajmniej dwie prawie prostopadłe kamery potrzebne są do mierzenia orientacji głównej osi w dwóch ortogonalnych płaszczyznach.

Z pierwszej kamery otrzymujemy <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image124.png)</span>, oraz <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image125.png)</span>. Podobnie z drugiej kamery otrzymujemy <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image126.png)</span>, oraz <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image127.png)</span>. Ta przestrzeń czterowymiarowa może być zredukowana do przestrzeni dwuwymiarowej w następujący sposób:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image128.png)</span>

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image129.png)</span>

Następnie autorzy pracy [1] proponują wykrycie głowy człowieka oraz system który polega na śledzeniu zmian położenia głowy oraz zachowania głównej w osi sylwetki w czasie. Wykrycie głowy opiera się częściowo na analizie koloru skóry głowy.

![](readme_pliki/image130.jpg)

Przykładowe rozpoznanie sylwetki za pomocą głównej osi oraz rozpoznania głowy, źródło: [1].

Rysunek przedstawia:

•<span style="font:7.0pt &quot;Times New Roman&quot;"></span> A, E- kolorowe obrazy,

•<span style="font:7.0pt &quot;Times New Roman&quot;"></span> B, F- główna oś (na zielono) oraz wykryta skóra osoby,

•<span style="font:7.0pt &quot;Times New Roman&quot;"></span> C- część skóry najbardziej zbliżona do głowy według modelu

•<span style="font:7.0pt &quot;Times New Roman&quot;"></span> D, G- pozycja głowy według modelu

![](readme_pliki/image131.jpg)

Przykładowe obrazy, na których osoba wchodzi w interakcję z przedmiotami (nie kwalifikujące się jako upadek), źródło: [1].

Opis ilustracji:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>pierwszy i drugi kadr przedstawia tą samą sytuację z dwóch perspektyw- osoba przenosi torbę, <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image132.png)</span>°, <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image133.png)</span>,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>podobnie trzeci i czwarty kadr- osoba przesuwa krzesło, <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image134.png)</span>°, <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image135.png)</span>

Jak widać, w przypadku niedokładnego wydzielenia sylwetki z tła (wraz z przedmiotami) kąt <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image105.png)</span> zbliżony do 90, może sugerować upadek. Przy zastosowaniu drugiej kamery (dla której kąt odpowiada pozycji stojącej) można nie kwalifikować powyższych sytuacji jako upadek, szczególnie gdy weźmie się pod uwagę pozycję głowy.

![](readme_pliki/image136.jpg)

Przykładowe obrazy, które system kwalifikuje jako upadek, źródło: [1].

Opis ilustracji:

pierwszy i drugi kadr- obserwowana osoba czyta i jest zasłonięta przez biurko na obrazach z dwóch kamer, <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image137.png)</span>°, <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image138.png)</span>,

trzeci i czwarty kadr- obserwowana jest częściowo zasłonięty na obydwu kadrach, jednakże sytuacja jest zakwalifikowana jako upadek, <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image139.png)</span>°, <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image140.png)</span>,

podobnie trzeci i czwarty kadr- przesuwa krzesło- <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image134.png)</span>°, <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image135.png)</span>

Jak widać, w przypadku niedokładnego wydzielenia sylwetki z tła (wraz z przedmiotami) kąt <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image105.png)</span> zbliżony do 90, może sugerować upadek. Przy zastosowaniu drugiej kamery (dla której kąt odpowiada pozycji stojącej) można nie kwalifikować powyższych sytuacji jako upadek, szczególnie gdy weźmie się pod uwagę pozycję głowy.

![](readme_pliki/image141.jpg)

Błędna segmentacja (wydzielenie sylwetki z tła) będąca przyczyną złej klasyfikacji, źródło: [1]

Opis ilustracji: Powyższy rysunek przedstawia błędną segmentację zdjęcia. Środek osoby nie jest rozpoznany.

Lewy obrazek- maska sylwetki po wydzieleniu.

Środkowy obrazek- rezultat kwalifikacji całej maski jako jednego obiektu (zielona linia obrazuje główną oś).

Prawy obrazek- rezultat kwalifikacji maski jako dwóch osobnych obiektów (zielone linie obrazują główne osie obu obiektów).

Autorzy pracy przeprowadzili analizę zależności kąta głównej osi sylwetki w stosunku do osi poziomej <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image142.png)</span>, oraz stosunku wariancji w osi poziomej do osi pionowej <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image105.png)</span> w zależności od wykonywanej czynności. Wyniki prezentowane są poniżej.

![praca1-wykres](readme_pliki/image143.png)

 Wykres zależności kąta głównej osi sylwetki <span style="font-family:&quot;Times New Roman&quot;,serif">ɸ</span> i stosunku wariancji w osi poziomej pionowej <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:3.0pt">![](readme_pliki/image144.png)</span> w zależności od wykonywanej czynności, źródło: [1].

### <a name="_Toc494548794"></a><a name="_Toc494697093"><span lang="EN-US">6.5.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Śledzenie pozycji głowy.</span></a>

W niektórych pracach (między innymi [1], [6]) śledzenie głowy osoby pełni rolę wspomagającą główny system podejmowania decyzji, czy upadek nastąpił czy nie. Istnieją także prace (np. [16]), które opisują algorytm polegający jedynie na śledzeniu głowy i jej prędkości w czasie.

Motywacją badania prędkości głowy jest fakt, że ludzie rzadko wykonują nią gwałtowne ruchy. Jeżeli sama głowa ma dużą prędkość podczas normalnych czynności, nie wiąże się to ze zmianą kąta głównej osi człowieka. Przykładem codziennych czynności, podczas których zmienia się prędkość głowy, może być kucanie (np. w celu wiązania butów), siadanie lub wstawanie z krzesła. Na charakterystykę prędkości głowy w zależności od wykonywanej czynności ma również wpływ wiek obserwowanej osoby. Oczywiście osoby starsze nie będą gwałtownie zmieniać pozycji.

Dodatkowo warto zwrócić uwagę na fakt, że zazwyczaj głowa jest widoczna na obrazie z kamery. Rozpoznawanie sylwetki może zawodzić, kiedy jest ona częściowo zasłonięta, a zdarza się to często (niepełny kadr, meble, itp.). Jednak na kamerach umieszczonych w rogu pomieszczenia głowa jest widoczna cały czas w całości, co czyni ją obiektem odpornym na różne zakłócenia.

Autorzy pracy [1] traktują śledzenie jako dodatkowa weryfikacja działania głównego algorytmu. Czasami zdarza się, że kamera rejestruje gwałtowne zmiany kąta głównej osi sylwetki. Na przykład osoba stojąca za stołem (czyli widoczna jedynie od pasa w górę) może wyciągnąć ręce. Powoduje to bardzo szybką zmianą kąta głównej osi sylwetki <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image142.png)</span> nawet o 90 stopni. Zazwyczaj podczas takich czynności pozycja głowy nie zmienia się znacznie. Warto przypomnieć, że autorzy pracy [1] w swoim systemie używają dwóch kamer, co oznacza że zazwyczaj głowa jest rozpoznana na przynajmniej jednej z nich, ponieważ twarz jest przynajmniej częściowo zwrócona do któreś z kamer. Zazwyczaj zatem głowa jest śledzona przez cały czas. Jeżeli system detekcji zakwalifikuje zachowanie osoby jako upadek, ale równocześnie na co najmniej jednej z kamer widać, że pozycja głowy nie zmieniła się, klasyfikacja jako upadek jest odrzucana. Ma to znaczny wpływ na zmniejszenie ilości błędnych klasyfikacji upadku (False Positive).

W celu znalezienia głowy na scenie, autorzy pracy [1] korzystają z modelu CrCb opartego na kolorze skóry, opisanego w [9]. Model uczony jest na fotografiach zawierających różne części ciała z odsłoniętą skórą. W ten sposób uzyskany jest model gaussowski oparty na kolorze skóry. Piksele z wartościach <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image145.png)</span>, <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image146.png)</span> jest klasyfikowany jako skóra, jeżeli równanie

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image147.png)</span>

<span style="line-height:150%"> </span>

<span style="line-height:
150%">gdzie</span> <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:11.5pt">![](readme_pliki/image148.png)</span><span style="line-height:150%">,</span> <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:11.5pt">![](readme_pliki/image149.png)</span><span style="line-height:150%">to wartości średnie, oraz macierz kowariancji</span> <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image150.png)</span><span style="line-height:150%"> z modelu gaussowskiego.</span>

<span style="line-height:
150%">Próg</span> <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image151.png)</span><span lang="EN-US" style="line-height:
150%"> </span><span style="line-height:150%">jest wybrany doświadczalnie i wynosi 10.</span>

W czasie szukania głowy, obszary z widoczną skórą są łączone z sylwetką. Największy obszar skóry najbardziej oddalony od głównej osii sylwetki jest kandydatem na głowę.

![](readme_pliki/image152.jpg)

Ilustracja dopasowywania głowdy do sylwetki

<span style="font-size:12.0pt;color:#333333;background:white"> </span>

Kandydat na głowę powinien spełniał 2 kryteria:

zawierać co najmniej 15 pikseli o kolorze skóry oraz być nie większy 5% całej wielkości obiektu,

jego mimośród powinien mieścić się między 0.5, a 2.

Kiedy głowa już zostanie znaleziona, może być śledzona w czasie nagrania. Na kolejnych klatkach obszar o kolorze skóry zbliżony pozycją najbardziej do poprzedniej pozycji głowy traktowany jest jako głowa. Autorzy pracy [1] zawężają obszar przeszukiwania ze względu na  niską ilość klatek na sekundę kamery, w celu uniknięcia pomyłki np. głowy z ręką podczas gwałtownych ruchów. Jeżeli żaden z obszarów nie jest klasyfikowany jako głowa, ze względu na powyższy warunek, procedura szukania głowy wykonywana jest od nowa. Opisany system polegający na kolorze skóry wg autorów sprawdza się dobrze ze względu na zastosowanie przez nich dwóch kamer. W sytuacji, gdy osoba nie jest zwrócona twarzą chociaż częściowo do kamery i przy zastosowanie tylko jednej kamery, mogą wystąpić problemy. Istnieją jednak inne algorytmy śledzenia głowy (artykuł [16]), które według ich autorów sprawdzają się dobrze pomimo zastosowania jednej kamery o niskiej jakości. Artykuł [16] zostanie opisany w dalszej części tego podrozdziału.

Istnieją także próby integracji algorytmu śledzenia głowy wraz z estymacją postawy poprzez stosunek wysokość/szerokość prostokąta ROI. Autorzy pracy [6] przeanalizowali 24 20-minutowe filmy na których upadek pojawił się podczas ostatnich 2 minut trwania filmu. Ostatecznie uzyskano 240 klasyfikowanych zachowań, z czego 24 były upadkami. Warunki eksperymentów były raczej trudne, ponieważ obejmowały zmiany oświetlenia, prześwietlenia oraz analizę więcej niż jednej osoby na filmie. Poniższy wykres obrazuje parametry zarejstrowane przez system w kontekście wystąpienia rzeczywistego upadku lub zdarzenia nie będącego upadkiem.

![HS-zrodlo6](readme_pliki/image153.png)

Analiza prędkości głowy wraz ze stosunkiem prostokąta ROI w kontekście zdarzeń upadków i nie-upadków, źródło: [6].

<span style="font-size:12.0pt;line-height:150%;font-family:&quot;Calibri&quot;,sans-serif;color:#333333;background:white">  
</span>

Na wykresie łatwo zauważyć, że wiele upadków ma charakterystykę zbliżoną do nie-upadków. Autorzy pracy [6] po analizie doszli do wniosku, że 90% takich przypadków ma następujące przyczyny:

1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> w 25 % przypadków 2 osoby były obecne w pomieszczeniu,

2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> w 20% przypadków inny obiekt w tle miał zbliżone rozmiary do śledzonej osoby.

W obu przypadkach, system przeskakiwał ze śledzenia osoby na drugą osobę lub wspomniany obiekt, wykrywając gwałtowne ruchy i zmiany stosunku wysokość/szerokość prostokąta ROI.

3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> W 25% przypadków, sylwetka osoby była rozpoznawana jako 2 obiekty o podobnym kształcie. Spowodowane było to prześwietleniem sceny, noszeniem przez osobę koszulki o kolorze zbliżonym do tła, błędnym klasyfikowaniem części osoby jako tła spowodowanym mechanizmem aktualizacji tła. Prowadziło to do zmiany stosunku prostokąta ROI i wykrywaniem szybkiego ruchu przez przeskakiwanie sytemu między dwoma rozpoznanymi częściami sylwetki.

4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> W 20% pozostałych przypadków błędy wynikały ze z interferencją odbicia lub przesuwanymi meblami.

Praca [16] natomiast opisuje system, który polega wyłącznie na śledzeniu głowy. Co więcej, mimo zastosowania jednej, niskiej jakości kamery pozycja głowy śledzona jest w trzech wymiarach.

Kamera umieszczona jest w rogu pomieszczenia przy suficie. Autorzy pracy używają niskiej jakości kamery internetowej USB o kącie widzenia 70 stopni, co dodatkowo powoduje zniekształcenia geometryczne.

Główne cechy systemu to:

1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Śledzenie głowy osoby. Wybór podyktowany jest tym, że głowa zazwyczaj jest widoczna w kamerze i przemieszcza się w szybkim tempie podczas upadku.

2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Śledzenie głowy 3D za pomocą jednej skalibrowanej kamery. Śledzenie 2D okazuje się niewystarczająca, z tego względu autorzy zdecydowali się na śledzenie 3D dla uzyskania lepszych efektów. Zazwyczaj do śledzenia 3D wykorzystuje się co najmniej 2 kamery, chociaż autorzy twierdzą, że ich system działa dobrze w oparciu o tylko jedną kamerę.

3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Detekcja upadku w oparciu o charakterystykę prędkości za pomocą śladu głowy 3D.

Zgodnie z cytowaną pracą [17] autorzy opisywanej pracy [16] twierdzą że na obrazie 2D głowa jest dobrze opisywana przez elipsę. Zgodnie z tym duchem reprezentują głowę w 3D jako elipsoidę.

Do zlokalizowania głowy używają algorytmu „POSIT”, który jako wejście przyjmuje:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Model 3D głowy zdefiniowany za pomocą danych antropometrycznych.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Zrzutowane do 2D dane odpowiadających punktów oraz po korekcji zniekształceń.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>dane kamery pobrane z pomocą Matlaba, służące do korekcji zniekształceń.

![znieksztalcenia](readme_pliki/image154.png)

Lewy kadr pokazuje obraz z kamery, prawy obraz jest już po korekcji zniekształceń, źródło: [18].

Algorytm POSIT zwraca dane pozycji głowy w relatywnym (względem kamery) układzie współrzędnych, tak że mogą być reprezentowane przez jednorodną macierz 4x4 (homogeneous matrix). Ponieważ chcemy znać pozycję głowy w standardowym układzie współrzędnych, musimy przekształcić dane z układu jednorodnego w sposób następujący:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image155.png)</span>

Gdzie:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:11.5pt">![](readme_pliki/image156.png)</span> to znana pozycja w układzie jednorodnym kamery,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:11.5pt">![](readme_pliki/image157.png)</span> to pozycja głowy w układzie jednorodym obliczonym przez algorytm „POSIT”,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:11.5pt">![](readme_pliki/image158.png)</span> to szukana pozycja w standardowym układzie współrzędnych.

Dzięki obliczeniu pozycji głowy, można śledzić głowę reprezentowaną jako elipsę dzięki filtrowi cząsteczkowemu (particle filter).

Poniższa ilustracja przedstawia pozycję głowy (reprezentowaną jako elipsę podczas upadku na dwóch następujących po sobie klatkach.  Przemieszczenie to 70 pikseli, przy 30 FPS i obrazie w rozdzielczości 640x480.

![](readme_pliki/image159.jpg)

Reprezentacja głowy jako elipsy, źródło: [16].

Istnieje wiele algorytmów śledzenia głowy na filmie. Autorzy pracy [16] zdecydowali się użyć algorytmu filtra cząstekowego (w literaturze anglojęzycznej: „particle filter”, ”Condensation algorithm” lub „Conditional Density Propagation”). Polega on na aproksymowaniu funkcji gęstości prawdopodobieństwa stanu systemu poprzez zbiór przykładowych próbek (cząstek) z nałożonymi wagami. Celem jest estymacja rozkładu prawdopodobieństwa <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image160.png)</span> danego wektora <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image161.png)</span> śledzonego obiektu oraz wektora <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image162.png)</span> reprezentującego wszystkie obserwacje w czasie <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image163.png)</span>. Prawdopodobieństwo może być aproksymowane przez zbiór <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image164.png)</span> ważonych próbek

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image165.png)</span>

Kroki algorytmu to:

1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Znadź nową próbkę <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image166.png)</span> poprzez resampling starej próbki <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image167.png)</span> za pomocą wag <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image168.png)</span>

2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Oblicz przewidywaną pozycję nowej próbki za pomocą dynamicznego modelu stochastycznego.

3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Oblicz nowe wagi <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image169.png)</span>

Średni stan systemu może być szacowany w czasie <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image163.png)</span> za pomocą wzoru;

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image170.png)</span>

Opisana metoda używająca filtra cząsteczkowego działa dobrze przy małym ruchu, jednak podczas upadku następuje duży ruch (szczególnie jeżeli odświeżanie kamery jest niskie).

Żeby poradzić sobie z tym problemem może być konieczne użycie wielu próbek (cząstek) w celu znalezienia nowej pozycji głowy na obrazie. Może być to wymagać dużej mocy obliczeniowej, przez co być nieefektywne w systemach czasu rzeczywistego. Co więcej, pozycja głowy może nie być dokładna. W związku z czym autorzy pracy [16] proponują kilka poprawek algorytmu.

Sugerują oni, że warto nałożyć 3 filtry cząsteczkowe z własną charakterystyką i zastosowaniem. Każda z próbek to elipsa reprezentowana jako wektor <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image171.png)</span>, gdzie:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image172.png)</span> to środek elipsy,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image173.png)</span>  to długość małej osi,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:11.5pt">![](readme_pliki/image174.png)</span> to stosunek wielkiej osi elipsy do jej małej osi.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image142.png)</span> to kierunek zorientowania elipsy.

Wagi próbek wynikają z gradientu Sobela. Również przedni plan wydzielany jest lokalnie w celu weryfikacji granic elipsy. Ostatecznie znalezione są „rozsądne” wagi.

Pierwszy filtr cząsteczkowy używany jest w celu sprawdzenia, czy pozycja głowy znajduje się w tym samym miejscu. W tym celu do pozycji <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image172.png)</span> dodaje się mały szum. Jeżeli pierwszy filtr nie zwróci twierdzącej odpowiedzi, drugi filtr używany jest do przeszukiwania bardziej oddalonych punktów w celu znalezienia aproksymacji nowej pozycji głowy przy większym szumie. Trzeci filtr poprawia aproksymację drugiego filtru używając słabego szumu na parametrach wektora stanu.

![filtry](readme_pliki/image175.jpg)

Ilustracja działania filtrów w postaci schematu blokowego, źródło: [16].

Poniżej znajdują się dwa kadry z upadku. Pierwszy rysunek przedstawia pierwszą klatkę. Następne 4 rysunki przedstawiają drugą klatkę z ilustracją kolejnych kroków algorytmu. Poniższe ilustracje pochodzą z [16].

![](readme_pliki/image176.png)

Czas t-1\. Głowa reprezentowana jest jako elipsa

![](readme_pliki/image177.png)

Pierwszy filtr

![](readme_pliki/image178.png)

Drugi filtr.

![](readme_pliki/image179.png)

Trzeci filtr.

![](readme_pliki/image180.png)

Czas t. Pozycja głowy została zaktualizowana.

Badania upadków przy pomocy czujników ruchu pokazują, że upadki mają inną charakterystykę prędkości od codziennych czynności, więc klasyfikator oparty o charakterystykę prędkości powinien dobrze sprawdzać się w celu wykrycia upadków.

Autorzy proponują posługiwanie się prędkością pionową (vertical velocity) <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image181.png)</span>, oraz prędkością poziomą <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image182.png)</span> (horizontal velocity). Za pomocą trajektorii elipsy w 3D można obliczyć te prędkości.

![](readme_pliki/image183.jpg)

Trajektoria 3D głowy uzyskana z filmu, źródło: [16].

<span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,sans-serif">  
</span>

Poniższy rysunek ilustruje charakterystykę prędkości podczas wykonywania różnych czynności. Zielona linia oznacza prędkość poziomą <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image184.png)</span>, natomiast niebieska lina reprezentuje prędkość pionową <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image181.png)</span>.  Prezentowane czynności to:

a.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> wstawanie,

b.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> siadanie,

c.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> osoba siedzi,

d.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> ponowne wstawanie,

e.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> chodzenie,

f.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> upadek.

Jak widać wypadek mocno odróżnia się od reszty czynności. Przy pomocy funkcji progowej z odpowiednimi parametrami progowych prędkości można zaklasyfikować czynność jako upadek.

![](readme_pliki/image185.png)

Ilustracja zmiany prędkości pionowej i poziomej podczas różnych czynności, źródło: [16].

Autorzy artykułu przetestowali swój algorytm na 19 filmach prezentujących codzienne czynności oraz symulowane upadki. 9 filmów przedstawiało różne upadki (do przodu, do tyłu, upadek w trakcie siadania, stracenie równowagi). 10 filmów przedstawiało codzienne czynności takie jak siadanie, wstawanie, kucanie. Rozdzielczość filmików to 640x480 pikseli i częstotliwość to 30 klatek na sekundę.  Pozycja głowy jest początkowo inicjalizowana ręcznie na pierwszej klatce, a następnie śledzona przy pomocy filtra cząsteczkowego opisanego powyżej. Trajektoria głowy 3D obliczana jest dla każdego filmu, dzięki czemu można było przeanalizować prędkość poziomą oraz pionową. Czynność upadku jest wykrywana, jeżeli w tym samym czasie pojawia się szczyt funkcji prędkości zarówno poziomej i pionowej i jeżeli ujemne wychylenie prędkości pionowej <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image184.png)</span> wynosi mniej niż -1,5 <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image186.png)</span>, a wychylenie prędkości poziomej <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image181.png)</span> wynosi ponad 2 <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image186.png)</span>. Wyniki są następujące

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>True Positive (upadek wystąpił i został poprawnie zaklasyfikowany): 6

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>False Negative (upadek wystąpił, ale czynność nie została zaklasyfikowana jako upadek): 3

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>False Positive (upadek nie wystąpił, ale czynność została zaklasyfikowana jako uapde): 1

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>True Negative (upadek nie wystąpił i czynność nie została zaklasyfikowana jako upadek): 9

System spełnia oczekiwania twórców. Większość codziennych czynności nie jest klasyfikowana jako upadek i dwie trzecie upadków klasyfikowanych jest poprawnie. Czasami zdarza się, że podczas upadków system traci pozycję głowy, ale nie jest to dużym problemem, ponieważ początkowa zmiana prędkości głowy jest wystarczająca, żeby wykryć upadek. Błędy zazwyczaj powstają kiedy osoba upada z pozycji siedzącej (wtedy prędkość głowy jest zbyt mała) lub kiedy system śledzenia głowy nie działa prawidłowo.

W pracy [18] (rok 2011) ci sami autorzy opisują wyniki swoich badań i moduł śledzenia głowy w celu detekcji upadku zaimplementowany w OpenCV, w C++. Opisane tam są badania z bardziej szczegółowymi wynikami samego algorytmu śledzenia głowy i wyniki na korpusie wideo HumanEva data set.

Poniżej prezentowane jest działanie algorytmu w praktyce. Rysunki pochodzą z [18].

![g1](readme_pliki/image187.png)

Chodzenie (klatki 300-400). Chodząca osoba generuje w przybliżeniu zerową prędkość pionową głowy, jej wysokość jest około 1,5 metra ponad podłogą.

![g2](readme_pliki/image188.png)

Siadanie (klatka 420). Wysokość głowy obniża się do 1 metra z małą prędkością pionową.

![g3](readme_pliki/image189.png)

Upadek (klatka 558) Osoba wstaje i upada z dużą absolutną prędkością pionową głowy (mniej niż -1 m/s). Na końcu upadku głowa znajduje się na wysokosci 50cm.

![hv](readme_pliki/image190.png)

Wertykalna prędkość głowy oraz jej poziom wysokości podczas czynności prezentowanych na kadrach powyżej.

<span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,sans-serif">  
</span>

_<span style="font-size:9.0pt;color:#44546A"> </span>_

### <a name="_Toc494548795"></a><a name="_Toc494697094">6.5.3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Estymacja postawy za środka ciężkości sylwetki (centroidu) oraz VPS (Vertical Projection Histogram).</a>

Autorzy pracy [15] do wykrycia upadku używają trzech informacji:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>środka ciężkości człowieka (centroidu) w sensie figury geometrycznej 2D,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>histogramu pionowego jego sylwetki (Vertical Projection Histogram),

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>długości wykrytej czynności.

Histogram pionowy wylicza się w sposób następujący:

![his-pion](readme_pliki/image191.png)

Ponieważ <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image192.png)</span> ma jednowymiarowy, można użyć metryki do obliczenia odległości dwóch histogramów <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image193.png)</span> oraz  <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image194.png)</span> z dwóch następujących po sobie klatek, może być to odległość <span style="font-size:11.5pt;line-height:150%;font-family:&quot;Times New Roman&quot;,serif">Bhattacharya:</span>

<span style="font-size:11.5pt;line-height:150%;font-family:&quot;Times New Roman&quot;,serif">![wzor-B](readme_pliki/image195.png)</span>

Jednak ze względów obliczeniowych autorzy proponują porównywanie po prostu porównywanie histogramów za pomocą wartości maksymalnych (czyli po prostu wysokości sylwetki):

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image196.png)</span>

![3](readme_pliki/image197.png)

Analiza 3 parametrów czynności- długości trwania (a), pozycji środka ciężkości (b) o raz wysokosci sylwetki (c), źródło: [15].

![p1](readme_pliki/image198.png)

Pozycja centroidu w zależności od czasu podczas upadku, chodzenia, kucania, źródło: [15].

![P2](readme_pliki/image199.png)

Wysokość sylwetki w zależności od czasu podczas upadku, chodzenia, kucania, źródło: [15].

Oczywiście wielkość sylwetki i pozycja centroidu może zmieniać się w zależność od zmiany pozycji w stosunku do kamery (kiedy osoba na przykład zbliża się lub oddala).

W celu detekcji upadku zamiast wektora pozycji centroidu oraz wysokości histogramu, używa się wektora ich znormalizowanych wartości:

![x](readme_pliki/image200.png) 

Gdzie:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image201.png)</span> to środek centroidu w n-tej klatce

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image202.png)</span> oznacza wysokość sylwetki (maksymalną wartość VPS) w n-tej klatce

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>SW to długość czasu okna śledzącego obiektu (np. liczba klatek).

Ponizsze rysunki pochodzą z [15] i przedstawiają rozkład zmian pionowego histogramu oraz centroidu w zależności od wykonywanej czynnosci dla trzech różnych czynności:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Ruch w kierunku do kamery (Motion Type 1),

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Ruch w kierunku od kamery (Motion Type 2),

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Ruch w osii poziomej kamery (Motion Type 3).

<span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,sans-serif">  
</span>

<span style="font-size:12.0pt;color:#333333;background:white"> </span>

![m2](readme_pliki/image203.png)![m1](readme_pliki/image204.png)

![m4](readme_pliki/image205.png)![m3](readme_pliki/image206.png)

<span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,sans-serif">  
</span>

<span style="font-size:12.0pt;color:#333333;background:white"> </span>

![m6](readme_pliki/image207.png)![m5](readme_pliki/image208.png)

Autorzy na tej podstawie wyznaczyli progi:

![progi](readme_pliki/image209.png)

Progi dla każdego z typu ruchów, źródło: [15].

W celu analizy skuteczności algorytmu użyto 30 filmików (15 upadków i 15 filmików przedstawiających chodzenie). 28 czynności zostało poprawnie rozpoznanych, dwa upadki nie zostały rozpoznane.

Na podstawie opisu algorytmu i faktu, że autorzy do ewaluacji nie sprawdzili innych typów czynności (siadanie, kucanie) można domyślać się, że algorytm nie jest specjalnie skuteczny. Na pewno jego zaletą jest prostota implementacji i niskie wymagania obliczeniowe. Ze względów oczywistych algorytm może sprawdzać się źle w przypadku zasłonięcia osoby przez inny obiekt, interakcje z otoczeniem lub niepełny kadr.

### <a name="_Toc494548796"></a><a name="_Toc494697095">6.5.4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Analiza aktywności i strefy pomieszczeń.</a>

Inne podejście do ropoznawania upadków mają autorzy pracy [14]. Proponują tworzenie modelu stref aktywności osoby w danym pomieszczeniu. Po wyuczeniu modelu (przez obserwację danego pomieszczenia przez dłuższy czas i przy różnym oświetleniu) można stworzyć klasyfikator, który pozwala na odróżnienie standardowych czynności od anomalii (upadków). Motywacją jest fakt, że ludzie dużą część czasu spędzają w niektórych miejscach (kanapa, fotel, łóżko), więc wpływ czynnika takiej strefy jest bardzo istotny w modelu detekcji upadku.

![](readme_pliki/image210.png)

Wyróżniające się miejsca: sofa (S), krzesło (C), drzwi od korytarza (H), drzwi tylne (R), źródło: [14].

Autorzy oparli swój system o kamerę o bardzo szerokim kącie widzenia znajdującą się na suficie, w centrum pomieszczenia. Kamera była nieskalibrowana.

Człowiek jest reprezentowany jako elipsa (wektor 5-elementowy), podobnie jak w sposób opisany w dziale „Analiza elipsy” niniejszej pracy. Śledzenie odbywa się przy pomocy filtra cząsteczkowego. Szczególne znaczenie dla analizy hipotetycznej elipsy miał pierścień wokół niej (do określonej ilości pikseli). Ilości pikseli wewnątrz elipsy (i ilość pikseli w tym pierścieniu) stanowi wejście dla funkcji oceniającej elipsę.

![](readme_pliki/image211.png)

Reprezentacja człowieka jako elipsy, źródło: [14].

<span style="font-size:12.0pt;color:#333333;background:white"> </span>

![](readme_pliki/image212.png)

Wewnętrzna elipsa wyznaczona jest na podstawie najmocniejszych próbek (cząstek), zewnętrzna to pierścień na podstawie którego elipsa jest oceniana, źródło: [14].

<span style="font-size:12.0pt;line-height:150%;font-family:&quot;Calibri&quot;,sans-serif;color:#333333;background:white">  
</span>

Podczas śledzenia czynności osoby bierze się pod uwagę jego prędkość i wygładzone trajektorie za pomocą filtra średniej ruchomej (średnia krocząca, moving average filter). Trajektoria brana jest ze środka elipsy.

W zależności od pomieszczenia można wyróżnić kilka miejsc, w których osoba spędza najwięcej czasu. W salonie to, na przykład, krzesła i kanapy. Te miejsca oznaczone są jako strefy bezruchu (inactivity zones). Dla osoby zajmującej te miejsca często nie wykazuje znacznych ruchów. Wyróżniamy również strefy (niezmienne) wejść (entry zones).

Typowa czynność obejmuje wejście do pomieszczenia przez strefę wejść, skorzystanie z jednej lub większej ilości stref bezruchu i ostatecznie wyjście z pomieszczenia również przez strefę wejść. Oczywiście występują inne schematy czynności, jednak ten schemat jest najczęściej powtarzający się. W związku z tym, autorzy proponują żeby oprzeć model zachowania na schemacie semantycznym, w którym cała czynność reprezentowana będzie przez:

1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> skorzystanie ze strefy wejścia (wejście do pomieszczenia),

2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> brak aktywności w strefie bezruchu,

3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> przemieszczanie się między strefami bezruchu,

4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> skorzystanie ze strefy wejście (wyjście z pomieszczenia).

W celu wykrycia czynności odstających od typowych, autorzy wyuczyli model kontekstu przestrzennego (spatial context model) za pomocą estymacji MAP (Maximum a posteriori estimation) modelu mieszanek gaussowskich (Gaussian Mixture model). Dzięki temu możliwe jest oznaczenie charakterystycznych stref bezruchu i stref wejścia. Wyuczony model kontekstu przestrzennego używa się także do automatycznej segmentacji trajektorii ruchu i wykrycia zachowań nietypowych.

![](readme_pliki/image213.png)

Wygładzone trajektorie, strefy bezruchu, oraz strefy wejść, źródło: [14].

Składowe modelu gaussowskiego odpowiadają za oznaczenie stref bezruchu oraz stref wejścia. Każda ze funkcji gęstości prawdopodobieństwa (gęstości prawdopodobieństwa, PDF, Probability Density Function), <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image214.png)</span> dostarcza model przestrzennego przedłużenia strefy <span style="font-size:11.0pt;line-height:107%;font-family:
&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image215.png)</span>.

Strefy wejść mogą być użyte do inicjalizacji i oznaczenia semantycznego etykietami punktów wejścia i wyjścia. Kiedy prędkość osoby spada do poziomu oznaczającego brak aktywności, PDF strefy pozwala na sprawdzenie czy brak aktywności następuje w strefie bezruchu. Do rozpoznania, czy brak aktywności występuje w strefie bezruchu wykorzystuje się prosty algorytm. Prędkość, <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image216.png)</span>, dla każdego kroku szacuje się za pomocą skończonej różnicy ponad 40 okien czasowych i sprawdza się czy wyrażenie <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image217.png)</span> przekracza wartość progową.

W celu demonstracji możliwości opisanej metody, poproszono aktora o przeprowadzenie serii symulacji polegającej na wykonaniu czynności oznaczanych przez H,R,C,S (jak w opisie w jednym z powyższych rysunków. Przykładowo (HSR):

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>wejdź przez korytarz do pomieszczenia,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>usiądź i użyj telefonu,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>wyjdź przez tylne drzwi.

Inny przykład (HCSH):

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>wejdź przez korytarz to pomieszczenia,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>usiądź i użyj telefonu,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>usiądź na sofie,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>wyjdź przez korytarz z pomieszczenia.

Poniższa tabela prezentuje zbiór opisanych sekwencji, długość ich trwania i błędy w rozpoznaniu przez system. Fall oznacza symulację upadku przez aktora (autorzy pracy nie mogli zdobyć nagrań prawdziwych upadków starszych ludzi z oczywistych względów).

![](readme_pliki/image218.jpg)

Oznaczone sekwencje, źródło: [14].

Błędy w rozpoznaniu nastąpiły w 3 spośród 73 sekwencji. Strefy wejść i strefy bezruchu były wyuczone na podstawie danych trenujących przez przestrzenne modele kontekstowe (spatial context models). Zastosowanie metody uczenia Bayesa w konsekwencji pozwoliło na oznaczenie stref w taki sposób, że relacja opisu stref była taka sama jak w instrukcjach danych aktorowi.

Poniższe ilustracje prezentują trajektorie w oznakowanych przez algorytm strefach, oraz rozpoznanie nietypowego zachowania (upadku). Źródło: [14].

![](readme_pliki/image219.jpg)![](readme_pliki/image220.jpg)

<span style="font-size:12.0pt;line-height:150%;font-family:&quot;Calibri&quot;,sans-serif;color:#333333;background:white">  
</span>

Zaprezentowana metoda wydaje się dobrym wsparciem dla innych metod detekcji upadku. Autorzy zastosowali dość nietypowe umieszczenie kamery (w centrum pokoju) i o dość nietypowych parametrach (bardzo szerokokątnej).  Trudno domyślić się jak metoda sprawdzać w kontekście kamery w rogu pokoju. Na pewno jednak automatyczne oznaczanie stref braku aktywności może okazać się przydatne, szczególnie że kamera nie wymaga kalibracji (natomiast minusem jest konieczność uczenia modelu dla każdego pomieszczenia i umiejscowienia kamery).

### <a name="_Toc494548797"></a><a name="_Toc494697096"><span lang="EN-US">6.5.5.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Analiza ruchu po upadku</span></a>

Większość systemów detekcji upadku skupia się na analizie samego upadku. Niektórzy autorzy starają się jednak sprawdzać również, czy po upadku następuje okres czasu w których osoba się nie rusza. Taki test na wykrycie ruchu w fazie po upadku co prawda nie wpływa na skuteczność detekcji upadku, jednak pozwala na odróżnienie upadków niebezpiecznych (po których człowiek traci przytomność lub nie jest w stanie samodzielnie wstać) od upadków po których osoba może wstać samodzielnie (lub z pomocą) i nie ma potrzeby alarmowania przez system komputerowy.

<span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,sans-serif">  
</span>

![](readme_pliki/image221.jpg)

Schemat algorytmu, który alarmuje tylko w poważnych  wypadkach, źródło: [18].

![](readme_pliki/image222.png)

Zmiana parametrów w czasie dla poważnych upadków. źródło: [18].

Niektórzy autorzy polegają głównie na czasie bezruchu człowieka w zależności od miejsca w pomieszczeniu ( przykładowo [3] - <span class="MsoHyperlink">[http://tunn.us/arduino/falldetector2.php](http://tunn.us/arduino/falldetector2.php)</span>).  Co prawda trudno mówić tu o detekcji upadku, ale taki system według autorów [3] może lepiej sprawdzać się by stwierdzić, czy dana osoba potrzebuje pomocy. Dla kanapy sugerują, że czas może wynosić 2 godziny, natomiast dla łóżka 12 godzin. Dla podłogi rozsądny próg czasowy może wynosić 2 minuty.

![](readme_pliki/image223.jpg)

System polegjący na Motion History Image by określić czy osoba pozostaje w bezruchu, źródło: [3].

### <a name="_Toc494548798"></a><a name="_Toc494697097">6.5.6.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Ukryte modele markowa korzystające z wideo i audio.</a>

W pracy [2] opisano system detekcji upadku polegającej zarówno na analizie obrazu jak i audio. Autorzy skupili się na analizie stosunku wysokość/szerokość prostokąta ROI <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image105.png)</span>. Doszli do wniosku, że analiza transformaty falkowej (wavelet transform) parametru <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image105.png)</span> jest bardziej odpowiednia do detekcji upadki, niż standardowa analiza zmiany <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image105.png)</span> w czasie, ponieważ transformata falkowa igoruje punkty stacjonarne, a dobrze wychwytuje nagłe zmiany sygnału. Proponują wytrenowanie ukrytego modelu markowa (Hidden Markov Model- HMM), który będzie określał czy nastąpił upadek, czy nie. Dodatkowo użyty został sygnał audio z kamerki jako parametr wspomagający odróżnienie czynności chodzenia od upadku.

Zatem w odróżnieniu do poprzednich prac, analizie podlegają współczynniki transormaty falkowej <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image105.png)</span>.

Poniższy wykres (górny) przedstawia zależność stosunku wysokość/szerokość prostokąta ROI <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image105.png)</span> w zależności od czasu dla osoby chodzącej. Wykres tuż pod nim przedstawia zmiany współczynników transformaty falkowej <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image105.png)</span> w zależności od czasu (dla tej samej czynności- chodzenia). Poniższe progi T1, T2 (przy założeniu T2>T1>0) wyznaczają granicę przejść między stanami w ukrytym modelu markowa.

![](readme_pliki/image224.jpg)

Osoba chodząca, źródło: [2].

Poniższy rysunek przedstawia te same wykresy, ale dla osoby która upadła.

![](readme_pliki/image225.jpg)

Osoba podczas upadku, źródło: [2].

W czasie <span style="font-size:11.0pt;line-height:
107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image226.png)</span>:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>jeżeli <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image227.png)</span> (gdzie <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image228.png)</span> oznacza i-ty współczynnik transformaty falkowej), oznacza że osiągany jest stan S1\.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Jeżeli <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image229.png)</span>, osiągany jest stan S2\.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Jeżeli T2 < <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image230.png)</span>, osiągany jest stan 3

W czasie fazy trenowania modelu wyznaczane są prawdopodobieństwa przejść

między stanami <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image231.png)</span> oraz <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image232.png)</span> (odpowiednio dla osoby chodzącej i upadającej), gdzie

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image233.png)</span>

Do treningu użyto 20 filmów.

![](readme_pliki/image234.jpg)

Trójstanowe modele Markowa dla chodzenia (a) oraz upadku (b), źródło: [2].

Dla osoby chodzącej, ponieważ ruch jest prawie-okresowy, oczekujemy podobnych prawdopodobieństw przejść między stanami. Zatem współczynniki <span style="font-size:11.0pt;line-height:107%;font-family:
&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image085.png)</span> powinny być podobne do siebie. Jednakże, jeżeli osoba przewraca się, sygnał falki przyjmuje wartości bliskie zeru. Zatem oczekujemy wyższego prawdopodobieństwa <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image235.png)</span>, niż innych wartości <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image086.png)</span> w modelu upadku, co odpowiada większemu prawdopodobieństwu stanu S1\. Stan S2 odpowiada za histertezę, a zatem nie pozwala na nagłe przejścia ze stanu S1, do stanu S2 i na odwrót.

Przedstawiony model, czyli analiza samego <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image105.png)</span>, nie spełniał oczekiwań, gdyż nie odróżniał upadków od czynności siadania i zostawania w tej pozycji przez chwilę czasu. W związku z tym autorzy doszli do wniosku, że warto analizować sygnał audio w celu odróżnienia tych dwóch czynności.

![](readme_pliki/image236.jpg)

Sygnał audio dla upadku (a), oraz czynności następujących po sobie: mówienia, schylania, mówienia, chodzenia, schylania, mówienia (b), źródło: [2].

![](readme_pliki/image237.jpg)

Wykres sygnału falki dla powyższych czynności, źródło: [2].

Dla każdego okna <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;position:relative;top:10.0pt">![](readme_pliki/image238.png)</span> liczymy wariancje sygnału audio <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image239.png)</span> oraz przejść przez zero <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image240.png)</span>

Okazuje się, że sygnał audio jest prawie-okresowy dla chodzenia w kontekście  <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image239.png)</span> oraz <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image240.png)</span> Jednakże, jeżeli osoba potknie się i przewróci, <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image241.png)</span> rośnie, a <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image239.png)</span> maleje. W związku z tym definiowany jest współczynnik:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:22.5pt">![](readme_pliki/image242.png)</span> ,

który przyjmuje wartości nieujemne.

Wariancja parametru <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image243.png)</span> zaprezentowana jest na poniższych wykresach.

![](readme_pliki/image244.jpg)

Kappa dla upadku (a), chodzenia (b), oraz mówienia, schylania, mówienia, chodzenia, schylania chodzenia (c), źródło: [2].

W tym przypadku, autorzy używają trzy trójstanowe modele markowa jako klasyfikatory dla chodzenia (a), mówienia (b), oraz upadku (c) oparte na sygnału audio.

Zatem dla i-tego okna:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>jeżeli <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image245.png)</span> osiągany jest stan S1,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>jeżeli <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image246.png)</span> osiągany jest stan S2,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>jeżeli <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image247.png)</span> osiągany jest stan S3.

Model jest trenowany w celu znalezienia <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image248.png)</span> <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image249.png)</span> dla <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image250.png)</span> za pomocą 500 próbek audio.

Klasyfikatory oparte o wideo i o audio łączone są w ten sposób, że czynność oznaczona jest jako upadek tylko wtedy, gdy zarówno HMM oparty o audio i HMM oparty o wideo oznaczają upadek.

Estymacja modelu jest następująca (dla modelu opartego o wideo, a także wideo + audio):

![](readme_pliki/image251.jpg)

Wyniki dla zbioru testowego, źródło: [2].

Testy zostały przeprowadzone na komputerze AMD AthlonXP 2000+ 1.66GHz w czasie rzeczywistym. Na wszystkich 64 filmach jest tylko jedna poruszająca się postać. Jak wynika z tabeli, model oparty wyłącznie o wideo nie jest w stanie w ogóle określić czy dana osoba usiadła, czy przewróciła się. W odróżnieniu do niego, model oparty o wideo + audio radzi sobie w tym przypadku.

<span style="font-size:12.0pt;color:#333333;background:white"> </span>

### <span lang="EN-US">6.5.7.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span> <a name="_Toc494548799"></a><a name="_Toc494697098"><span lang="EN-US">PCANet + SVM</span></a>

Ciekawą metodę opartą o deep learning i SVM (Support Vector Machine, Maszyna wektorów nośnych) zaprezentowali w 2015 roku autorzy pracy [13]. Proponują oni framework, w którym początkowo model PCANet najpierw jest uczony za pomocą pojedynczych oznaczonych klatek przez człowieka, by zaklasyfkikować postawę osoby na klatce. Następnie za pomocą tego modelu używając SVM trenowany jest model dla sekwencji klatek (filmu). Autorzy przeprowadzili badania porównawcze z innymi metodami. Z eksperymentów wynika, że algorytm jest wysoce skuteczny.

Algorytm opiera się o na dwóch modelach. Pierwszy model uczy się na pojedynczych klatkach, które są zaetykietowane jako:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>0- osoba nie upadająca,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>1- osoba w trakcie upadku,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>2- osoba która upadła.

Początkowo autorzy próbowali klasyfikować tylko dwa stany. Okazało się jednak, że stan pośredni jest bardzo pomocny, ponieważ niektóre upadki mogą kończyć się w niejednoznacznej pozycji. Przykładowo osoba przewracająca się zostaje na kolanach. Wtedy algorytm może mieć problem z rozróżnieniem, czy osoba przewróciła się czy nie. Ten stan jest zatem reprezentowany przez etykietę 1- osoba w trakcie upadku. Sylwetka osoby wydzielana jest za pomocą algorytmu ViBe. Następnie w celu nauczenia modelu klasyfikacji pojedynczych, wydzielonych klatek, używany jest PCANet (metoda deep learning) oraz SVM do klasyfikacji.

Ponieważ upadek jest raczej zdarzeniem ciągłym, drugi model (Action model) działa na sekwencji klatek. Przykładowo 30 ostatnich klatek. Korzysta on z oznaczeń dla każdej klatki modelu pierwszego i w ten sposób przy pomocy SVM uczony jest klasyfikacji krótkiego fragmentu filmu jako upadek lub nie-upadek. Ilustracja modeli pokazana jest na poniższym rysunku.

<span style="font-size:12.0pt;color:#333333;background:white"> </span>

![](readme_pliki/image252.jpg)

![](readme_pliki/image253.jpg)

Ilustracja modelu pierwszego i drugiego, źródło: [13].

PCANet to framework do rozpoznawania obrazów oparty o PCA (Principal component analysis) Analiza składowych głównych). Autorzy wybrali PCANet do klasyfikacji obrazów z następujących przyczyn. Nisko-poziomowe, ręcznie tworzone klasyfikatory oparte o SIFT, HOG sprawdzają się ogólnie nieźle, jednak w detekcji upadku nie są specjalnie przydatne. Detekcja upadku według autorów powinna opierać się na cechach wysokopoziomowych opartych o deep learning, a nie niskopoziomowych (wybieranych ręcznie), gdyż zapewniają większą niezmienność wariancji wewnątrzklasowej. PCANet natomiast w porównaniu do konwolucyjnych sieci neuronowych (CNN, Convolutional Neural Network), nie wymaga skomplikowanych procesów dobierania parametrów i długiego treningu. Z tego względu PCANet daje się łatwo wytrenować oraz adaptować do zmiennych warunków.

Istnieją implementacje PCANet także o otwartym kodzie źródłowym), na przykład:

<span lang="EN-US" style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span class="MsoHyperlink"><span lang="EN-US">[https://github.com/IshitaTakeshi/PCANet](https://github.com/IshitaTakeshi/PCANet)</span></span> <span lang="EN-US">-python,</span>

<span lang="EN-US" style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span class="MsoHyperlink"><span lang="EN-US">[https://github.com/Ldpe2G/PCANet](https://github.com/Ldpe2G/PCANet)</span></span> <span lang="EN-US">-c++, scala,</span>

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span class="MsoHyperlink">[https://www.rdocumentation.org/packages/pcaMethods/versions/1.64.0/topics/pcaNet](https://www.rdocumentation.org/packages/pcaMethods/versions/1.64.0/topics/pcaNet)</span> -język R,

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Istnieje także implementacja w matlabie (zamknięty kod),

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>i inne.

W artykule [13] struktura sieci opisana jest bardziej szczegółowy. Poniżej umieszczony jest ogólny schemat użytej sieci PCANet.

![](readme_pliki/image254.jpg)

Schemat sieci PCANet, źródło: [18].

            Model SVM używany jest dwukrotnie. Raz dla pierwszego modelu, który klasyfikuje pojedyncze klatki jako opisane wcześniej 3 stany. Drugi raz dla drugiego modelu (Action model), który działa na 30 następujących po sobie klatkach i klasyfikuje ich sekwencje jako 2 stany -upadek lub nie upadek.

W celu ewaluacji proponowanej metody, autorzy użyli publicznie dostępnego zbioru „Multiple Cameras Fall Dataset” ( <span class="MsoHyperlink">[http://www.iro.umontreal.ca/~labimage/Dataset/](http://www.iro.umontreal.ca/~labimage/Dataset/)</span> ) oraz własnych danych. Dostępny publicznie zbiór „Multiple Cameras Fall Dataset” składa się z symulowanych upadków i codziennych czynności. Czynności nagrywane są ośmioma kamerami w rozdzielczości 720x480 pikseli i 30 klatkach na sekundę. Składa się on z 24 różnych upadków z różnych pozycji wyjściowych osoby. Zbiór nagrany przez autorów pracy składał się ze 192 fimów z upadkami nagranych w 30 klatkach na sekundę, z rozdzielczością 352x288 pikseli.  Upadki były z nagrywane z 4 różny pozycji kamer,  w 4 różnych pozycji wyjściowych osoby, 4 różnych kierunkach upadku, 3 krotnie dla każdej takiej kombinacji.

Przeprowadzone zostało 8 eksperymentów na pierwszym zbiorze oraz 4 eksperymenty na zbiorze autorów pracy, każdy eksperyment był dla jednej pozycji kamery. Dla każdego eksperymentu wybierany był film jako testowy, a reszta zbioru była użyta do treningu. Poniższa ilustracja przedstawia 3 typy upadów.

![](readme_pliki/image255.jpg)

3 typy upadków dla zbioru dostępnego publicznie i nagranego przez autorów artykułu, źródło: [13].

Poniżej przedstawiony na przykładzie jest model klasyfikacji sekwencji 30 kadrów. Na początku tło model tła uzyskiwany jest za pomocą algorytmu ViBe, następnie wydzielona jest maska z człowiekiem. Wydzielany jest fragment z człowiekiem i uzyskiwany prostokąt ROI. Jest on normalizowany do obrazka 60x60 pikseli w celu predykcji przez PCANet. Ostatni wiersz przedstawia etykietę, którą dostarczył model.

![](readme_pliki/image256.jpg)

Ilustracja działania pierwszego modelu na przykładzie, źródło: [13].

W celu oszacowania skuteczności metody względem innych metod użyto czułości (Sensivity) i swoistości (Specifity). Wyniki przedstawione są poniżej. Metoda biomechaniczna (Biomechancs approach) wymaga noszenia czujników przez osobę.

![](readme_pliki/image257.jpg)

Porównanie algorytmów detekcji upadku, źródło: [13].

![](readme_pliki/image258.jpg)

Wyniki przedstawianego algorytmu dla różnych kamer, źródło: [13].

Wyniki wydają się być zadowalające. Autorzy spodziewają się lepszych wyników po zastosowaniu większych zbiorów treningowych.

### <a name="_Toc494548800"></a><a name="_Toc494697099"><span lang="EN-US">6.5.8.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Spatial temporal interest points.</span></a>

Znaną powszechnie metodą analizy zdjęć jest szukanie punktów charakterystycznych (interest points). Algorytmy wykorzystywane do tego to np. SIFT, albo SURF (zaimplementowane w OpenCV). Sekwencję zdjęć można analizować wyszukując te punkty charakterystyczne.

Od niedawna wśród rozpoznawania akcji popularnością cieszą się algorytmy polegające na Spatial Temporal Interest Points (STIP, Space Time Interest Points, punkty charakterystyczne w czasoprzestrzeni). SIFT, czy SURF szukają punktów charakterystycznych na obrazie (czyli 2D). Mogą to być na przykład krawędzie, miejsca o dużej zmienności (wariancji). Jako, że przestrzenią punktów charakterystycznych jest obraz (płaszczyzna dwuwymiarowa), punkty maja współrzędne <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image259.png)</span> STIP jest wybierany na całej sekwencji zdjęć. Jeżeli czas w filmie potraktujemy jako dodatkowy wymiar otrzymujemy przestrzeń trójwymiarową ze współrzędnymi punktu <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image260.png)</span>. Punkty STIP to przykładowo punkty, które są mogą mieć dużą prędkość (czyli duża zmienność w czasie i przestrzeni).

![](readme_pliki/image261.jpg)

Film jako przestrzeń trójwymiarowa, źródło: [20].

<span style="font-size:12.0pt;color:#333333;background:white"> </span>

Obecnie w samym Opencv nie ma implementacji STIP, jednak istnieją otwarte implementacje algorytmów szukających STIP ( <span class="MsoHyperlink">[http://www.di.ens.fr/~laptev/download.html](http://www.di.ens.fr/~laptev/download.html)</span> ). Sam STIP można zaimplementować obliczając HOG ( Histogram of Oriented Gradients ) dla każdej klatki, a później tworząc wektor zmian w przedziale czasowym a następnie klasyfikując zmiany przy pomocy klasyfikatora SVM [21]. <span lang="EN-US">Można tez od razu policzyć pochodne w przestrzeni trójwymiarowej.</span>

![](readme_pliki/image262.jpg)

STIP liczone wg algorytmu autorów [22], źródło: [22].

![](readme_pliki/image263.jpg)

Ilustracja STIP wg różnych algorytmów, źródło: [22].

Powyższy rysunek ilustruje STIP wg różnych algorytmów dla Multi-KTH dataset

<span lang="EN-US">(a)<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">I. Laptev, T. Lindeberg, Space-time interest points, in: ICCV, 2003</span>

<span lang="EN-US">(b)<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">P. Doll´ar, V. Rabaud, G. Cottrell, S. Belongie, Behavior recognition via</span>

<span lang="EN-US">(a)<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">sparse spatio-temporal features, in: VS-PETS, 2005</span>

<span lang="EN-US">(c)<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">G. Willems, T. Tuytelaars, L. V. Gool, An efficient dense and scaleinvariant</span>

<span lang="EN-US">spatio-temporal interest point detector, in: ECCV, 2008.</span>

(d)<span style="font:7.0pt &quot;Times New Roman&quot;"></span> [22].

Dzięki wydzieleniu STIP, na ich podstawie można wytrenować model SVM, który będzie działał jako klasyfikator dla zachowań ludzi.

![](readme_pliki/image264.jpg)

Klasyfikator zachowań na podstawie STIP wg pracy [22], źródło: [22].

Autorzy pracy [9] proponują wykorzystanie STIP do klasyfikacji zachowań i tym samym detekcji upadków. Upadki charakteryzują się bardzo dużą prędkością. Powstanie wielu punktów STIP jest zatem charakterystyczne dla nagłych ruchów i upadków. Autorzy opierają swój system na wielu kamerach. Tworzony jest wektor wstrząsu, który jest charakterystyczny dla różnego rodzaju zdarzeń. Po wybraniu STIP na filmach ze zbioru treningowe tworzony jest klasyfikator na podstawie modelu SVM, który może bazować na wektorze wstrząsu (Shock), rozproszeniu punktów STIP (scatter) i ich powierzchni (area). Filmy analizowane są w częściach po 200 klatek (przy filmach w 120 klatkach na sekundę).

![](readme_pliki/image265.jpg)

![](readme_pliki/image266.png)

Powyższe ilustracje pochodzą z [9] i przedstawiają STIP (zielone okręgi) dla różnych wartości progowych algorytmu.

![](readme_pliki/image267.jpg)

Stopień wstrząsu w zależności od czynnosci (a)- upadek, (b)- normalna czynność, źródło: [9].

Następnie autorzy [9] proponują algorytm głosujący, który bazuje na wielu kamerach (jako że ich system bazuje na wielu kamerach).

Autorzy zaimplementowali system na komputerze z Windowsem 7, w C++ z użyciem OpenCV, LIBS. Użyto 104 filmów wideo (13 akcji z 8 kamer) do testów. Patrząc na wyniki należy wziąć pod uwagę że o wyniku decyduje wszystkie 8 kamer.

Poniżej znajdują się wyniki eksperymentów. F-score definiowany jest następująco:

<span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif">![](readme_pliki/image268.png)</span>

![](readme_pliki/image269.jpg)

Figure 1 Definicje miar, źródło: [9].

![](readme_pliki/image270.png)

Czułość i swoistość dla różnych parametrów, dodatkowe oznaczenia punktów: F-score i Accuracy, źródło: [9].

![](readme_pliki/image271.jpg)

Maksymalny F-score dla różnych cech, dla każdego pomiaru największa wartość jest pogrubiona, źródło: [9].

![](readme_pliki/image272.jpg)

Wykres (a), źródło: [9].

![](readme_pliki/image273.jpg)

Wykres (b), źródło: [9].

Powyższe wykres przedstawiają czułość swoistość. Dodatkowe oznaczenia w nawiasach to kolejno maksymalny F-score oraz Accuracy. (a) przedstawia kamery 1,3,5,7\. (b) przedstawia 2,4,6,8.

![](readme_pliki/image274.jpg)

Maksymalny F-score fla różnych cech, (a)-kamery 1,3,5,7; (b)- kamery 2,4,6,8, źródło: [9].

Wyniki są wysokie, ale trudno oszacować jak zachowywałby się algorytm dla pojedynczej kamery. Należy również wziąć pod uwagę że kamery nagrywają w 120 FPS, co być może jest bardzo istotne w wyznaczaniu punktów STIP. Nagrania są dobrej jakości (brak przysłoniętych sylwetek). Mimo tego system jest jednym z najnowocześniejszych i ma wysoką skuteczność.

<span style="font-size:12.0pt;line-height:150%;font-family:&quot;Calibri&quot;,sans-serif;color:#333333;background:white">  
</span>

### <a name="_Toc494548801"></a><a name="_Toc494697100"><span lang="EN-US">6.5.9.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Pictorial Structures for Articulated Pose Estimation</span></a>

<span lang="EN-US"> </span>

Autorzy [27] (wraz z [23], [24], [25], [26]) również proponują rozwiązanie polegające na uczeniu maszynowym. Skupiają się oni na oszacowaniu postawy ludzkiej, a nie na wykryciu upadku, ale ta informacja może okazać się kluczowa przy rozwijaniu systemu detekcji upadku (wraz z innymi rozwiązaniami). Segmentują oni postawę ludzką na obszary, np. autorzy [23] wyszczególniają głowę, tors, dwa przedramiona (lewe i prawe), dwa ramiona, dwa uda i dwa dolne części nóg. Model można wyuczyć za pomocą zdjęć z oznaczonymi częściami ciała. W fazie trenowania uczy się on rozpoznawać poszczególne cześć ciała na obrazie.

![](readme_pliki/image275.png)

Obraz z oznaczonymi częściami ciała (z prawej) i szacowaną sylwetką (z lewej), źródło: [25].

![](readme_pliki/image276.png)

Obraz z oznaczymi częściami ciała (z prawej) i szacowaną sylwetką (z lewej), źródło: [25].

<span style="font-size:12.0pt;color:#333333;background:white"> </span>

![](readme_pliki/image277.png)

Znalezione części ciała i model sylwetki, źródło: [25}.

Wzór estymujący postawę wygląda nasępująco:

![](readme_pliki/image278.jpg)

![](readme_pliki/image279.png)

![](readme_pliki/image280.jpg)

![](readme_pliki/image281.jpg)

Faza trenowania i estymacji wyników wyglądają następująco:

![](readme_pliki/image282.jpg)

źródło: [25].

Segmentację sylwetki na większą ilość części ciała proponują autorzy [25]. Według nich jest to korzystne, ze względu na to, że zmniejsza to wariancję tych części. Na przykład w modelu wariancja torsu jest dosyć spora, gdyż są osoby o różnych sylwetkach (chude, otyłe). Dodatkowo wariancja zwiększa się ze względu na różne obroty (prawy, lewy profil, różny kąt widzenia) i skalę osoby. Segmentacja na bardzo wiele punktów częściowo eliminuje te wady i według autorów [25] poprawia wyniki bez konieczności trenowania na dużym zbiorze treningowym (który mógłbym wyeliminować wariancję wewnątrzklasową.

![](readme_pliki/image283.jpg)

Duża wariancja wewnątrzklasowa wynikająca z obrotów, różnej skali, róznych modeli sylwetek lub wyglądu, źródło: [25].

<span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,sans-serif">  
</span>

Co ważne, istnieje otwarta implementacja w C++ wykrywająca części ciała- [27]. Niektóre części kodu (ucząca) wymaga Octave/Matlab.

### <a name="_Toc494548802"></a><a name="_Toc494697101">6.5.10. <span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wnioskowanie rozmyte bazujące na upadku i czasu nieaktywności osoby.</a>

Autorzy [11] proponują użycie wnioskowania rozmytego do detekcji upadków wymagających alarmu, ponieważ wnioskowanie rozmyte jest dobrze zbadane i użyteczne w temacie rozpoznawania wzorców. Co prawda używają kamer IR, jednak bazują oni na wydzielonej sylwetce człowieka, więc zwykłe kamery powinny być równie użyteczne.

Algorytm najpierw próbuje rozpoznać upadek na podstawie zmiany stosunku wysokość/szerokość prostokąta ROI w czasie, a następnie sprawdza jak długo po upadku osoba znajduje się w bezruchu. Jeżeli jest to odpowiednio długo czas włączany jest system alarmujący.

![](readme_pliki/image284.png)

Ilustracja działania algorytmu, źródło: [11].

Zmienne lingwistyczne takie jak zmiana prędkości, zmiana wysokości, stosunek wysokość/szerokość prostokąta ROI obliczane są wprost dzięki analizie zmian ROI w czasie w wybranej skali.

![](readme_pliki/image285.jpg)

Figure 2 Maksymalne zakresy zmienny lingwistycznych, źródło: [11].

Następnie wybrany jest zakres wartości dla poszczególnych cech odpowiadający zbiorom rozmytym. Poniższy zbiory odpowiadają zmianie prędkości (VelocityChange):

![](readme_pliki/image286.jpg)

Figure 3 Przyporządkowanie do zbiorów rozmytych dla zmiennej zmiany prędkości, źródło: [11].

Później określany jest zbiór reguł dla wnioskowania rozmytego.

<span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,sans-serif">  
</span>

<span style="font-size:12.0pt;color:#333333;background:white"> </span>

![](readme_pliki/image287.jpg)

![Zbiór reguł wnioskowania, źródło: [11].](readme_pliki/image288.png)

<span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,sans-serif">  
</span>

<span style="font-size:12.0pt;color:#333333;background:white"> </span>

Jeżeli zostanie wykryty upadek, zmienna Alarm ma ustawianą wartość FallDeceted (w innym przypadku wartość NoFallDetected). Jeżeli osoba nie rusza się przez dłuższy czas, zmienna Assistance ustawiana jest na AssistanceRequired (w innym przepadku na NoAssistanceRequired).

![](readme_pliki/image289.jpg)

Wyniki działania algorytmu, źródło: [11].

<span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,sans-serif">  
</span>

# <a name="_Toc494697102"><span lang="EN-US">7.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">LBP</span></a>

## <a name="_Toc494697103"><span lang="EN-US">7.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Wstęp</span></a>

Algorytm LBP służy do opisu lokalnych właściwości tekstury.  Przekształca on wejściowy, czarno - biały, obraz w nowy, przyporządkowując każdemu pikselowi i jego otoczeniu pewną liczbę, wyznaczoną za pomocą określonej metody. W zależności od użytej metody wyróżnia się podstawowy LBP oraz warianty Extended LBP (inaczej zwane Circular LBP).

W wersji podstawowej obliczenia dla każdego piksela bazują na jego naturalnym otoczeniu 3x3\. Ogólniejsze algorytmy posługują się parametrami r oraz p, oznaczającymi odpowiednio promień i liczbę punktów. Zamiast sąsiadów danego piksela, wyznaczają one p równomiernie rozłożonych punktów na okręgu o promieniu r o środku w danym pikselu, a następnie interpolują ich kolory. Po wyznaczeniu otoczenia, algorytmy przyporządkowują jego punktom wartości binarne, z których powstaje jedna liczba zapisana dwójkowo. W ten sposób uzyskuje się wartość piksela wynikowego obrazu. Zaletą rozszerzonej wersji jest to, że lepiej koduje cechy niezmiennicze na skale czy obroty.

## <a name="_Toc494697104">7.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Działanie algorytmu</a>

Algorytm liczy lokalne przedstawienie tekstury. Najpierw obraz zostaje przekształcony do skali szarości. Następnie dla każdego piksela zostanie wybrany obszar o promieniu r wokół centralnego piksela. Każdemu pikselowi jest przypisywana liczba dziesiętna po binaryzacji liczby otrzymanej z otoczenia tego piksela. Wartości są przetrzymywane w macierzy. Na ich podstawie tworzony jest histogram. Wartości tak wyliczone są konkatenowane do postaci wektora. Wektory histogramów są porównywane z wektorami obrazów w bazie.

## <a name="_Toc494697105">7.3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> LBP Extended</a>

Dzięki wykorzystaniu LBP Extended możliwe jest zakodowanie większej ilości wzorców, które będą się różnić poziomem wygładzenia i dokładności detali. Im większy promień r, tym bardziej jest wygładzony obraz LBP.

<span lang="EN-US" style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">ror</span>

<span lang="EN-US" style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Uniform</span>

<span lang="EN-US" style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">nri_Uniform</span>

<span lang="EN-US" style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">var</span>

## <a name="_Toc494697106"><span lang="EN-US">7.4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Porównywanie histogramów</span></a>

Podobieństwo (lub niepodobieństwo) histogramów uzyskanych ze zdjęć po zastosowaniu LBP można badać różnymi miarami statystycznymi. <span lang="EN-US">W OpenCV istnieje funkcja:</span>

<span lang="EN-US">![](readme_pliki/image290.png)</span>

spełniająca to zadanie. Uzyskana przez nią metryka jest znajdowana na podstawie statystycznej metody int method, która może być jedną z poniższych:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Odległość χ<sup>2</sup> (HISTCMP_CHISQR w openCV)

![](readme_pliki/image291.jpg)

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Jest to odległość oparta na teście χ<sup>2</sup>, którą oblicza się ze wzoru

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Korelacja (HISTCMP_CORREL w openCV). Jest to zwykła korelacja między histogramami:

![](readme_pliki/image292.jpg)

gdzie _cov_ oznacza kowariancję, a _std_ odchylenie standardowe:

![](readme_pliki/image293.jpg)

a <span style="font-size:11.0pt;line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;
position:relative;top:10.0pt">![](readme_pliki/image294.png)</span> oznacza średnią arytmetyczną.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Odległość Hellingera  (HISTCMP_HELLINGER w openCV) wyraża się ona wzorem:

![](readme_pliki/image295.jpg)

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Alternatywna odległość χ<sup>2</sup> (HISTCMP_CHISQR_ALT w openCV) wyraża się wzorem:

![](readme_pliki/image296.jpg)

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Przekrój (HISTCMP_INTERSECT w openCV) zwraca sumę minimów odpowiadających wartości:

![](readme_pliki/image297.jpg)

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Dywergencja Kullbacka-Leiblera (HISTCMP_KL_DIV w openCV) Mierzy ona względną entropię histogramów H<sub>1</sub>,H<sub>2</sub>. Im jest wyższa, tym większą informacje daje H<sub>1</sub> od H<sub>2</sub>. Z uwagi na to, że nie jest ona symetryczna, nie można jej traktować jako metryki statystycznej. Dywergencję Kullbacka-Leiblera można obliczyć ze wzoru:

![](readme_pliki/image298.jpg)

## <a name="_Toc494697107">7.5.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Jednorodność</a>

Natomiast liczba punktów p jest ważna ze względu na jednorodność. Jednorodność decyduje o opisie rotacji i poziomach wariacji w skali szarości. Funkcja LBP jest uznawana za jednorodną, gdy liczba binarna, która jest przypisywana każdemu pikselowi, ma co najwyżej 2 zmiany 0-1 lub 1-0. Im więcej punktów p jest wybieranych, tym gęstość ostatecznego histogramu wzrasta. Dla każdego punktu p istnieje p + 1 jednorodnych tekstur.

## <a name="_Toc494697108">7.6.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> LBP i rozpoznawanie twarzy</a>

Metodę LBP wykorzystuje się do rozpoznawania twarzy ze względu na możliwość porównywania histogramów struktur dla danego obrazu. Metoda ta wykorzystuje różnice w odległości między histogramami i następnie je porównuje.

![](readme_pliki/image299.png)

Przykładowe zdjęcia z bazy

Poprzednie zdjęcia należą do kolekcji Train, na której **nasz program** uczył się rozpoznawać osobę o indeksie 54.  Nasze doświadczenia zostały przeprowadzone na bazie około **650 zdjęć** przygotowanych dla **55 osób**. Zostały one podzielone w stosunku około 5:1 na zdjęcia Train (służące do nauki rozpoznawania danej osoby), a Test (służące do testowania skuteczności algorytmu).

![](readme_pliki/image300.png)

Zmiana parametrów zdjęcia

Osobę “54” i jej powyższe zdjęcia testowe będziemy śledzić na przestrzeni różnych opcji i metod algorytmu LBP. Pomoże ona nam wizualizować i prześledzić jak różne parametry wpływają na rozpoznawanie twarzy.

## <a name="_Toc494697109">7.7.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Zależność efektywności wykrywania obrazu od kolejnych parametrów</a>

![](readme_pliki/image301.png)

Wyniki testów dla algorytmu LBP

Powyższa tabela przedstawia wyniki pierwszego z dwóch robionych przez nas testów. Można z niej wywnioskować zależność pomiędzy rodzajem algorytmu LBP (Option), metodą porównywania histogramów (Method), szybkością jego działania (Speed) i efektywnością rozpoznawania twarzy (Result).

![](readme_pliki/image302.png)

Średni wynik względem LBP Option

<span style="background:white">Poniżej znajdują przykładowe zdjęcia osoby **_"54"_** w trakcie działania różnych opcji algorytmu LBP. Wszystkie inne parametry zostały zachowane identyczne _(promień 1, metoda porównywania histogramów chisqr_alt, szybkie obliczenia, rozmiar 500 x 500px_).</span>

![](readme_pliki/image303.png)

<span style="background:white"> </span>

<a name="_rrbljtfoka3k"></a><a name="_s70av727x4p3"></a><a name="_fofal6625xtm"></a><a name="_f5mb33c2ih7b"></a><span style="background:
white">Po lewej ilustracja opcji nri_uniform, osiągająca statystycznie najlepsze wyniki w trakcie naszych próbek. Obok opcja domyślna default.</span>

![](readme_pliki/image304.png)

<a name="_g0lhdmnbqc8g"></a><a name="_3cndo14q7d2b"></a><a name="_eiax661woy0x"></a><a name="_q8jc40lq9e9v"></a><span style="background:
white">Opcje ror i uniform. **Uniform dla wybranej osoby _“54”_ jak i dla całej naszej próbki najczęściej ma problemy z rozpoznawaniem twarzy.** W tym wypadku pogląd jest prawie całkowicie bezużyteczny. Dla opcji uniform będzie to ciągłe zjawisko, co zaobserwujemy w późniejszych częściach raportu.</span>

![](readme_pliki/image305.png)

Średni wynik względem metody porównywania histogramów (Method)

Można zaobserwować, że metody kl_div oraz intersect osiągają znacząco gorsze wyniki od pozostałych metod niezależnie od innych parametrów. Pozostałe metody uzyskały średni wynik w przedziale 44% - 52%.

![](readme_pliki/image306.png)

Średni wynik względem prędkości (Speed)

Lepsze wyniki osiągane były przy większej prędkości wykonywania algorytmu.

Biorąc pod uwagę te wnioski warto przyjrzeć się dokładniejszym korelacjom Opcji i Metod. Poniżej cztery wykresy opisują dobrze zależności pomiędzy nimi - można zauważyć, że najkorzystniej wypada połączenie nri_uniform - Hellinger, zostawiając daleko w tyle wyniki pozostałych Opcji i Metod.

![](readme_pliki/image307.png)

![](readme_pliki/image308.png)

![](readme_pliki/image309.png)

![](readme_pliki/image310.png)

![](readme_pliki/image311.png)

![](readme_pliki/image312.png)

## <a name="_Toc494697110"><span style="background:white">7.8.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="background:white">Analiza parametrów dla LBP</span></a>

Wyniki pierwszego eksperymentu pokazały, że wolna prędkość obliczeń dawała znacząco gorsze wyniki, niż opcja szybka. W drugiej serii testów pominęliśmy więc parametr prędkości wykonywania algorytmu - wszystkie działania wykonane były wykonane z opcją szybką. Pozwoliło to też znacząco przyspieszyć czas obliczeń i ograniczyć go do ok. 30 minut.

W drugiej próbie został również dodany nowy parametr, będący rozmiarem obrazka. Zdjęcia twarzy przed wykonaniem algorytmów będą zmniejszane do ustalonych z góry rozmiarów. Pierwsza próba opierała się na stałych rozmiar 255 na 255 pikseli.

Do drugiej dodano opcję 50, 100 oraz 500 pikseli.

Rozpoznawanie większych obrazów zajmuje więcej czasu, ale powinno dać lepszą skuteczność dla wszystkich pozostałych parametrów.

Interesujące okazało się pytanie, czy odwrotne stwierdzenie dla mniejszych obrazów również będzie prawdziwe.

Wyniki względem Opcji i Metody były zbliżone do danych uzyskanych przy poprzednim podejściu, co ilustrują wykresy poniżej. **Opcja Default okazała się więc być najdokładniejsza z ograniczeniem szybkości fast oraz modyfikując rozmiary obrazków.**

![](readme_pliki/image313.png)

![](readme_pliki/image314.png)

![](readme_pliki/image315.png)

Na powyższym wykresie wyraźnie widać, że rozmiar obrazka wpływa na wynik działania algorytmu LBP. Skuteczność rozpoznawania twarzy wzrasta, gdy użyte zostaje zdjęcie o większym rozmiarze. Różnica pomiędzy najgorszym a najlepszym wynikiem wynosi zaledwie 3,435672%, dlatego też ta zmienna nie została uznana za bardzo znaczącą.

![](readme_pliki/image316.png)

Podgląd działania algorytmów w zależności od rozmiaru dla parametrów ror, promień 1, fast

Wniosek: Przy przetwarzaniu znaczącej ilości zdjęć słuszne więc wydaje się skalowanie ich do najmniejszych rozmiarów w celu przyspieszenia obliczeń.

# <a name="_Toc494697111">8.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Porównanie algorytmów LBP i Haar</a>

Oba algorytmy wykrywania twarzy występujące w bibliotece OpenCV działają na podobnej zasadzie, lecz każdy z nich ma swoje mocniejsze i słabsze strony. Cechą łączącą oba algorytmy jest to, że muszą zostać wytrenowane, aby mogły spełniać swoją funkcję. Przebieg pracy obydwu algorytmów przedstawia poniższa ilustracja.

![](readme_pliki/image317.png)

Pomimo kilku podobieństw obydwu algorytmów łatwo zauważyć też ich różnice w budowie, użyteczności czy wykorzystaniu. Zbiór tych różnic przedstawia poniższa tabela.

<table class="MsoNormalTable" border="1" cellspacing="0" cellpadding="0" width="0" style="width:548.25pt;margin-left:-43.75pt;border-collapse:collapse;
 border:none">

<tbody>

<tr>

<td width="89" valign="top" style="width:66.75pt;border:solid black 1.0pt;
  padding:5.0pt 5.0pt 5.0pt 5.0pt">

<span lang="EN-US">Algorytm</span>

</td>

<td width="320" valign="top" style="width:240.0pt;border:solid black 1.0pt;
  border-left:none;padding:5.0pt 5.0pt 5.0pt 5.0pt">

<span lang="EN-US">Zalety</span>

</td>

<td width="322" valign="top" style="width:241.5pt;border:solid black 1.0pt;
  border-left:none;padding:5.0pt 5.0pt 5.0pt 5.0pt">

<span lang="EN-US">Wady</span>

</td>

</tr>

<tr>

<td width="89" valign="top" style="width:66.75pt;border:solid black 1.0pt;
  border-top:none;padding:5.0pt 5.0pt 5.0pt 5.0pt">

<span lang="EN-US" style="font-size:12.0pt;line-height:115%">Haar</span>

</td>

<td width="320" valign="top" style="width:240.0pt;border-top:none;border-left:  none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;
  padding:5.0pt 5.0pt 5.0pt 5.0pt">

<span lang="EN-US">Wysoka dokładność wykrywania</span>

<span lang="EN-US">Niska liczba błędnych odczytów</span>

</td>

<td width="322" valign="top" style="width:241.5pt;border-top:none;border-left:  none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;
  padding:5.0pt 5.0pt 5.0pt 5.0pt">

<span lang="EN-US">Duża złożoność algorytmu i wolna praca</span>

<span lang="EN-US">Dłuższy czas szkolenia algorytmu</span>

<span lang="EN-US">Niska dokładność przy słabym oświetleniu</span>

<span lang="EN-US">Ograniczone działanie przy trudnych warunkach oświetleniowych</span>

<span lang="EN-US">Mniej odporny na okluzje</span>

</td>

</tr>

<tr style="height:90.0pt">

<td width="89" valign="top" style="width:66.75pt;border:solid black 1.0pt;
  border-top:none;padding:5.0pt 5.0pt 5.0pt 5.0pt;height:90.0pt">

<span lang="EN-US" style="font-size:12.0pt;line-height:115%">LBP</span>

</td>

<td width="320" valign="top" style="width:240.0pt;border-top:none;border-left:  none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;
  padding:5.0pt 5.0pt 5.0pt 5.0pt;height:90.0pt">

<span lang="EN-US">Obliczeniowo prosty i szybki w działaniu</span>

<span lang="EN-US">Krótszy czas szkolenia</span>

<span lang="EN-US">Odporny na chwilowe zmiany oświetlenia</span>

<span lang="EN-US">Odporny na okluzje</span>

</td>

<td width="322" valign="top" style="width:241.5pt;border-top:none;border-left:  none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;
  padding:5.0pt 5.0pt 5.0pt 5.0pt;height:90.0pt">

<span lang="EN-US">Mniejsza dokładność</span>

<span lang="EN-US">Wyższa liczba błędnych odczytów</span>

</td>

</tr>

</tbody>

</table>

Każdy klasyfikator wykrywania twarzy OpenCV ma swoje wady i zalety, ale największe różnice to dokładność i szybkość. W przypadku konieczności dokładniejszego wykrycia, klasyfikator Haar wydaje się być lepszym wyborem. Algorytm ten wydaje się być bardziej przydatny w takich technologiach, jak systemy zabezpieczeń czy wysokiej klasy stalking.

Klasyfikator LBP charakteryzuje się dużą szybkością, dlatego powinien być używany w aplikacjach mobilnych lub wbudowanych systemach.

# <a name="_Toc492963340"></a><a name="_Toc494697112">9.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wstępne przetwarzanie obrazu dla algorytmów Eigenfaces oraz Fisherfaces</a>

## <a name="_Toc492963341"></a><a name="_Toc494697113">9.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Obróbka obrazu dla detekcji twarzy</a>

Aby umożliwić efektywne rozpoznawanie twarzy, najpierw konieczna jest ich lokalizacja na obrazie w postaci prostokątnych regionów. W tym kroku nie jest jeszcze istotne, do kogo należy twarz. Od 2001 roku nastąpiło znaczne polepszenie wykrywalności twarzy wraz z nadejściem klasyfikatora kaskadowego „Haar Feature-based Cascade Classifier”. Algorytm ten jest szybki, ponieważ detekcja odbywa się w czasie rzeczywistym z użyciem np. zwykłej kamery internetowej VGA. Cechuje się także sporą niezawodnością, ponieważ sprawdza się w 95% przypadków twarzy zwróconych przodem do kamery. Można się nim posługiwać w bardziej szczegółowej analizie wycinku obrazu, wykorzystując go jako narzędzie do wykrywania części twarzy, np. ust, nosa, oczu. Wykorzystywany jest również w ogólnej detekcji obiektów, takich jak log firm. Jego ulepszeniem jest klasyfikator oparty o LBP (Local Binary Pattern), i jest potencjalnie kilka razy szybszy niż jego poprzednik.

Biblioteka OpenCV umożliwia wczytanie wytrenowanych klasyfikatorów w postaci plików XML, takich jak poniżej:

![Capture](readme_pliki/image318.png)

Lista popularnych plików XML dla detekcji obiektów

Wczytanie danych do klasyfikatora odbywa się w następujący sposób:

![](readme_pliki/image319.png)

Wczytanie wytrenowanego pliku XML

Przed przystąpieniem do wykrywania twarzy, należy przeprowadzić wstępne przetwarzanie obrazu, który będzie się składał głównie z:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>**Konwersji do skali szarości** – zmniejszona liczba kanałów obrazu do jednego znacznie przyspiesza obliczenia.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>**Zmniejszenia rozmiaru** – prędkość wykrywania obiektów zależy w dużej mierze od rozmiaru obrazu wejściowego i mimo tego detekcja będzie wciąż skuteczna nawet w przypadku mniejszych rozdzielczości.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>**Znormalizowanie histogramu** – wykrywanie twarzy jest mniej niezawodne w warunkach ograniczonego światła widzialnego. Aby zwiększyć skuteczność metody, należy dokonać zwiększenia kontrastu i jasności poprzez wyrównanie maksymalnych wartości histogramu klatki.

Biblioteka umożliwia w łatwy sposób konwertować obrazy do skali szarości dzięki użyciu funkcji             <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">cvtColor()</span>. Aby tego dokonać należy sprecyzować format konwertowanego obrazu wejściowego, np. dla trzykanałowego obrazu BGR lub czterokanałowego obrazu BGRA:

![](readme_pliki/image320.png)

Konwersja do skali szarości

W celu zmniejszenia rozmiaru obrazu należy wywołać funkcję <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">resize()</span>. Wykrywanie twarzy sprawdza się dość dobrze dla obrazów większych niż 240x240 pikseli. Przykładowo można zmniejszyć szerokość zdjęcia do 320 pikseli. Podczas próby detekcji minimalne rozmiary interesującego obiektu nie będą na ogół mniejsze niż 20x20 pikseli. Zamiast zmniejszania skali obrazu, można by użyć większych rozmiarów ramki obiektu, choć wpłynęło by to znacząco na prędkość obliczeń. Wyniki znalezionych twarzy również zostaną poddane skalowaniu, dlatego należy pamiętać o utrzymaniu początkowego współczynnika proporcji.

![](readme_pliki/image321.png)

Zmniejszenie obrazu wraz z zachowaniem proporcji

Mając już przeskalowany obraz, można przejść do korekcji histogramu dla twarzy. Operacja ta sprowadza się do wywołania funkcji <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">equalizeHist()</span>.

![](readme_pliki/image322.png)

Zobrazowanie wyrównania histogramu

Może się zdarzyć, że obrazy o naprawdę małym początkowym kontraście będą wydawały się prześwietlone po wyrównaniu histogramu, lecz w większości przypadków, polepszy to jasność i kontrast obrazu oraz pomoże w detekcji niewykrywalnych dotąd twarzy.

![](readme_pliki/image323.png)

Listing 1 Normalizacja histogramu

## <a name="_Toc492963342"></a><a name="_Toc494697114">9.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Rozpoznawanie twarzy</a>

Można teraz przejść do wykorzystania przetworzonego wcześniej zdjęcia i wyszukać na nim twarze. Posłuży do tego metoda <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">CascadeClassifier::detectMultiScale()</span>. Jako argumenty należy podać m. in. obraz wejściowy, dla którego funkcja zwróci wynik w postaci prostokątów wyznaczających obszary występowania twarzy. Kolejnym argumentem jest wartość zwiększanej skali przeszukiwania przy następnych skanowaniach - <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">searchScaleFactor</span>. Dla wysokiej skuteczności wykrywania warto przyjąć wartość <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">1.1</span>. Parametr <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">minNeighbors</span> odpowiedzialny jest za eliminację przypadków wykrycia typu false positive. Zbyt duża wartość może do doprowadzić do utraty wyników true positive a więc tych, które niosą za sobą informację o właściwej pozycji twarzy. Oprócz tego dostępne są także flagi za pomocą których można określić sposób działania detektora, który domyślnie nastawiony jest na wyszukiwanie wielu twarzy. Opcja <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">CASCADE_FIND_BIGGEST_OBJECT</span> ustawia tryb wyszukiwania jedynie największej twarzy. Do flag, które mogą nieznacznie przyspieszyć wyszukiwanie twarzy zaliczają się np.: <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">CASCADE_DO_ROUGH_SEARCH</span> lub <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">CASCADE_SCALE_IMAGE</span>. Istotny jest także wymieniony wcześniej minimalny rozmiar twarzy, który brany jest pod uwagę - <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">minFeatureSize</span>. Dla przeskalowanych obrazów najrozsądniej jest przyjąć, że minimalny rozmiar twarzy wynosić będzie 20x20 lub 30x30 pikseli.

![](readme_pliki/image324.png)

Wywołanie funkcji detekcji twarzy

Jeśli użyte zostało w poprzednim przykładzie skalowanie, nasze wykrywane obszary również ulegną transformacji. Aby przywrócić oryginalne pozycje wierzchołków prostokątów w celu wyświetlenia ich na oryginalnym obrazie, należy dokonać następujących operacji.

![](readme_pliki/image325.png)

Skalowanie wykrytych obszarów twarzy

## <a name="_Toc492963343"></a><a name="_Toc494697115">9.3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Przetwarzanie wykrytej twarzy</a>

Rozpoznawanie twarzy jest niezwykle wrażliwe na zmiany warunków oświetlenia, obrót, czy wyrażane emocje, dlatego tak ważne jest, aby w najwyższym możliwym stopniu zminimalizować różnice pomiędzy obrazami twarzy danej osoby. W przeciwnym przypadku algorytm będzie „widział” więcej podobieństw między twarzami należącymi do dwóch różnych osób w tych samych warunkach np. oświetlenia niż dla twarzy pojedynczej osoby.

Najprostszym sposobem zniwelowania owych różnic jest zastosowanie normalizacji histogramu. Niestety byłoby to odpowiednie jedynie dla projektów, gdzie oświetlenie oraz kąty nie zmieniałyby prawie wcale. Aby uzyskać bardziej niezawodny algorytm, warto posłużyć się bardziej zaawansowanymi technikami wykrywania np. części twarzy, takich jak oczy, brwi, usta, nos. Najbardziej użyteczne okazuje się przetwarzanie, które wykorzystuje pozycje prawego i lewego oka. Przykładowa ekstrakcja cech twarzy wygląda następująco:

## <a name="_Toc492963344"></a><a name="_Toc494697116">9.4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wykrywanie obszaru oczu</a>

Biblioteka dostarcza gotowych klasyfikatorów do wykrywania nie tylko otwartych i zamkniętych oczu, ale też oczu występujących u osób noszących okulary. Należy pamiętać, że jeśli opis klasyfikatora odnosi się np. do lewego oka, na obrazie należy wyszukiwać go po prawej stronie i odpowiednio dla lewego oka. Przykłady klasyfikatorów:

<span lang="EN-US" style="line-height:106%;font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US" style="line-height:106%;font-family:&quot;CourierStd&quot;,serif">haarcascade_mcs_lefteye.xml</span>

<span lang="EN-US" style="line-height:106%;font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US" style="line-height:106%;font-family:&quot;CourierStd&quot;,serif">haarcascade_mcs_righteye.xml</span>

<span lang="EN-US" style="line-height:106%;font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US" style="line-height:106%;font-family:&quot;CourierStd&quot;,serif">haarcascade_lefteye_2splits.xml</span>

<span lang="EN-US" style="line-height:106%;font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US" style="line-height:106%;font-family:&quot;CourierStd&quot;,serif">haarcascade_righteye_2splits.xml</span>

<span style="line-height:106%;font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="line-height:106%;font-family:&quot;CourierStd&quot;,serif">haarcascade_eye.xml</span>

<span lang="EN-US" style="line-height:106%;font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span style="line-height:106%;font-family:&quot;CourierStd&quot;,serif">haarcascade_eye_tree_eyeglasses.xml</span>

Istotnym problemem jest obszar wokół oka, z którym będzie się musiał zmierzyć klasyfikator<span style="line-height:150%">. Na przykład pliki klasyfikatorów</span> <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">haarcascade_eye.xml</span> oraz <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">haarcascade_eye_tree_eyeglasses.xml</span> nadają się lepiej do bardzo wąskimego otoczenia oka, natomiast pliki <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">haarcascade_mcs_lefteye.xml</span> oraz <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">haarcascade_lefteye_2splits.xml</span> pracują lepiej ze zwiększonym obszarem wokół oka. Poniższa tabela zawiera względne położenia dla wykrytych twarzy:

![Capture2](readme_pliki/image326.png)

Tabela 1 Odległości względne obszarów wyszukiwania

![Regions](readme_pliki/image327.png)

Spodziewane obszary oczu dla wybranych klasyfikatorów

Pozyskanie interesujących nas regionów wyszukiwania wygląda następująco:

![](readme_pliki/image328.png)

Uzyskanie wycinków obrazu do detekcji oczu

Wydajność detekcji nie idzie w parze z niezawodnością. Dla obrazów w rozdzielczości 320x240 z otwartymi lub zamkniętymi oczami oraz z okularami dla dedykowanego klasyfikatora, parametry prezentują się następująco:

![Table](readme_pliki/image329.png)

Wydajność klasyfikatorów dla danych zbiorów

Warto zwrócić też uwagę na to, że podczas gdy rekomendowane jest zmniejszenie obrazu przed wstępnym wykrywaniem twarzy, nie musi być do dobra praktyka w wyszukiwaniu oczu. W tym przypadku wymagana jest największa możliwa rozdzielczość, a więc warto odnieść się do oryginalnego rozmiaru obrazu. Opierając się na tabeli w przypadku, gdy bardzo wysoka wydajność nie jest kluczowa, najlepiej jest użyć dla detektora najpierw pliku <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">mcs_*eye</span>, a później <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">eye_2splits</span>.

Do wykrycia lewego lub prawego oka służy ta sama metoda wykorzystana już wcześniej przy detekcji twarzy, ale tym razem wykorzystany zostanie obszar w oryginalnej rozdzielczości w celu polepszenia wykrywalności. Łatwo w ten sposób wyszukać początkowo np. lewe oko. Jeśli nie zostanie wykryte, wtedy należy spróbować posłużyć się innym detektorem.

![](readme_pliki/image330.png)

Wyszukiwanie obszaru oczu

## <a name="_Toc492963345"></a><a name="_Toc494697117">9.5.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Kombinacja transformacji</a>

Posiadając już wykryte obszary twarzy i oczu, można przystąpić do dalszych operacji, tj.:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Transformacja geometryczna i przycinanie – w tym procesie zawierają się skalowanie, obrót i przesunięcie obrazu w taki sposób, aby w obszarze po usunięciu czoła, policzków, uszu i tła znalazły się oczy, nos oraz usta.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Osobne wyrównanie histogramów dla prawej i lewej strony – zmniejszony wpływ na kąt oświetlenia.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Wygładzanie – redukowanie szumów na zdjęciu dzięki filtrowi bilateralnemu.

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Maska eliptyczna – pomaga w usunięciu pozostałych włosów i tła z obrazu.

![](readme_pliki/image331.png)

Kroki przetwarzania

Znaleziony obszar twarzy może okazać się zbyt słabo ustandaryzowany. Aby uniknąć nachodzenia na siebie różnych części twarzy, podczas rozpoznawania, tj. aby część nosa nie nachodziła na część oka innej zapamiętanej twarzy, warto wykorzystać pozycję oczu do wyrównania pozycji twarzy. Współrzędne posłużą do czynności takich jak:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Rotacja – układ oczu będzie idealnie horyzontalny

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Skalowanie – odległość pomiędzy oczami będzie zawsze taka sama

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Translacja (przesunięcie) – oczy są umieszone w centralnej pozycji i na pożądanej wysokości

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Przycięcie – pozbycie się zewnętrznych części twarzy, tj. włosów, czoła, uszu, podbródka oraz tła.

Obliczone zostaną współrzędne środka oczu, kąt i dystans między nimi.

![](readme_pliki/image332.png)

Obliczenia środka, kąta i dystansu oczu

Do odpowiedniego rozciągnięcia zdjęcia wykorzystana zostanie transformacja afiniczna. Najpierw jednak konieczne jest utworzenie macierzy przekształcenia na podstawie obliczonych wartości. Służy do tego metoda <span style="line-height:150%;font-family:&quot;CourierStd&quot;,serif">getRotationMatrix2D.</span> Następnym krokiem jest przekształcenie afiniczne w celu umiejscowienia oczu w jednakowej pozycji dla każdej twarzy.

![](readme_pliki/image333.png)

Przekształcenie afiniczne w wykorzystaniem macierzy transformacji

## <a name="_Toc492963346"></a><a name="_Toc494697118">9.6.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Oddzielne wyrównywanie histogramów dla prawej oraz lewej strony</a>

W realnych warunkach bardzo często zdarza się, że oświetlenie podkreśla tylko jedną część twarzy, drugą pozostawiając zacienioną. Brak równomiernej jasności ma negatywny wpływ na jakoś rozpoznawania, dlatego ważne jest jak najlepsze zrównanie jasności obydwu stron. Gdyby po prostu zastosować wyrównywanie histogramu najpierw dla lewej części i odpowiednio dla prawej, dałoby to w rezultacie obraz o wyraźnej różnicy kontrastów pomiędzy granicami przetwarzanych obszarów. Dlatego w celu zniwelowania owego rozdzielenia, zastosowane zostanie stopniowe przejście od lewej wartości wyrównania, przez środkowy obszar, którego wartość wyrównywania będzie uśredniona dla całego obrazu, aż po prawy obszar z łagodnym przejściem.

![Untitled3](readme_pliki/image334.png)

Połączenie rozjaśnionych obszarów

Teraz trzeba utworzyć podobrazy-kopie i zastosować zwiększanie kontrastu dla każdego z nich.

![](readme_pliki/image335.png)

Normalizacja dla całej twarzy oraz obu stron

Tak powstałe obrazy należy połączyć w jeden z łagodnym przejściem. Z racji tego, że obrazy nie mają zbyt dużych rozmiarów, można łączyć obrazy piksel po pikselu w następujący sposób:

![](readme_pliki/image336.png)

Łączenie znormalizowanych obrazów

Tak powstały obraz, powinien znacząco zredukować efekty uboczne związane z nierównomiernym oświetleniem twarzy oraz powinien pomóc w precyzyjności rozpoznawania. Warto pamiętać, że ten sposób nie usunie zupełnie wszystkich efektów zacienienia, ponieważ ludzka twarz ma bardzo skomplikowaną strukturę przestrzenną.

## <a name="_Toc492963347"></a><a name="_Toc494697119">9.7.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wygładzanie</a>

Aby zredukować występujący na klatce szum, posłużyć się można np. filtrem bilateralnym. Jest to filtr nieliniowy, wygładzający obraz z zachowaniem krawędzi. Oparty jest o wagi liczone w zależności od odległości od piksela oraz od jasności pomiędzy pikselami otaczającymi. Jasność danego piksela jest zastępowana sumą ważoną jasności tego piksela.

![Capture4](readme_pliki/image337.png)

Filtr bilateralny

Wyrównanie histogramu może znacząco podnieść poziom szumu. Parametry są dobrane w ten sposób, aby zająć się jedynie szumem na poziomie kilku pikseli, nie tracąc właściwości twarzy.

![](readme_pliki/image338.png)

Wykorzystanie filtru bilateralnego

## <a name="_Toc492963348"></a><a name="_Toc494697120">9.8.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Maska eliptyczna</a>

Pomimo tego, że mamy już usuniętą większość z zbędnych elementów twarzy, wciąż pozostaje pozbyć się elementów w rogach obrazu, takich jak np. część szyi, szczególnie wtedy, gdy twarze nie są zwrócone przodem do kamery.

![Untitled43](readme_pliki/image339.png)

Końcowe przycięcie obrazu

Środkiem elipsy będzie punkt o położeniu względnym 0.5 dla osi X oraz 0.4 dla osi Y. Stosunek osi wielkie do osi małe wynosi 10:16.

![](readme_pliki/image340.png)

Utworzenie maski eliptycznej

Tak przetworzone twarz mogą posłużyć jako wejście w fazie rozpoznawania zarówno w procesie zbierania treningowych danych, jak i w przypadku prób rozpoznania wykrytych twarzy z klatki kamery.

![Eliptical](readme_pliki/image341.png)

Końcowy efekt przetwarzania obrazu twarzy

# <a name="_Toc494697121">10.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wykorzystanie sieci neuronowych w rozpoznawaniu twarzy</a>

## <a name="_Toc494697122">10.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wstęp</a>

Rozpoznawanie obrazów jest zadaniem, w którym sieć neuronowa (lub inny automatyczny system rozpoznający) ma podejmować decyzje na temat przynależności określonych obiektów do ustalonych klas. Obiekty mogą być różnych rodzajów – mogą to być obrazy cyfrowe wprowadzone wprost z cyfrowych aparatów lub kamer cyfrowych, albo obrazy analogowe, przetworzone przez skaner lub „frame grabber”.

![](readme_pliki/image342.png)

 Proces przygotowania danych do uczenia sieci neuronowej

Sieć neuronowa rozpoznająca obrazy ma zwykle kilka wejść, na które podawane są sygnały odpowiadające wyróżnionym cechom rozpoznawanych obiektów. Na ogół takich wejść bywa dużo, bo trzeba dokładnie „pokazać” sieci wszystkie cechy rozpoznawanego obiektu, żeby mogła się prawidłowo nauczyć go rozpoznawać. Jednak liczba cech, charakteryzujących obraz, jest znacząco mniejsza niż liczba elementów (pikseli) samego obrazu. Jeśli na wejście sieci rozpoznającej podałoby się bezpośrednio obraz cyfrowy, wówczas liczba wejść sieci (równa liczbie pikseli na takim obrazie) mogłaby sięgać setek tysięcy lub nawet wielu milionów! Dlatego praktycznie nigdy nie stosuje się takich sieci neuronowych, które pobierałyby wprost na swoje wejście analizowany obraz jako taki, tylko zwykle wprowadza się tam właśnie wspomniane cechy obrazu, wydobyte i ustalone za pomocą programów analizujących rozpoznawane obrazy poza siecią neuronową, co czasem bywa określane jako preprocessing tych obrazów.

Sieć neuronowa rozpoznająca ma też zwykle wiele wyjść. Na ogół są one wykorzystywane w ten sposób, że do każdego wyjścia przypisuje się określone rozpoznanie. Na przykład w systemie automatycznego rozpoznawania znaków alfanumerycznych (tzw. zadania OCR) korzysta się z ponad 60 wyjść, z których każde przypisane jest do innego znaku – na przykład pierwszy neuron powinien sygnalizować pojawienie się litery A, drugi – B itd.

![](readme_pliki/image343.jpg)

Schematyczna budowa wielowarstwowej sieci neuronowej

Pomiędzy wejściem i wyjściem jest też zwykle przynajmniej jedna warstwa ukryta. Najogólniej mówiąc, warstwa ta może mieć więcej albo mniej neuronów – i potoczna interpretacja zjawisk zachodzących w sieciach neuronowych nakazuje nam uważać, że sieć mająca tam więcej neuronów jest bardziej „inteligentna”. Jednak wcale nie jest dobrze dążyć do posiadania sieci o możliwie największej „wrodzonej inteligencji”, gdyż sieć taka bywa czasem zaskakująco nieefektywna.

## <a name="_Toc494697123">10.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Typy sieci neuronowych w rozpoznawaniu twarzy</a>

Najczęściej do rozwiązywania tego typu problemów używa się sieci MLP (Multi-Layer Perceptron) oraz rekurencyjnych sieci neuronowych Hopfielda, w której wszystkie neurony są ze sobą nawzajem powiązane na zasadzie sprzężeń zwrotnych. W pierwszym przypadku sieć jest zwykle dwuwarstwowa i składa się z neuronów sigmoidalnych. W procesie uczenia wykorzystuje się propagację wsteczną, czyli podstawowy algorytm uczenia nadzorowanego wielowarstwowych, jednokierunkowych sieci neuronowych. Podaje on przepis na zmianę wag dowolnych połączeń elementów przetwarzających rozmieszczonych w sąsiednich warstwach sieci. Charakteryzuje się jednak dość wolną zbieżnością. Drugi typ – sieć rekurencyjna –cechuje się prostym sposobem uczenia i łatwością implementacji w postaci układu fizycznego. Składa się z jednej warstwy zawierającej M jednakowych neuronów najczęściej opisanych charakterystykami skokowymi. Ogólnie funkcja aktywacji neuronów występujących w sieci Hopfielda może być również opisana funkcją ciągłą, np. z wykorzystaniem funkcji tangens hiperboliczny. Sieć Hopfielda może spełniać funkcję pamięci skojarzeniowej. Wartości wag są dobierane w ten sposób, aby sieć mogła zapamiętać kilka wektorów które traktuje się jako wzorcowe. Jeżeli na wejście sieci poda się jakiś wektor wzorcowy lub zaburzony wektor wzorcowy, to na wyjściu sieci powinien ustalić się ten sam nie zaburzony wektor wzorcowy. W inicjującym kroku działania sieci na wejście sieci podaje się wektor wejściowy, np. obraz w postaci mapy bitowej o wartościach +1, -1\. Stan na wyjściu sieci w kolejnym kroku uzyskujemy w wyniku działania algorytmu iteracyjnego. Sieć kończy działanie, kiedy w dwóch kolejnych krokach wyjście sieci nie zmienia się. Wadą sieci Hopfielda jest jej mała pojemność, wyrażająca się małą liczbą klas, jakie sieć może zapamiętać.

## <a name="_Toc494697124">10.3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Zastosowanie</a>

Sieci neuronowe znajdują zastosowanie m. in. w detekcji twarzy, rozpoznawaniu, analizie wyrazu oraz w kategoryzacji na podstawie rysów twarzy. Pozwalają także przede wszystkim:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>zdyskryminować wszystkie cechy dla każdego wzorca i każdej cesze nadać odpowiednia wartość

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>wybrać cechy najbardziej istotne dla danych wzorców wejściowych

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>automatycznie zbudować topologie sieci

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>wyliczyć wagi, funkcje aktywacji na każdym neuronie

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>zawsze rozpoznać wszystkie wzorce uczące

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>umieć rozpoznawać zaburzone wzorce uczące

![](readme_pliki/image344.png)

Przykładowa twarz i jej wektor wag utworzona

przy wykorzystaniu 20 twarzy własnych

# <a name="_Toc494697125">11.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Łączenie klasyfikatorów detekcji twarzy w celu polepszenia wyników</a>

## <a name="_Toc494697126">11.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Framework</a>

Detekcja twarzy jest do dzisiaj problemem otwartym. Potrafi być zawodna w systemach stosowanych na dużą skalę, podczas częściowego zasłonięcia twarzy lub w przypadku zmiany pozycji, rotacji. Eksperymenty z łączeniem klasyfikatorów wykazały zwiększenie pozytywnych przypadków wyszukiwania, przy jednoczesnym zmniejszeniu współczynnika wykrycia przypadków typu _false-negative_. Metoda scalania uwzględnia dodatkowo przydatne informacje o oszacowanej pozycji i wyrównania twarzy.  

Najbardziej przykuwającym uwagę detektorem wśród społeczności zajmującej się detekcją twarzy jest _Viola-Jones object detection framework*_. Bazuje na analizie cech tzw. obrazu integralnego, z wykorzystaniem falek Haara. Proces klasyfikacji zrealizowano z użyciem algorytmu AdaBoost (kaskada klasyfikatorów). 

![Frameworks](readme_pliki/image345.png)

Porównanie frameworków

Wysoką skuteczność uzyskać można między innymi dzięki wykorzystaniu jak największego zbioru o zróżnicowanych cechach położenia, oświetlenia, itp. przy jednoczesnym określeniu zbioru obrazów niezwiązanych z poszukiwaniami.

![Viola-Jones](readme_pliki/image346.jpg)

Trenowanie klasyfikatora kaskadowego

Dzięki bibliotece OpenCV, narzędzia trenujące są dostępne dla szerokiego grona badaczy. Klasyfikatory są zazwyczaj skonstruowane dla frontalnych pozycji głowy, więc ich niezawodność zależy głównie od kąta względem kamery w płaszczyźnie, jak i poza nią. W tej części wykorzystane połączone zostaną globalne oraz lokalne detektory, aby uzyskać znaczącą poprawę całościowego działania.

## <a name="_Toc494697127">11.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Rezultaty eksperymentalne</a>

Wśród wielu klasyfikatorów twarzy i głowy zawartych w bibliotece OpenCV wybrano FA2, oznaczony jako _haarcascade_frontalface_alt2_, który wykorzystywany jest często jako główny punkt odniesienia w porównywaniu wyników. Poniższa tabela przedstawia parametry dla dwóch zbiorów danych - _Yale Face Database_ oraz _The CMU Multi-PIE Face Database_.

![Comparison](readme_pliki/image347.png)

Porównanie klasyfikatorów

Do eksperymentu wykorzystano heurystyki w połączeniu klasyfikatorów dla oka, ust, nosa, czy ucha. Porównano różne strategie:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>**F**: wykorzystanie jedynie klasyfikatora FA2

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>**FC**: wykorzystanie klasyfikatora FA2 oraz wyszukiwanie części twarzy w spodziewanych rejonach. Nieudana próba znalezienia przynajmniej czterech elementów skutkuje zaklasyfikowaniem zdjęcia jako pomyłka. W rezultacie tak pozytywne, jak i negatywne współczynniki klasyfikacji ulegają polepszeniu.

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Prawe i lewe oko: lewy górny róg obszarów (0, 0) and (sx × 0.4, 0) i ich wymiary: (sx × 0.6, sy <span style="font-family:&quot;Cambria Math&quot;,serif">∗</span> 0.6).

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Nos: lewy górny róg obszaru (sx × 0.2; sy × 0.25) i ich wymiary: (sx × 0.6, sy × 0.6).

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Usta: lewy górny róg ROI: (sx × 0.1; sy × 0.4) i ich wymiary: (sx × 0.8, sy × 0.6).

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Lewe i prawe ucho: lewe górne rogi wynoszą odpowiednio:(−sx/3; sy × 0.2) and (sx/2; sy × 0.2), i ich wymiary: (sx/3 + sx/2, sy × 0.6).

![Face_ROIs_areas](readme_pliki/image348.png)

Face ROIs areas

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>FC2: metoda podobna do FC, lecz cechująca się dodatkowym przeskalowaniem obrazu dla części oka

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>**FFs**: brak detektora całej twarzy, zamiast tego analiza na postawie jednocześnie występujących no najmniej trzech spójnie i logicznie występujących elementów. Główne zasady:

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>usta muszą być poniżej innych części twarzy (ale względnie blisko)

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>nos musi występować poniżej obu oczu, ale powyżej ust

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>centrum lewego oka musi być wysunięte najbardziej na lewo od pozostałych elementów i powyżej nosa i ust

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>centrum prawego oka odpowiednio musi być występować na najdalej wysuniętym na prawo obszarze, także powyżej nosa i ust

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>uszy muszą być o obu stronach

<span style="font-family:&quot;Courier New&quot;">o<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>proporcje odległości pomiędzy twarzami muszą być spójne

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>**FFFs**: łączy metodą F i FFs, jest mniej wrażliwa na zasłonięte części twarzy lub na obrót od FFCFFs**: łączy FC i FFFs w celu redukcji współczynnika false-positive

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>**FC2FFs**: łączy FC2 i FFFs w celu zarówno redukcji współczynnika *false positive*, jak i wzrostu pozytywnego współcznynnika klasyfikacji

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>**XXR**: zastosowane wszystkie poprzednie podejścia nie tylko dla obrazu wejściowego, ale też lekko obróconych o ok. 15 stopni w prawo i w lewo aby mieć do czynienia w większą różnorodnością pozycji.

Wyniki dla True Positive Rate (TPR) i False Positive Rate (FPR) umieszczone są w poniższej tabeli:

![First_table](readme_pliki/image349.png)

Wyniki dla oryginalnego obrazu wejściowego

![First_table](readme_pliki/image350.png)

Wyniki dla obróconego obrazu

## <a name="_Toc494697128">11.3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Spostrzeżenia</a>

Głównym punktem odniesienia jest pierwszy test na poziomie 71% poprawnej detekcji oraz mylącym się w ok. 5% przypadków. Warto zauważyć, że podejście FC2 zachowuje się praktycznie tak samo dla pozytywnych wyników i jest aż dziesięciokrotnie skuteczniejszy przy pozbywaniu się błędów. Podobnie błędnych klasyfikacji można uniknąć stopując metodę FC oraz FFs (opartą jedynie na deskryptorach wyszukujących części twarzy), chociaż radzi sobie jedynie z połową poprawnych obrazów twarzy. Najlepsze oba współczynniki można zaobserwować w metodzie FC2FFs oraz FFFs.

![Results_1](readme_pliki/image351.png)

Rezultat polepszenia wskaźnika True Positive

![Results_2](readme_pliki/image352.png)

Rezultat polepszenia wskaźnika True Negative

Zaangażowanie klasyfikatorów wykrywających wiele obszarów składających się na całą twarz przyczynia się do zwiększenia obliczeń, choć w dobie komputerów wielordzeniowych, wydaje się to mniej znaczące.

Aby zilustrować lepiej korzyści, jakie płyną z owych obliczeń, dla każdego z podejścia łączonego wykonano osobne testy. Osobne wyniki uzyskano dla podejścia niezawierającego obróconych obrazów oraz zawierającego obrócone obrazy. 

![ROC_1](readme_pliki/image353.png)

Krzywe operacyjne odbiornika ROC dla nieobróconych zdjęć

![ROC_1](readme_pliki/image354.png)

Krzywe operacyjne odbiornika ROC dla obróconych zdjęć

Prezentują poprawę, choć koszt obliczeń wzrasta trzykrotnie. Dodatkowymi zaletami jest informacja o pozycji głowy, kącie obrotu. Tym samym, znajdując jedno ucho na obrazie, wysoce prawdopodobne jest, że twarz jest częściowo lub całkowicie odwrócona. Kolejnym krokiem weryfikującym może być próba odnalezienia oka w spodziewanym obszarze.

![Kierunek](readme_pliki/image355.png)

Oszacowanie obrotu głowy

Rezultaty wykazują, że połączenie klasyfikatorów skutkuje zwiększoną skutecznością wykrywania twarzy, jak i nie-twarzy w porównaniu z obecnym stanem wiedzy. Zyskuje odporność na niewielkie kąty obrotu dodaje wiedzy do dalszego analizy klasyfikowanej twarzy.

<span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,sans-serif">  
</span>

# <a name="_Toc494697129">12.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wpływ niskiej rozdzielczości na jakość detekcji i rozpoznawania twarzy</a>

W tej części przeanalizowana zostanie niezawodność systemów wizyjnych czasu rzeczywistego, które wykrywają oraz rozpoznają twarze z obrazów niskiej rozdzielczości. Obraz ten pochodzi z kamer przemysłowych CCTV. Podczas badań wzięta zostanie pod uwagę zarówno poprawność ekstrakcji (wykrywania) obiektów, jaki i ich właściwa klasyfikacja (rozpoznawanie). Do testów wykorzystano zbiór danych umożliwiający analizę tolerancji na zmienne warunki oświetlenia i pozycji głowy.

W dzisiejszych czasach coraz więcej systemów automatycznego dostępu oparte są na analizie cech biometrycznych. Systemy rozpoznawania twarzy cieszą się dużą popularnością za względu na coraz większą niezawodność. Głównym problemem jednak wciąż jest niezadowalająca jakość szczegółów pochodzących z kamer, zwłaszcza z większej odległości. Identyfikacja obiektów ze znacznej odległości w tych warunkach pozostaje wciąż ważnym naukowym problemem do rozwiązania. W tego typu przypadkach można zauważyć oddzielne etapy dla detekcji i rozpoznawania.

Testy odbyły się w środowisku Matlab z przygotowanym środowiskiem graficznym ułatwiającym badanie efektywności algorytmów dla różnych parametrów. Rozpoznawanie oparte jest na podejści Eigenfaces.

## <a name="_Toc494697130">12.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Biometryczne standardy rozpoznawania twarzy odnoszące się do CCTV</a>

Istnieje kilka norm opisu głównych zasad, kierunków i cech dotyczących wejściowych danych biometrycznych, m. in. obrazów twarzy. Należą do nich między innymi: _<span style="font-size:9.5pt;line-height:150%;font-family:&quot;YxyxvfTimesTen-Roman&quot;,serif;color:#131413">ISO/IEC 19794 5</span>_<span style="font-size:9.5pt;line-height:150%;font-family:&quot;YxyxvfTimesTen-Roman&quot;,serif;color:#131413">, _ANSI/INCITS 385 2004_ oraz europejska norma</span> _<span style="font-size:9.5pt;line-height:150%;font-family:&quot;CdfxwfTimesTen-Italic&quot;,serif;color:#131413">EN 50132-7: CCTV</span>_<span style="font-size:9.5pt;line-height:150%;font-family:&quot;CdfxwfTimesTen-Italic&quot;,serif;color:#131413">.</span>

![](readme_pliki/image356.png)

Rekomendowany minimalny rozmiarobserwowanych obiektów według europejskiego standardu EN 50132-7: „CCTV and alarm systems”

Wspomniane normy można odnieść do zbiorów danych przygotowanych dla instytucji naukowych. Poniższa tabela przedstawia zestawienie wyników w porównaniu do owych norm.

</div>

<span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,sans-serif">  
</span>

<div class="WordSection2">

![](readme_pliki/image357.jpg)

Porównanie norm biometrycznych

</div>

_<span style="font-size:11.0pt;font-family:&quot;Georgia&quot;,serif;color:#44546A">  
</span>_

<div class="WordSection3">

Baza danych Uniwersytetu Sheffield zawiera 564 obrazy 20 osób. Każda z nich jest ujęta innej pozycji – od pozycji bocznej do frontalnej. Pliki zapisane są w formacie PGM (Portable Greymap) w różnych rozmiarach i w 256-bitowej skali szarości. Wadą tej bazy danych jest niewątpliwie brak rozdzielenia frontalnych ujęć twarzy od reszty pozycji. Ze wszystkich zdjęć, do eksperymentu wykorzystano jedynie twarze odwrócone przodem do kamery.

Jedną z najlepiej nadających się do badań pod względem łatwości zastosowania, przetwarzania i sortowania plików według cech jest baza danych _Yale Face Database_. Zawiera ona 5760 obrazów 10 ludzi. Każdej fotografowanej osobie przypada 576 zdjęć w 9 pozycjach i innych warunkach oświetleniowych. Każdy plik w tej bazie może być łatwo odseparowany z powodu jasnego nazewnictwa zdjęć frontalnych i innych. Obrazy dobrej jakości występują w rozdzielczości 640x480, ale wciąż nie spełniają wspomnianych wcześniej wymagań.

![](readme_pliki/image358.png)

<a name="_Ref493225981">Przykład detekcji twarzy dla obrazu o niskiej rozdzielczości ze zbioru Yale database</a>

Następną przetestowaną bazą jest „MUCT Face Database” pochodzącą z Uniwersytetu Kapsztadzkiego. Składa się z 3755 obrazów dla podzielonych na 276 klas obiektów. Każda twarz była fotografowana przy użyciu pięciu różnych kamer w tym samym momencie. Dzięki temu uzyskano pięć różnych pozycji. Dodatkowo, każda jednostka była fotografowana w czterech różnych ustawieniach oświetlenia.

Baza danych „The Color FERET Database” z Uniwersytetu Georga Masona była tworzona między rokiem 1993 i 1996 i zawiera 2413 obrazów twarzy, reprezentujących 856 osób.

Podsumowując, tabela pokazuje, że żadna z powyższych baz danych nie przestrzega całości wymaganych wytycznych biometrycznych. Główną przyczyną problemów związanych ze zgodnością standardów są: niewłaściwe proporcje wymiarów obrazu i zbyt duże odległości od fotografowanych osób w stosunku do obiektywu (szczególnie w przypadku starszych baz danych). Należy zwrócić uwagę, że niepodporządkowanie się zbiorów do rekomendowanych biometrycznych norm jest w pewnym sensie zaletą, ponieważ pozwalają analizować efektywność identyfikacji w przypadkach niewspółdziałania z testowanymi zdjęciami jednostek. Dokładnie takie sytuacje występują, kiedy analiza twarzy jest oparta na nagraniach z kamer nadzorujących.

![EN norm](readme_pliki/image359.png)

Porównanie minimalnych rozmiarów dla normy EN 50132-7 do wyników eksperymentalnych

## <a name="_Toc494697131">12.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wpływ na detekcję twarzy</a>

We wcześniejszym fragmencie tej pracy detekcja twarzy została dokładanie opisana, dlatego w tej części poruszone zostaną jedynie istotne kwestie dotyczące zależności między jakością obrazu a precyzją metody.

Istnieje wiele podejść do metod detekcji. Pierwsza z nich polega na lokalizacji twarzy na podstawie koloru skóry, który jest zależny od natężenia światła (luminancji), ale ma ten sam odcień. Dzięki temu, inne elementy występujące w obrazie, które nie odpowiadają skórze, mogą być efektywnie usunięte. Następnie, wykorzystując morfologiczne operacje na wybranym obszarze (ROI), inne cechy mogą być oddzielone. Przykłady wykrywania twarzy tą metodą w różnych warunkach widoczne są na poniższym rysunku Algorytm ten na ogół działa poprawnie, ale posiada istotne wady. Jako że ta metoda stara się wyryć skórę, może się zdarzyć, że uwzględni także część szyi lub włosy koloru blond. Metoda ta nie sprawdza się jednak w przypadku słabego lub silnego bocznego oświetlenia.

![](readme_pliki/image360.png)

Przykłady wykorzystania metod w różnych warunkach

Kolejną techniką lokalizacji twarzy jest użycie metod antropometrycznych oraz tworzeni modeli geometrycznych części twarzy. Zaletą tej metody jest szansa działania na statycznym obrazie w skali szarości. Wykorzystuje wiedzę o geometrii standardowej twarzy człowieka oraz zależności pomiędzy nimi – pozycję, dystans, itp. Korzysta się w tym przypadku z metryki Hausdorffa.- porównywana jest odległość pomiędzy zwartymi podzbiorami przestrzeni metrycznej zupełnej. Niestety, nie jest odporna na rotację w pionie i poziomie większą niż 45° oraz na odchylenie większe niż 25°.

Trzecią metodą jest wykorzystanie znanego już klasyfikatora _Haar-like_. Program wykorzystuje cztery klasyfikatory części twarzy (która jest w odwrócona frontem do kamery). Początkowo wyszukiwana jest para oczu, później każde oko osobno. Dzięki metodzie wyrównywania histogramu, klasyfikator dobrze radzi sobie ze zmiennym oświetleniem, włączając w to oświetlenie boczne. Ta metoda okazała się najbardziej odporna wpływ czynników zewnętrznych i została wykorzystana w dalszych testach do rozpoznawania twarzy.

![Wydajność metod](readme_pliki/image361.png)

Wpływ rozdzielczości oraz różnych warunków oświetleniowych na proces detekcji twarzy dla klasyfikatora Haar-like

## <a name="_Toc494697132">12.3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Doświadczalny wpływ rozdzielczości na proces detekcji twarzy</a>

Podczas badania wpływu rozdzielczości na proces detekcji, przebadano 5760 obrazów twarzy z bazy _Yale database_ w pięciu konfiguracjach – zaczynając od rozmiarów 640 x 480, a następnie redukując wymiary dwukrotnie, czterokrotnie, ośmiokrotnie oraz dwunastokrotnie. Tak jak było wspomniane wcześniej, baza danych _Yale_ zawiera zdjęcia dziesięciu osób. Każda została sfotografowana w dziewięciu różnych pozycjach, z których z kolei każda wykonana była w sześćdziesięciu czterech różnych warunkach oświetleniowych. Stąd łącznie zbadano ok 28000 zdjęć z użyciem klasyfikatora _Haar-like_.

Powyższy rysunek pokazuje wpływ rozdzielczości obrazu w procesie detekcji dla rozmaitych rozmieszczeń twarzy. Łatwo zauważyć, że nawet czterokrotne zmniejszenie rozdzielczości praktycznie nie wpływa na jakość procesu. Ośmiokrotna redukcja wymiarów prowadzi do zmniejszenia efektywności o 10% dla osoby w pozycji P00 i 20% w przypadku osoby w pozycji P08 (głowa skierowana w dół pod kątem ok. 45° oraz w lewo pod kątem ok 30°. Można dojść do wniosku, że wybrana metoda najlepiej sprawdza się dla pozycji P00 -P03\.

Można zaobserwować, że twarze mogą być wykrywane nawet z obrazów o rozdzielczości nie większej niż ok. 50x40 pikseli, gdzie obszar twarzy był rozmiarów jedynie 17 x 21 pikseli. Pozostałe testowane w bazie twarze były podobnych rozmiarów. Ten przykład dowodzi, że średni limit rozdzielczości dla całego obrazu wynosi 54 x 20 pikseli, natomiast zadowalający obszar twarzy wynosi średnio 20 – 30 pikseli szerokości i wysokości).

## 12.4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span>  <a name="_Toc494697133">Badanie wpływu źródła światła na detekcję twarzy</a>

Wykresy pokazują wpływ padania kąta światła w zależności od rozdzielczości. Podobnie, jak w przypadku obracania twarzy, niskie wartości rozdzielczości nie miały większego wpływu na niezawodność.

![](readme_pliki/image362.png)

Zależność kąta emisji światła dla detekcji twarzy z obrazów o rozdzielczości 640 x 480 pikseli

![](readme_pliki/image363.png)

Zależność kąta emisji światła dla detekcji twarzy z obrazów o rozdzielczości 160 x 120 pikseli

Porównując powyższe wykresy- dla rozdzielczości 640 x 480 i 160 x 120, nie sposób zauważyć większych różnic, a poprawna wykrywalność wacha się od 70 do 100 %.

![](readme_pliki/image364.png)

Zależność kąta emisji światła dla detekcji twarzy z obrazów o rozdzielczości 54 x 40 pikseli

Jak zostało pokazane, niezawodność wzrastała nieznacznie dla oświetlenia spodniego, gdyż utrzymywała się na poziomie 80 % dla większych i średnich obrazów i na poziomie 20 % dla najmniejszych. Najgorsze wyniki występowały dla obiektów oświetlanych prawej strony od spodu. W tym przypadku dla zdjęć o wymiarach 640 × 480 oraz 160 × 120 poprawne wyniki występowały w 70–90 %, natomiast dla wymiarów 54 × 40 pikseli detektor działał właściwie w 0 do 19 % procent przypadków.

Na wykresach przedstawiony jest wpływ niezawodności detekcji w zależności od kąta padania światła (osobno w pionie i w poziomie) oraz od omawianych wcześniej pozycji głowy dla pozycji P00, P01, P03, P05 i P07.

![](readme_pliki/image365.png)

Wpływ kąta padania światła (w poziomie) na detekcję twarzy dla głowy w pozycjach (P00, P03, P07)

![](readme_pliki/image366.png)

Wpływ kąta padania światła (w pionie) na detekcję twarzy dla głowy w pozycjach (P00, P03, P07)

![](readme_pliki/image367.png)

Wpływ kąta padania światła (w poziomie) na detekcję twarzy dla głowy w pozycjach (P00, P01, P05)

![](readme_pliki/image368.jpg)

Wpływ kąta padania światła (w pionie) na detekcję twarzy dla głowy w pozycjach (P00, P01, P05)

To opracowanie może być przydatne do implementacji np. inteligentnego monitoringu, ponieważ można wybrać odpowiednie położenie kamery w stosunku do panujących warunków oświetlenia wewnątrz pomieszczenia w celu uzyskania wysokiej skuteczności wykrywania.

## <a name="_Toc494697134">12.5.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Rozpoznawanie twarzy</a>

Podczas analizy rozpoznawania twarzy, zmodyfikowano oprogramowanie, z włączeniem algorytmu opartego o PCA (principal component analysis), zwanych także jako _Eigenfaces_. Metoda ta jest oparta na określeniu odległości od najbliższej klasy odnoszącej się do osoby.

![](readme_pliki/image369.png)

GUI systemu rozpoznawania twarzy w środowisku Matlab

Pierwszy tryb oprogramowania (continous processing) umożliwia pozyskiwanie obrazu z bezprzewodowej kamery IP lub standardowej kamery USB i rozpoznawanie twarzy osób, które znajdują się przed obiektywem. W przypadku występowania problemów w związku z pobieraniem obrazów w przestrzeni kolorów YUV, zastosowano automatyczną procedurę konwersji przestrzeni kolorów z YUV na RGB. Kolejnymi możliwymi operacjami do zastosowania w tym trybie była redukcja szumu, zastosowano redukcję koloru skóry omówioną już wcześniej, a także usuwanie tła. Obrazy wchodzące do bazy danych muszą być tych samych rozmiarów. Z tego powodu, należało przeskalować jeszcze raz obszary ROI do rozmiarów ok. 120 x 100 pikseli.

![](readme_pliki/image370.png)

Uproszczony diagram blokowy programu rozpoznawania twarzy

Tryb „batch processing” daje dwie możliwości: analizę pobranych już danych z bazy danych i zapis wyników w postaci plików .xls lub wykorzystywanie wyszczególnionych klatek z kamery.

## <a name="_Toc494697135">12.6.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Wpływ rozdzielczości obrazu na rozpoznawanie twarzy</a>

W prezentowanym eksperymencie, wykorzystano bazy _Yale Face Database_, _Full Faces_ i _MUCTI_.

Pierwsza baza (Yale Face Database) zawiera obrócone twarze dziesięciu osób w dziewięciu pozycjach oraz w sześćdziesięciu czterech różnych kątach oświetlenia dla każdej pozycji. Z powodu trudności, jakich można się spodziewać przy znacznych kątach oświetlenia w stosunku do frontalnej pozycji kamery, zdecydowano się ograniczyć bazę danych do 25% początkowych rozmiarów. Osoba z piątym identyfikatorem została odrzucona z powodu niskiej detekcji wynoszącej około 50%. Z oryginalnych zdjęć o rozdzielczości 640 x 480 pikseli pozyskano obszary twarzy, a następnie przepróbkowane do rozdzielczości najbliższej potędze liczby dwa, a więc w tym przypadku wynoszącej 256 x 256 pikseli. Wszystkie obrazy zostały następnie pomniejszone dwu-, cztero-, ośmio- oraz dwunastokrotnie. Uzyskane rozdzielczości wynoszą odpowiednio: 128 × 128, 64 × 64, 32 × 32 i 21 × 21.

Jako odniesienie do doświadczenia ze zbiorem _Yale_, wybrano 48 losowych obrazów każdej osoby. Pozostałe 95 twarzy (dla klasy) zostało wykorzystane jako zbiór testowy.

![](readme_pliki/image371.png)

Wykres dokładności rozpoznawania twarzy dla oryginalnych oraz pomniejszonych zdjęć (stosunek FAR/FRR) dla bazy Yale Face Database

<span lang="EN-US">**<span lang="PL">Błąd! Nie można odnaleźć źródła odwołania.</span>**</span> pokazuje stosunek FAR (false acceptance rate) do FRR (false rejection rate) dla rozdzielczości od 256 × 256 aż do 21 × 21\. Można zobaczyć, że dwukrotne i czterokrotne zmniejszenie rozdzielczości nie ma większego wpływu na rozpoznawanie twarzy, natomiast jeszcze większa zmiana jakości powoduje zmniejszenie efektywności rozpoznawania o około 8%. EER w trzech przypadkach wynosi ok 30%. Rezultaty pokazują, że nawet czterokrotne zmniejszenie rozdzielczości nie wpływa na dokładność rozpoznawania, nie bacząc na obrót twarzy czy warunki oświetlenia. Łączenie różnych warunków oświetlenia powoduje jednak pewne pogorszenie wyników.

Druga baza danych (FullFaces) zawiera dziesięć (poziomo i pionowo) obróconych obrazów twarzy trzydziestu osób w stałym oświetleniu z zachowaniem rozdzielczości na poziomie 512 × 342 pikseli. Wykorzystano wszystkie obrazy do eksperymentu. Po fazie detekcji obrazy twarzy zostały zapisane w rozdzielczości 256 x 256, a następnie pomnieszone 2, 4, 8 i 12 krotnie. W tym przypadku, 4 losowe zdjęcia posłużyły do utworzenia modelu a następne 6 zostały wykorzystane w fasie rozpoznawania. W przeciwieństwie do bazy danych _Yale_, twarze obrócone są jednie w pionie i poziome. To znacząco wpływa na dokładność.

Współczynnik EER jest w każdym przypadku na poziomie 18-23 %, czyli bardzo zmniejszone z zdjęcia wciąż są rozpoznawane.

![](readme_pliki/image372.png)

Wykres dokładności rozpoznawania twarzy dla oryginalnych oraz pomniejszonych zdjęć (stosunek FAR/FRR) dla bazy FullFaces

Główne różnice mogą być widoczne na końcach wykreślonych linii. W rezultacie można wywnioskować, że przy stałych warunkach oświetleniowych, obrót twarzy i decymacja nie mają znaczącego wpływu na rozpoznawanie (dla algorytmu PCA).

Trzecią bazą jest baza MUCT z kolorowymi zdjęciami w większej liczbie osób. W związku z tym, że baza ta zawiera niestety różne adnotacja dla jednostek, badania podzielono na dwa etapy.

Pierwszym z nich była analiza efektywności rozpoznawania dla zmiennych pozycji twarzy. Dwieście siedemdziesiąt sześć obiektów sfotografowanych z pięciu pozycji zostało określonych do rozpoznawania. Do trenowania posłużyło jedno zdjęcia, natomiast do testów pozostałe cztery. Niezawodność prezentuje się następująco.

![MUCT](readme_pliki/image373.png)

Poprawność rozpoznawania dla zbioru MUCT dla różnych ujęć twarzy (rozmiary oryginalne i zmniejszone)

Rozmiary obrazu wpłynęły na jakość rozpoznawania w stopniu nie większym niż dziesięcio-procentowym. Największe trudności sprawia pochylenie w dół, w przeciwieństwie do pozostałych kierunków.

Drugim etapem było rozpatrywanie wpływu zmian kąta naświetlenia.

![](readme_pliki/image374.png)

Wyniki rozpoznawania dla MUCT w różnych warunkach oświetleniowych.

Sto dziewięćdziesiąt dziewięć różnie oświetlonych jednostek zostało wziętych pod uwagę. Podzielono je na dwie klasy. Występują nieznaczne różnice w wynikach po dwunastokrotnym zmniejszeniu rozdzielczości z poziomu 71% do 61%.

# <a name="_Toc493851923"></a><a name="_Toc494111826"></a><a name="_Toc494200742"></a><a name="_Toc494315505"></a><a name="_Toc494697136">13.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Algorytmy detekcji źródła i kąta padania światła w kontekście rozpoznawania twarzy</a>

## <a name="_Toc493851925"></a><a name="_Toc494111827"></a><a name="_Toc494200743"></a><a name="_Toc494315506"></a><a name="_Toc494697137">13.1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Poszukiwanie informacji</a>

Obraz twarzy człowieka jest jedną z podstawowych danych biometrycznych, która od wielu lat jest umieszczana w dokumentach (dowód osobisty, paszport) jako gwarant tożsamości. Identyfikacja osoby na podstawie obrazu twarzy może mieć wiele zastosowań, np. ułatwić ustalenie tożsamości sprawcy przestępstwa czy stanowić dodatkową formę potwierdzenia tożsamości klienta w instytucjach finansowych.

Nie istnieje jednak automatyczny system, który bezbłędnie identyfikowałby osoby przedstawione na zdjęciu czy obrazie video. Czynnikami, które utrudniają prawidłową identyfikację, są nie tylko obiekty maskujące rzeczywisty wygląd twarzy (makijaż, zarost itd.), lecz także natężenie i kąt padania światła. Cienie i refleksy padające na twarz mogą sprawić, że algorytmy identyfikacji nie będą działały prawidłowo. Naszym zadaniem było znalezienie informacji o algorytmach, które pozwalają na rozpoznawanie twarzy nawet przy nietypowym oświetleniu.

## <a name="_Toc493851926"></a><a name="_Toc494111828"></a><a name="_Toc494200744"></a><a name="_Toc494315507"></a><a name="_Toc494697138">13.2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Jak sprawić, by oświetlenie nie wpływało negatywnie na działanie algorytmów rozpoznawania twarzy?</a>

Rozróżnia się trzy główne sposoby przystosowania algorytmów służących do rozpoznawania twarzy do zmiennych warunków oświetleniowych. Są to kolejno:

1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> stosowanie modelu niewrażliwego na natężenie i kąt padania światła

2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> normalizacja oświetlenia na przetwarzanym obrazie

3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> modelowanie różnych warunków oświetleniowych.

Ze względu duże na zróżnicowanie warunków oświetleniowych, metoda nr 3 jest rzadko używana w praktyce. Najczęściej stosowanym rozwiązaniem jest normalizowanie oświetlenia na obrazie video.

## <a name="_Toc494697139">13.3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Normalizacja światła</a>

Jeden ze sposobów normalizowania oświetlenia został opisany w artykule „Illumination Normalization for Color Face Images”. Algorytm normalizuje oświetlenie, uwzględniając cienie wiele źródeł światła kierunkowego, odbicie lustrzane (specular) i lambertowskie. Z jednego zdjęcia twarzy wyznaczane jest albedo (stosunek światła odbitego do padającego), tworzony jest model 3D ze zdjęcia. Następnie znajdowane są źródła światła z odbić skóry ludzkiej i tworzony jest model Phonga ze składowych RGB, który wraz z oryginalnym zdjęciem i modelem 3D zostaje użyty do wyznaczenia albedo dla twarzy w kolorze.

Albedo skóry twarzy danej osoby ma generalnie stałą wartość. Odcienie skóry dla danej rasy są bardzo podobne i różnią się o stałą odchylenia standardowego. Dodatkowo wyznacza się statystyki dla konkretnego zdjęcia i łączy się oba wyniki.

Odbicia lustrzane (specular) powstają gdy kąt między odbitym światłem i obserwatorem jest niewielki. Odbicia specular wykrywa się, szukając plam o zmniejszonym nasyceniu koloru (saturation). Dla każdego piksela znajdowany jest kąt azymutu i nachylenia względem obserwatora. Piksele oświetlone tym samym światłem będą się grupować razem przy metodzie hierarchical clustering. Niedominujące klastry będą odrzucane. Pozostałe reprezentują źródła oświetlające zdjęcie.

Twarz ludzka sama rzuca na siebie cień (selfshadowing). Użyto zbiorów rozmytych, by wygenerować cienie z klastrów światła. Każdy klaster podzielono na 4 sektory z indywidualnymi kierunkami światła z klastrów. Wyznaczony jest cień uśredniony, a także cień ze zbiorów rozmytych.

Poniższe grafiki przedstawiają efekty działania opisanego wcześniej algorytmu.

![](readme_pliki/image375.jpg)

<span lang="EN-US" style="color:windowtext"> Żródło: Faisal R. Al-Osaimi, Mohammed Bennamoun, Ajmal Mian ,,Illumination Normalization for Color Face Images"</span>

Mając dominujące źródła światła, ich kierunki oraz cienie rozmyte szukamy albedo i natężenia poszczególnych źródeł światła. Zostają one podstawione do modelu Phonga. Model dopasowuje się dla 1000 pikseli twarzy, szukając jak najmniejszych różnic między nimi, a tymi wygenerowanymi przez model.

Znając zmienne wolne w modelu Phonga i warunki oświetleniowe można obliczyć albedo dla każdego piksela twarzy.

![](readme_pliki/image376.jpg)

<span style="color:windowtext"> <span lang="EN-US">Żródło: Faisal R. Al-Osaimi, Mohammed Bennamoun, Ajmal Mian ,,Illumination Normalization for Color Face Images"</span></span>

Zaprezentowany algorytm jest w pełni automatyczny, polega na minimalnej liczbie założeń i dobrze przybliża kierunki i natężenia światła na ludzkiej skórze. Oblicza albedo przez odwrócenie procesu tworzenia zdjęcia. Algorytm był testowany na bazie FRGC v2.0\. Z eksperymentu wynika, że algorytm jest dokładny i solidny.

Inną metodę normalizowania oświetlenia przedstawiono w artykule ,,Dataset Augmentation for Pose and Lighting Invariant Face Recognition''. Autorzy podają sposób na modyfikowanie kąta padania światła na zdjęciu zawierającym twarz, poprzez wygenerowanie modelu 3D głowy ze zdjęcia, a następnie modyfikowanie na nim oświetlenia. Przy uwzględnieniu oszacowanego kształtu 3D, wektory normalne modelu 3D są używane do obliczania czynnika modulacji, który definiuje stosunek kierunkowo odbitego światła do światła otaczającego. Aby uniknąć artefaktów blisko krawędzi modelu 3D, czynniki modulacyjne są ekstrapolowane poza zasięg obrazka przez rozwiązywanie dyskretnych przybliżeń równań Laplace'a, przy użyciu znanych

wartości modulacyjnych jako warunków brzegowych. Pożądane otoczenie oraz światło kierunkowe są potem używane do modulowania nasycenia obrazu w każdym pikselu.

W artykule ,,Enhanced Local Texture Feature Sets for Face Recognition Under Difficult Lighting Conditions’’ przedstawione zostało wykorzystanie deskryptorów  LBP i LTP stosowanego do klasyfikacji w komputerowej wizji.  Sprawdzono w jaki sposób wpływają na rozpoznawanie twarzy. 

W eksperymencie zostały wykorzystane widoki czołowe. Oświetlenie, ekspresja i tożsamość mogą się różnić między sobą.  Przed analizą, wszystkie obrazy poddawane są normalizacji geometrycznej: konwersja na 8-bitowe obrazy w skali szarości; sztywne skalowanie i obracanie obrazu, aby umieścić centra obu oczu w stałych pozycjach, używając współrzędnych oka dostarczonych z pierwotnymi zbiorami danych; kadrowanie obrazu do 120 x 120 pikseli.

<span style="font-family:&quot;Arial&quot;,sans-serif"> </span>

**<span style="font-size:12.0pt;line-height:150%;color:#222222">Lokalne wzorce binarne (LBP):</span>**

<span class="notranslate"><span style="color:#222222">Wektor cechujący LBP jest tworzony w następujący sposób:</span></span>

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span class="notranslate"><span style="line-height:150%;color:#222222">Badane okno trzeba podzielić na komórki (np. 16x16 pikseli).</span></span>

<span class="notranslate"><span style="line-height:150%;font-family:Symbol;color:#222222">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span></span><span class="notranslate"><span style="line-height:150%;color:#222222">Dla każdego piksela w komórce porównaj piksel z każdym z 8 sąsiadów.</span></span>

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span class="notranslate"><span style="line-height:150%;color:#222222">Postępuj zgodnie z pikselami wzdłuż kręgu, tj. Zgodnie z ruchem wskazówek zegara lub przeciwnie do ruchu wskazówek zegara</span></span>

<span class="notranslate"><span style="line-height:150%;font-family:Symbol;color:#222222">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span></span><span class="notranslate"><span style="line-height:150%;color:#222222">Jeśli wartość piksela środkowego jest większa od wartości sąsiada, wpisz "0".</span></span> <span class="notranslate"><span style="line-height:150%;color:#222222">W przeciwnym razie napisz "1".</span></span> <span class="notranslate"><span style="line-height:150%;color:#222222">Dostajemy 8-cyfrową liczbę binarną</span></span>

<span class="notranslate"><span style="font-size:12.0pt;line-height:150%;color:#222222"> </span></span>

**<span style="font-size:12.0pt;line-height:150%">Lokalne modele potrójne (LTP):</span>**

Jest rozszerzeniem wcześniejszej metody LBP. Każdy piksel posiada jedną z 3 wartości: -1,0,1.

Wykorzystuje następujące wartości: k jako stałą progową, c jako wartość środkowego piksela, sąsiedniego piksela p.

Wzór:

![](readme_pliki/image377.png)

<span style="font-size:12.0pt;line-height:150%"> </span>

<span style="font-size:12.0pt;line-height:150%"> </span>

Przykład:  k=5,     [54-k, 54+k] 

![](readme_pliki/image378.png)

<span style="font-size:12.0pt;line-height:150%"> </span>

Wyniki eksperymentu:

Na poniższym wykresie pokazane jest jak można poprawić współczynnik rozpoznawania.

DT - Współczynnik podobieństwa transformacji

PP- Wstępne przetwarzanie

![](readme_pliki/image379.png)

<span lang="EN-US" style="color:windowtext">Żródło: Xiaoyang Tan, Bill Triggs ,,Enhanced Local Texture Feature Sets for Face Recognition Under Difficult Lighting Conditions’’</span>

<span lang="EN-US"> </span>

•<span style="font:7.0pt &quot;Times New Roman&quot;"></span> W porównaniu do standardowego nieprzetworzonego LBP kolejne udoskonalenia są o około 45% skuteczniejsze.

•<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Najwyższy i porównywalny współczynnik rozpoznawania jest przy wykorzystaniu: PP+LBP+DT i PP+LTP+DT.

<span style="font-size:12.0pt;line-height:150%"> </span>

## <a name="_Toc494697140">13.4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Optymalny kąt padania światła</a>

<span lang="EN-US">Próby odpowiedzi na to pytanie podjęli się autorzy artykułu ,,The Effect of Lighting Direction/Condition on the Performance of Face Recognition Algorithms”.</span> W pracy opisano wpływ kierunku światła na proces rozpoznawania twarzy. Celem było wyznaczenie kierunku padania światła, który wpłynie pozytywnie na wydajność algorytmu rozpoznawania twarzy. W tym celu wykonano zdjęcia twarzy w specjalnie wykonanej konstrukcji. Możliwe było stworzenie zestawów różnych ustawień kamery (lewy, prawy profil), obrotu światła.

<span style="font-size:12.0pt;line-height:150%"> </span>

<span style="font-size:12.0pt;line-height:150%"> </span>

<span style="font-size:12.0pt;line-height:150%"> </span>

Każde zdjęcie zostaje podzielone na 4 kwadraty. Linia podziału poziomego znajduje się tuż pod oczami i pionowa linia podziału pomiędzy oczami. Z każdego kwadratu oblicza się 3 wskaźniki statystyczne (statistical measures: the mean, first order autocorrelation, 2nd order moment). Kierunek światła można wyliczyć za pomocą rozkładu luminancji (luminance distribution). Do identyfikacji ust, oczu zostały wykorzystane różne kolory na obrazach.

 Testowano 3 bazy danych:

1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Składa się tylko z 150 obrazów twarzy – każdy obraz przypisany jest do innej osoby, oświetlenie o stałym kierunku.

2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Składa się z 750 obrazów – każda osoba posiada 5 zdjęć widok z przodu, kierunek światła zmienia się od lewej do prawej.

3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Składa się z 7500 obrazów – każda osoba posiada 50 zdjęć, 50 pozycji światła.

![](readme_pliki/image380.jpg)

<span style="color:windowtext">Współczynnik skuteczności algorytmu Eigenfaces w zależności od kąta padania światła.</span> <span lang="EN-US" style="color:windowtext">Żródło: Fahmy G., El-Sherbeeny A., Mandala S., Abdel-Mottaleb M., Ammar H., ,,The Effect of Lighting Direction/Condition on the Performance of Face Recognition Algorithms"</span>

<span lang="EN-US" style="font-size:
12.0pt;line-height:150%;color:black"> </span>

<span lang="EN-US"> </span>

<span lang="EN-US" style="font-size:
12.0pt;line-height:150%"> </span>

<span lang="EN-US" style="font-size:
12.0pt;line-height:150%"> </span>

<span lang="EN-US" style="font-size:
12.0pt;line-height:150%"> </span>

Powyższy wykres przedstawia szybkość wykrywania różnych punktów na twarzy w różnych warunkach oświetleniowych.

Szybkość rozpoznawanie twarzy zmniejsza się gdy kierunek światła jest poza zasięgiem 60-120 stopni. Najszybciej rozpoznaje twarz przy 90 stopniach. Wynika to z fakt, że gdy światło jest bliżej prawego lub lewego profilu twarzy, rozkład luminancji na twarzy jest nierówny, co wpływa na wykrywania obiektów.

## <a name="_Toc494697141"><span style="font-size:12.0pt;line-height:106%">13.5.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Zwiększanie skuteczności algorytmu przez dobór oświetlenia</a>

Badania dotyczące wpływu sztucznych źródeł światła na procesy identyfikacji twarzy zostały opisane w artykule ,,Wpływ czynników środowiskowych na proces identyfikacji osób w oparciu o obraz twarzy". Wyróżniono następujące typy sztucznych źródeł światła: żarówki wolframowe, żarówki halogenowe, świetlówki kompaktowe oraz lampy LED.

Autorzy zauważyli, że kluczowe znaczenie w procesie identyfikacji twarzy współczynnik równomierności oświetlenia (d) rozumiany jako iloraz najmniejszego natężenia oświetlenia na danej płaszczyźnie do średniego natężenia oświetlenia na tej płaszczyźnie. Przeprowadzono badania mające na celu określenie współczynnika równomierności oświetlenia dla poszczególnych sztucznych źródeł światła. Wyniki przeprowadzonego eksperymentu przedstawiono w tabeli:

![](readme_pliki/image381.jpg)

<span style="color:windowtext">Współczynnik równomierności oświetlenia dla poszczególnych sztucznych źródeł światła.</span>

<span style="color:windowtext">Źródło: Wiśnios M., Dąbrowski T., Bednarek M. ,, Wpływ czynników środowiskowych na proces identyfikacji osób w oparciu o obraz twarzy”</span>

<span style="font-size:12.0pt;line-height:115%;font-family:&quot;Tw Cen MT&quot;,sans-serif"> </span>

Autorzy badania podkreślają, że dla zachowania wysokiej skuteczności identyfikacji odległość pomiędzy sztucznym źródłem światła a twarzą identyfikowanej osoby nie powinna wynosić więcej niż 2 metry. Gdy odległość jest większa, współczynnik równomierności oświetlenia gwałtownie spada.

Celem badania było ustalenie dla jakiego źródła światła algorytm rozpoznawania twarzy osiągnie największą skuteczność. Do przeprowadzenia badania wykorzystano bazę danych zawierającą wizerunki 20 osób:

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Po 25 obrazów dla każdego z czterech typów oświetlenia (przeznaczone do procesu uczenia)

<span style="font-family:Symbol">·<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Po 5 obrazów dla każdego z czterech typów oświetlenia (przeznaczone do testowania algorytmu).

<span style="font-size:12.0pt;line-height:150%"> </span>

<span style="font-size:12.0pt;line-height:150%"> </span>

Łącznie w bazie zgromadzono 2400 obrazów twarzy. Parametrami niezmienniczymi były wartość natężenia oświetlenia oraz odległość kamery od twarzy. Schemat stanowiska pomiarowego przedstawia ilustracja:

![](readme_pliki/image382.jpg)

<span style="color:windowtext"> Schemat stanowiska pomiarowego.</span>

<span style="color:windowtext">Źródło: Wiśnios M., Dąbrowski T., Bednarek M. ,,Wpływ czynników środowiskowych na proces identyfikacji osób w oparciu o obraz twarzy”</span>

<span style="font-size:12.0pt;line-height:150%"> </span>

Badania dowiodły, że największą skuteczność uzyskuje się dla obrazów pobieranych przy oświetleniu LED SMD. Wówczas średni współczynnik prawidłowych identyfikacji wynosi 83%. Zauważono również, że sposobem na zwiększenie skuteczności algorytmu jest zastosowanie w procesie uczenia i identyfikacji tego samego źródła światła. Wówczas współczynnik prawidłowych identyfikacji wynosił 98-99%.

<span style="font-size:12.0pt;line-height:150%"> </span>

![](readme_pliki/image383.jpg)

<span style="color:windowtext"> Prawidłowość identyfikacji twarzy dla poszczególnych typów oświetlenia.</span>

<span style="color:windowtext">Źródło: Wiśnios M., Dąbrowski T., Bednarek M. ,, Wpływ czynników środowiskowych na proces identyfikacji osób w oparciu o obraz twarzy”</span>

_<span style="font-family:&quot;Georgia&quot;,serif;color:#44546A"> </span>_

# <a name="_Toc494697142">14.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> Literatura</a>

<span lang="EN-US" style="color:black">1.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Advanced Concepts for Intelligent Vision Systems, Springer, 2008, s. 319 Video-Based Fall Detection in the Home Using Principal Component Analysis, Lykele Hazelhoff, Jungong Han, and Peter H.N. de With</span>

<span lang="EN-US" style="color:black">2.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Computer Vision in Human-Computer Interaction, Springer, 2005, s 211 HMM Based Falling Person Detection Using Both Audio and Video” B. Ugur Töreyin, Yigithan Dedeoglu, and A. Enis Çetin</span>

<span style="color:black">3.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>https://github.com/infr/falldetector-public wraz z http://tunn.us/arduino/falldetector.php oraz http://tunn.us/arduino/falldetector2.php (strony projektu)

<span style="color:black">4.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>https://github.com/harishrithish7/Fall-Detection

<span lang="EN-US" style="color:black">5.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">IFMBE Proceedings, Volume 25/5, Springer 2009, A “Video-based Algorithm for Elderly Fall Detection”,  Jared Willems, Glen Debard, Bart Vanrumste, and Toon Goedem</span>

<span lang="EN-US" style="color:black">6.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Camera-based fall detection on real world data, G. Debard1, P. Karsmakers, M. Deschodt, E. Vlaeyen, E. Dejaeger, K. Milisen, T. Goedem´e, B. Vanrumste oraz T. Tuytelaars.</span>

<span lang="EN-US" style="color:black">7.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Rozprawa doktorska: Detekcja upadku i wybranych akcji na sekwencjach obrazów cyfrowych, mgr inż. <span lang="EN-US">Michał Kępski.</span>

<span lang="EN-US" style="color:black">8.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Fall Incidents Detection for Intelligent Video Surveillance, Ji Tao, Mukherjee Turjo, Mun-Fei Wong, Mengdi Wang and Yap-Peng Tan</span>

<span lang="EN-US" style="color:black">9.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Multi-view fall detection based on spatio-temporal interest points, Songzhi Su & Sin-Sian Wu & Shu-Yuan Chen & Der-Jyh Duh & Shaozi Li</span>

<span lang="EN-US" style="color:black">10.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Intelligent video surveillance for monitoring fall detection of elderly in home environments, Homa Foroughi, Baharak Shakeri Aski2 i Hamidreza Pourreza</span>

<span lang="EN-US" style="color:black">11.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Fall Detection from Human Shape and Motion History using Video Surveillance, Caroline Rougier, Jean Meunier, Alain St-Arnaud, Jacqueline Rousseau</span>

<span lang="EN-US" style="color:black">12.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Fast Human Face Detection Using Successive Face Detectors with Incremental Detection Capability, Zuo, F.</span>

<span lang="EN-US" style="color:black">13.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Human fall detection in surveillance video based on PCANet, Shengke Wang & Long Chen & Zixi Zhou & Xin Sun1 & Junyu Dong</span>

<span lang="EN-US" style="color:black">14.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Activity Summarisation and Fall Detection in a Supportive Home Environment, Hammadi Nait-Charif and Stephen J. McKenna</span>

<span lang="EN-US" style="color:black">15.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Compressed-Domain Fall Incident Detection for Intelligent Homecare, Chia-Wen Lin, Zhi-Hong Ling, Yeng-Cheng Chang, and Chung J. Kuo</span>

<span lang="EN-US" style="color:black">16.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Monocular 3D Head Tracking to Detect Falls of Elderly People, Caroline Rougier, Jean Meunier, Alain St-Arnaud and Jacqueline Rousseau</span>

<span lang="EN-US" style="color:black">17.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Elliptical head tracking using intensity gradients and color histograms, S. Birchfield</span>

<span lang="EN-US" style="color:black">18.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Video Surveillance for Fall Detection, Caroline Rougier, Alain St-Arnaud, Jacqueline Rousseau and Jean Meunier</span>

<span lang="EN-US" style="color:black">19.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Human activity detection based on edge point movements and spatio-temporal features in indoor, Lakshmi Priya, Smitha Suresh</span>

<span lang="EN-US" style="color:black">20.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">https://www.cs.colostate.edu/~cs510/yr2014sp/more_progress/L19_STIP.pdf</span>

<span lang="EN-US" style="color:black">21.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">https://angel.co/projects/280211-human-activity-recognition-in-videos-using-opencv-python</span>

<span lang="EN-US" style="color:black">22.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Selective spatio-temporal interest points, Bhaskar Chakrabortya, Michael B. Holteb, Thomas B. Moeslundb, Jordi Gonz`aleza</span>

<span lang="EN-US" style="color:black">23.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Pictorial Structures Revisited: People Detection and Articulated Pose Estimation, Mykhaylo Andriluka, Stefan Roth, and Bernt Schiele</span>

<span lang="EN-US" style="color:black">24.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Pose Machines: Articulated Pose Estimation via Inference Machines, Varun Ramakrishna, Daniel Munoz, Martial Hebert, J. Andrew Bagnell, and Yaser Sheikh</span>

<span lang="EN-US" style="color:black">25.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Pictorial Structures for Articulated Pose Estimation, ( Human-aware Robotics Seminar ), Yezhou Yang</span>

<span lang="EN-US" style="color:black">26.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">https://github.com/wg-perception/PartsBasedDetector</span>

<span lang="EN-US" style="color:black">27.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Articulated Pose Estimation with Flexible Mixtures-of-Parts, Yi Yang, Deva Ramanan,</span>

<span lang="EN-US" style="color:black">28.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Mastering OpenCV with Practical Computer Vision Projects, Detecting the face, Face preprocessing, s. 261-281, Birmingham (2012), EBSCOhost</span>

<span lang="EN-US" style="color:black">29.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications, Combining Face and Facial Feature Detectors for Face Detection Performance Improvement, s. 82-89, Springer (Sep 2012)</span>

<span lang="EN-US" style="color:black">30.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Image Analysis and Recognition, From Optical Flow to Tracking Objects on Movie Videos, Springer (2011)</span>

<span lang="EN-US" style="color:black">31.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Learning OpenCV Computer Vision with the OpenCV Library, Tracking and motion, s 316-369, (2008)</span>

<span lang="EN-US" style="color:black">32.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Soft Computing for Recognition Based on Biometrics, Springer, (2010)</span>

<span lang="EN-US" style="color:black">33.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Advances in Multimedia Information Processing - PCM 2007, Human Face and Action Recognition</span>

<span style="color:black">34.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Szczepan Kurnyta „Optymalizacja algorytmu wyznaczającego przepływ optyczny dla obrazów kolorowych”

<span lang="EN-US" style="color:black">35.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">David Stavens „The OpenCV Library: Computing Optical Flow”</span>

<span lang="EN-US" style="color:black">36.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">David J. Fleet, Yair Weiss  „Optical Flow Estimation”</span>

<span lang="EN-US" style="color:black">37.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Dr. Florian Raudies  „Optical Flow”</span>

<span style="color:black">38.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Marcin Nazimek, „Porównanie skuteczności algorytmów detekcji ruchu dla systemów wizyjnych ruchu ulicznego w wykrywaniu pojazdów”

<span style="color:black">39.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Marcin Bzdawski, „Śledzenie obiektów w sekwencjach obrazów”

<span style="color:black">40.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Andrzej Głowacz, Zbigniew Mikrut, Piotr Pawlik, „Algorytm wideodetekcji korzystający z metody obliczenia przepływu optycznego”

<span lang="EN-US" style="color:black">41.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Aroh Barjatya, „Block Matching Algorithms For Motion Estimation”</span>

<span lang="EN-US" style="color:black">42.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Dhara Patel, „Optical Flow Measurement using Lucas kanade Method”</span>

<span lang="EN-US" style="color:black">43.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Wikipedia, „Haar-like feature”</span>

<span lang="EN-US" style="color:black">44.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Adam Harvey, „Adam Harvey Explains Viola-Jones Face Detection”</span>

<span lang="EN-US" style="color:black">45.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span class="MsoHyperlink"><span lang="EN-US">[http://docs.opencv.org/3.1.0/d7/d8b/tutorial_py_face_detection.html](http://docs.opencv.org/3.1.0/d7/d8b/tutorial_py_face_detection.html)</span></span>

<span lang="EN-US" style="color:black">46.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">http://docs.opencv.org/3.2.0/dc/d88/tutorial_traincascade.html</span>

<span lang="EN-US" style="color:black">47.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Ramiz Raja, „Face detection using OpenCV and Python: a beginner’s guide”</span>

<span lang="EN-US" style="color:black">48.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Faisal R. Al-Osaimi, Mohammed Bennamoun, Ajmal Mian ,,Illumination Normalization for Color Face Images"</span>

<span lang="EN-US" style="color:black">49.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Daniel Crispell, Octavian Biris, Nate Crosswhite, Jeffrey Byrne, Joseph L. Mundy, ,,Dataset Augmentation for Pose and Lighting Invariant Face Recognition''</span>

<span lang="EN-US" style="color:black">50.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Xiaoyang Tan, Bill Triggs, ,,Enhanced Local Texture Feature Sets for Face Recognition Under Difficult Lighting Conditions’’</span>

<span lang="EN-US" style="color:black">51.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span><span lang="EN-US">Fahmy G., El-Sherbeeny A., Mandala S., Abdel-Mottaleb M., Ammar H., ,,The Effect of Lighting Direction/Condition on the Performance of Face Recognition Algorithms"</span>

<span style="color:black">52.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Wiśnios M., Dąbrowski T., Bednarek M. ,, Wpływ czynników środowiskowych na proces identyfikacji osób w oparciu o obraz twarzy”

<span style="color:black">53.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>http://docs.opencv.org/3.2.0/d7/d8b/tutorial_py_lucas_kanade.html

<span style="color:black">54.<span style="font:7.0pt &quot;Times New Roman&quot;"></span> </span>Bzdawski M. ,,Śledzenie obiektów w sekwencjach obrazów”

</div>